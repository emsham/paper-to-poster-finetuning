{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761ffccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majd/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 110376.42it/s]\n",
      "Loading weights: 100%|██████████| 750/750 [00:02<00:00, 350.72it/s, Materializing param=model.visual.pos_embed.weight]                                 \n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a heartwarming and serene photograph capturing a moment of connection between a woman and her dog on a beach at sunset.\\n\\n**Key Elements:**\\n\\n*   **The Subjects:** A woman and a yellow Labrador Retriever are the central focus.\\n*   **The Woman:** She is sitting cross-legged in the sand, wearing a plaid shirt and dark pants. She has long, dark hair and is smiling warmly, looking at the dog with affection. Her right hand is extended, and the dog is gently placing its paw on her hand, likely as part of a training exercise or a playful interaction. She is also holding a']\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-8B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen3-VL-8B-Instruct\",\n",
    "#     dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a31185",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"/home/majd/Documents/Projects/paper-to-poster-finetuning/images/0cdf61037d7053ca59347ab230818335.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5d3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (3456, 2304), mode: RGB\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "image_path = \"/home/majd/Documents/Projects/paper-to-poster-finetuning/images/0cdf61037d7053ca59347ab230818335.png\"\n",
    "\n",
    "# Load image with PIL first to ensure it's valid\n",
    "image = Image.open(image_path)\n",
    "print(f\"Image size: {image.size}, mode: {image.mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0ed5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"poster\": {\n",
      "    \"orientation\": \"landscape\",\n",
      "    \"aspect_ratio\": \"16:9\",\n",
      "    \"background\": \"#FFFFFF\"\n",
      "  },\n",
      "  \"header\": {\n",
      "    \"height_pct\": 10,\n",
      "    \"background\": \"#FFFFFF\",\n",
      "    \"gradient_colors\": null,\n",
      "    \"title_alignment\": \"center\",\n",
      "    \"logo_positions\": [\"left\", \"right\"],\n",
      "    \"has_author_affiliation_superscripts\": true\n",
      "  },\n",
      "  \"footer\": {\n",
      "    \"present\": true,\n",
      "    \"height_pct\": 5,\n",
      "    \"background\": \"#FFFFFF\",\n",
      "    \"content\": \"references\"\n",
      "  },\n",
      "  \"body\": {\n",
      "    \"columns\": 3,\n",
      "    \"column_widths\": [\"30%\", \"40%\", \"30%\"],\n",
      "    \"gutter_pct\": 5\n",
      "  },\n",
      "  \"sections\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"title\": \"Introduction\",\n",
      "      \"column\": 1,\n",
      "      \"column_span\": 1,\n",
      "      \"row_in_column\": 1,\n",
      "      \"height_pct\": 20,\n",
      "      \"style\": {\n",
      "        \"header_bg\": \"#0033A0\",\n",
      "        \"header_text_color\": \"#FFFFFF\",\n",
      "        \"body_bg\": \"#FFFFFF\",\n",
      "        \"border\": \"#0033A0\",\n",
      "        \"border_radius\": \"small\"\n",
      "      },\n",
      "      \"content_type\": \"text\",\n",
      "      \"content_layout\": {\n",
      "        \"arrangement\": \"vertical\",\n",
      "        \"split\": null,\n",
      "        \"figure_count\": 0,\n",
      "        \"has_equations\": false\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"title\": \"Results\",\n",
      "      \"column\": 2,\n",
      "      \"column_span\": 1,\n",
      "      \"row_in_column\": 1,\n",
      "      \"height_pct\": 20,\n",
      "      \"style\": {\n",
      "        \"header_bg\": \"#0033A0\",\n",
      "        \"header_text_color\": \"#FFFFFF\",\n",
      "        \"body_bg\": \"#FFFFFF\",\n",
      "        \"border\": \"#0033A0\",\n",
      "        \"border_radius\": \"small\"\n",
      "      },\n",
      "      \"content_type\": \"mixed\",\n",
      "      \"content_layout\": {\n",
      "        \"arrangement\": \"vertical\",\n",
      "        \"split\": \"table and figure\",\n",
      "        \"figure_count\": 2,\n",
      "        \"has_equations\": false\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"title\": \"Cross-task Normalization effect\",\n",
      "      \"column\": 1,\n",
      "      \"column_span\": 1,\n",
      "      \"row_in_column\": 2,\n",
      "      \"height_pct\": 20,\n",
      "      \"style\": {\n",
      "        \"header_bg\": \"#0033A0\",\n",
      "        \"header_text_color\": \"#FFFFFF\",\n",
      "        \"body_bg\": \"#FFFFFF\",\n",
      "        \"border\": \"#0033A0\",\n",
      "        \"border_radius\": \"small\"\n",
      "      },\n",
      "      \"content_type\": \"mixed\",\n",
      "      \"content_layout\": {\n",
      "        \"arrangement\": \"vertical\",\n",
      "        \"split\": \"table and text\",\n",
      "        \"figure_count\": 0,\n",
      "        \"has_equations\": false\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"title\": \"Continual Normalization\",\n",
      "      \"column\": 2,\n",
      "      \"column_span\": 1,\n",
      "      \"row_in_column\": 2,\n",
      "      \"height_pct\": 20,\n",
      "      \"style\": {\n",
      "        \"header_bg\": \"#0033A0\",\n",
      "        \"header_text_color\": \"#FFFFFF\",\n",
      "        \"body_bg\": \"#FFFFFF\",\n",
      "        \"border\": \"#0033A0\",\n",
      "        \"border_radius\": \"small\"\n",
      "      },\n",
      "      \"content_type\": \"mixed\",\n",
      "      \"content_layout\": {\n",
      "        \"arrangement\": \"vertical\",\n",
      "        \"split\": \"text, figure, text\",\n",
      "        \"figure_count\": 1,\n",
      "        \"has_equations\": true\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"title\": \"Summary\",\n",
      "      \"column\": 3,\n",
      "      \"column_span\": 1,\n",
      "      \"row_in_column\": 1,\n",
      "      \"height_pct\": 20,\n",
      "      \"style\": {\n",
      "        \"header_bg\": \"#0033A0\",\n",
      "        \"header_text_color\": \"#FFFFFF\",\n",
      "        \"body_bg\": \"#FFFFFF\",\n",
      "        \"border\": \"#0033A0\",\n",
      "        \"border_radius\": \"small\"\n",
      "      },\n",
      "      \"content_type\": \"text\",\n",
      "      \"content_layout\": {\n",
      "        \"arrangement\": \"vertical\",\n",
      "        \"split\": null,\n",
      "        \"figure_count\": 0,\n",
      "        \"has_equations\": false\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"figures\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"caption\": \"Better and faster convergence\",\n",
      "      \"position\": \"Results section, right side\",\n",
      "      \"type\": \"line chart\",\n",
      "      \"description\": \"Two line charts comparing performance over tasks for different methods.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"caption\": \"Continual Normalization architecture\",\n",
      "      \"position\": \"Continual Normalization section, left side\",\n",
      "      \"type\": \"3D diagram\",\n",
      "      \"description\": \"3D cube diagram illustrating feature map normalization.\"\n",
      "    }\n",
      "  ],\n",
      "  \"flowcharts\": [],\n",
      "  \"special_elements\": [],\n",
      "  \"color_scheme\": {\n",
      "    \"primary\": \"#0033A0\",\n",
      "    \"secondary\": \"#FFFFFF\",\n",
      "    \"accent\": \"#000000\"\n",
      "  },\n",
      "  \"reading_order\": \"columns-left-to-right\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"You are a precise layout analysis system for academic posters. Your task is to extract structural and spatial information that enables exact LaTeX reconstruction.\n",
    "\n",
    "Focus on:\n",
    "- Grid structure and column layout\n",
    "- Section positions and dimensions (as percentages)\n",
    "- Section styling (colors, borders, headers)\n",
    "- Figure types and arrangements within sections\n",
    "- Visual hierarchy and reading flow\n",
    "\n",
    "Do NOT transcribe text content - only capture structure.\n",
    "\n",
    "Output valid JSON following the schema exactly.\n",
    "\n",
    "Analyze this academic poster's layout structure. Output JSON following this schema:\n",
    "\n",
    "{\n",
    "  \"poster\": {\n",
    "    \"orientation\": \"landscape|portrait\",\n",
    "    \"aspect_ratio\": \"e.g., 16:9, 4:3, 1.41:1\",\n",
    "    \"background\": \"#hex\"\n",
    "  },\n",
    "  \"header\": {\n",
    "    \"height_pct\": number,\n",
    "    \"background\": \"#hex|gradient|none\",\n",
    "    \"gradient_colors\": [\"#hex1\", \"#hex2\"] or null,\n",
    "    \"title_alignment\": \"left|center\",\n",
    "    \"logo_positions\": [\"left\", \"right\", \"none\"],\n",
    "    \"has_author_affiliation_superscripts\": boolean\n",
    "  },\n",
    "  \"footer\": {\n",
    "    \"present\": boolean,\n",
    "    \"height_pct\": number or null,\n",
    "    \"background\": \"#hex|gradient|none\",\n",
    "    \"content\": \"contact|references|none\"\n",
    "  },\n",
    "  \"body\": {\n",
    "    \"columns\": number,\n",
    "    \"column_widths\": [\"equal\"] or [\"30%\", \"40%\", \"30%\"],\n",
    "    \"gutter_pct\": number\n",
    "  },\n",
    "  \"sections\": [\n",
    "    {\n",
    "      \"id\": number,\n",
    "      \"title\": \"Section Title Text\",\n",
    "      \"column\": number,\n",
    "      \"column_span\": number,\n",
    "      \"row_in_column\": number,\n",
    "      \"height_pct\": number,\n",
    "      \"style\": {\n",
    "        \"header_bg\": \"#hex|none\",\n",
    "        \"header_text_color\": \"#hex\",\n",
    "        \"body_bg\": \"#hex|transparent\",\n",
    "        \"border\": \"#hex|none\",\n",
    "        \"border_radius\": \"none|small|medium|large\"\n",
    "      },\n",
    "      \"content_type\": \"text|bullets|figure|table|equation|flowchart|mixed\",\n",
    "      \"content_layout\": {\n",
    "        \"arrangement\": \"vertical|horizontal|grid\",\n",
    "        \"split\": \"description if mixed\",\n",
    "        \"figure_count\": number or null,\n",
    "        \"has_equations\": boolean\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"figures\": [...],\n",
    "  \"flowcharts\": [...],\n",
    "  \"special_elements\": [...],\n",
    "  \"color_scheme\": {...},\n",
    "  \"reading_order\": \"columns-left-to-right|rows-top-to-bottom|numbered|arrows\"\n",
    "}\"\"\"\n",
    "\n",
    "# Replace with your image path or URL\n",
    "image_url = \"YOUR_POSTER_IMAGE_URL_HERE\"\n",
    "# Or for local file:\n",
    "# image_path = \"/path/to/your/poster.png\"\n",
    "\n",
    "# Prepare messages - content must always be a list of dicts for Qwen2-VL\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": user_prompt\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n",
    "# Preparation for inference\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference\n",
    "generated_ids = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=4096,\n",
    "    temperature=0.1,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006b25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
