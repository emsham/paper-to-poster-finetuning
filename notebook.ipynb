{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e4e74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d72dfde2964d4b8f2b528fb3672952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1bee26d7754668a803808752e05cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/7.95M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729c53b89188401ab26da6ffd5835983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/2.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097a8d777d3546be94e8ab1917197846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/2.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd11ef3b48d49ea938344f467a2065b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87c51c8a78f4a57b92c0234e9c48596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3226dad47b493aa303f96a53809bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"rohitsaxena/PosterSum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27d4cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conference', 'year', 'paper_id', 'title', 'abstract', 'topics', 'image_url'],\n",
       "        num_rows: 10305\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['conference', 'year', 'paper_id', 'title', 'abstract', 'topics', 'image_url'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conference', 'year', 'paper_id', 'title', 'abstract', 'topics', 'image_url'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1550379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/rohitsaxena/PosterSum/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2336e995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "topics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "image_url",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "37662a98-6039-43b2-8bc6-6ce90b6891c1",
       "rows": [
        [
         "0",
         "ICLR",
         "2024",
         "19205",
         "A Fast and Provable Algorithm for Sparse Phase Retrieval",
         "We study the sparse phase retrieval problem, which seeks to recover a sparse signal from a limited set of magnitude-only measurements. In contrast to prevalent sparse phase retrieval algorithms that primarily use first-order methods, we propose an innovative second-order algorithm that employs a Newton-type method with hard thresholding. This algorithm overcomes the linear convergence limitations of first-order methods while preserving their hallmark per-iteration computational efficiency. We provide theoretical guarantees that our algorithm converges to the $s$-sparse ground truth signal $\\boldsymbol{x}^{\\natural} \\in \\mathbb{R}^n$ (up to a global sign) at a quadratic convergence rate after at most $O(\\log (\\Vert\\boldsymbol{x}^{\\natural} \\Vert /x_{\\min}^{\\natural}))$ iterations, using $\\Omega(s^2\\log n)$ Gaussian random samples. Numerical experiments show that our algorithm achieves a significantly faster convergence rate than state-of-the-art methods.",
         "['Signal Processing' 'Computational Mathematics' 'Optimization'\n 'Algorithms']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19205.png"
        ],
        [
         "1",
         "ICLR",
         "2024",
         "19234",
         "Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression",
         "Data augmentation is critical to the empirical success of modern self-supervised representation learning, such as contrastive learning and masked language modeling.However, a theoretical understanding of the exact role of the augmentation remains limited.Recent work has built the connection between self-supervised learning and the approximation of the top eigenspace of a graph Laplacian operator, suggesting that learning a linear probe atop such representation can be connected to RKHS regression.Building on this insight, this work delves into a statistical analysis of augmentation-based pretraining.Starting from the isometry property, a geometric characterization of the target function given by the augmentation, we disentangle the effects of the model and the augmentation,and prove two generalization bounds that are free of model complexity.Our first bound works for an arbitrary encoder, and it is the sum of an estimation error bound incurred by fitting a linear probe, and an approximation error bound by RKHS approximation.Our second bound specifically addresses the casewhere the encoder extracts the top-d eigenspace of a finite-sample-based approximation of the underlying RKHS.A key ingredient in our analysis is theaugmentation complexity,which we use to quantitatively compare different augmentations and analyze their impact on downstream performance.",
         "['Self-Supervised Learning' 'Representation Learning' 'Data Augmentation'\n 'Theoretical Analysis in Machine Learning'\n 'Reproducing Kernel Hilbert Space ' 'Statistical Learning Theory']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19234.png"
        ],
        [
         "2",
         "NeurIPS",
         "2023",
         "71333",
         "Regularized Behavior Cloning for Blocking the Leakage of Past Action Information",
         "For partially observable environments, imitation learning with observation histories (ILOH) assumes that control-relevant information is sufficiently captured in the observation histories for imitating the expert actions. In the offline setting wherethe agent is required to learn to imitate without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet effective method for imitation learning. However, when the information about the actions executed in the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own past actions. In this paper, we address this catastrophic failure by proposing a principled regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind our approach is to leverage the classical notion of conditional independence to mitigate the leakage. We compare different instances of our framework with natural choices of conditional independence metric and its estimator. The result of our comparison advocates the use of a particular kernel-based estimator for the conditional independence metric. We conduct an extensive set of experiments on benchmark datasets in order to assess the effectiveness of our regularization method. The experimental results show that our method significantly outperforms prior related approaches, highlighting its potential to successfully imitate expert actions when the past action information leaks into the observation histories.",
         "['Imitation Learning' 'Reinforcement Learning'\n 'Partially Observable Environments' 'Behavior Cloning'\n 'Regularization Techniques']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/71333.png"
        ],
        [
         "3",
         "NeurIPS",
         "2023",
         "72466",
         "Koopman Kernel Regression",
         "Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging.Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear time-invariant (LTI) ODEs, turning multi-step forecasts into sparse matrix multiplication.Though there exists a variety of learning approaches, they usually lack crucial learning-theoretic guarantees, making the behavior of the obtained models with increasing data and dimensionality unclear.We address the aforementioned by deriving a universal Koopman-invariant reproducing kernel Hilbert space (RKHS) that solely spans transformations into LTI dynamical systems. The resulting Koopman Kernel Regression (KKR) framework enables the use of statistical learning tools from function approximation for novel convergence results and generalization error bounds under weaker assumptions than existing work. Our experiments demonstrate superior forecasting performance compared to Koopman operator and sequential data predictors in RKHS.",
         "['Reinforcement Learning' 'Dynamical Systems' 'Predictive Modeling'\n 'Kernel Methods' 'Statistical Learning' 'Optimization' 'Control Theory']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72466.png"
        ],
        [
         "4",
         "ICML",
         "2023",
         "25261",
         "Learning GFlowNets From Partial Episodes For Improved Convergence And Stability",
         "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\\lambda$) algorithm in reinforcement learning, we introduce *subtrajectory balance* or SubTB($\\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.",
         "['Reinforcement Learning' 'Probabilistic Modeling'\n 'Algorithmic Development']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/25261.png"
        ],
        [
         "5",
         "ICML",
         "2024",
         "33124",
         "Viewing Transformers Through the Lens of Long Convolutions Layers",
         "Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of longrange layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal modifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our theory and experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies.",
         "['Deep Learning' 'Natural Language Processing' 'Transformer Models'\n 'Long-Range Dependencies' 'Neural Network Architectures']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/33124.png"
        ],
        [
         "6",
         "ICLR",
         "2022",
         "6644",
         "Towards Model Agnostic Federated Learning Using Knowledge Distillation",
         "Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data at hand. Knowledge distillation (KD) is the obvious tool of choice to design such protocols. However, surprisingly, we show that most natural KD-based federated learning protocols have poor performance.        To investigate this, we propose a new theoretical framework, Federated Kernel ridge regression, which can capture both model heterogeneity as well as data heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further validate our framework by analyzing and designing new protocols based on KD. Their performance on real world experiments using neural networks, though still unsatisfactory, closely matches our theoretical predictions.",
         "['Federated Learning' 'Knowledge Distillation' 'Model Agnostic Methods'\n 'Data Heterogeneity' 'Neural Networks']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202022/28267ab848bcf807b2ed53c3a8f8fc8a_Qg0wg4t.png"
        ],
        [
         "7",
         "ICML",
         "2023",
         "24401",
         "Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning",
         "Policy optimization methods with function approximation are widely used in multi-agent reinforcement learning. However, it remains elusive how to design such algorithms with statistical guarantees. Leveraging a multi-agent performance difference lemma that characterizes the landscape of multi-agent policy optimization, we find that the localized action value function serves as an ideal descent direction for each local policy. Motivated by the observation, we present a multi-agent PPO algorithm in which the local policy of each agent is updated similarly to vanilla PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent quantities, our algorithm converges to the globally optimal policy at a sublinear rate. We extend our algorithm to the off-policy setting and introduce pessimism to policy evaluation, which aligns with experiments. To our knowledge, this is the first provably convergent multi-agent PPO algorithm in cooperative Markov games.",
         "['Multi-Agent Reinforcement Learning' 'Policy Optimization'\n 'Function Approximation' 'Cooperative Markov Games'\n 'Algorithm Design and Analysis']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24401.png"
        ],
        [
         "8",
         "NeurIPS",
         "2023",
         "79608",
         "Grounding Code Generation with Input-Output Specifications",
         "Large language models (LLMs) have demonstrated significant potential in code generation. However, the code generated by these models occasionally deviates from the user's intended outcome, resulting in executable but incorrect code. To mitigate this issue, we propose Gift4Code, a novel approach for the instruction fine-tuning of LLMs specifically tailored for code generation. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program input-output specifications, is provided to the LLM to facilitate fine-tuning. We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000. Our results suggest that the method enhances the LLM's alignment with user intentions, reducing the incidence of executable but incorrect outputs.",
         "['Code Generation' 'Natural Language Processing' 'Software Engineering']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/79608.png"
        ],
        [
         "9",
         "ICLR",
         "2024",
         "18178",
         "REFACTOR: Learning to Extract Theorems from Proofs",
         "Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6\\% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems. Code can be found at https://github.com/jinpz/refactor.",
         "['Automated Theorem Proving' 'Formal Methods' 'Computational Mathematics']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18178.png"
        ],
        [
         "10",
         "ICLR",
         "2024",
         "20866",
         "Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel",
         "Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. However, to compress further text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations with current technology. We push semantic compression as low as 100 μbpp (up to 10,000× smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 μbpp level represents a soft limit on semantic compression at standard image resolutions.",
         "['Image Compression' 'Semantic Compression' 'Computer Vision'\n 'Natural Language Processing']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/20866.png"
        ],
        [
         "11",
         "NeurIPS",
         "2023",
         "69905",
         "Automated Classification of Model Errors on ImageNet",
         "While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over 95% accuracy and shifting the focus on investigating why the remaining errors persist.Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 900 models. Perhaps surprisingly, we find that across model architectures, scales, and pre-training corpora, top-1 accuracy is a strong predictor for theportionof all error types. In particular, we observe that the portion of severe errors drops significantly with top-1 accuracy indicating that, while it underreports a model's true performance, it remains a valuable performance metric.We release all our code at https://github.com/eth-sri/automated-error-analysis.",
         "['Computer Vision' 'Image Classification' 'Model Evaluation'\n 'Error Analysis']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/69905.png"
        ],
        [
         "12",
         "ICLR",
         "2023",
         "11321",
         "Understanding Embodied Reference with Touch-Line Transformer",
         "We study embodied reference understanding, the task of locating referents using embodied gestural signals and language references. Human studies have revealed that, contrary to popular belief, objects referred to or pointed to do not lie on the elbow-wrist line, but rather on the so-called virtual touch line. Nevertheless, contemporary human pose representations lack the virtual touch line. To tackle this problem, we devise the touch-line Transformer: It takes as input tokenized visual and textual features and simultaneously predicts the referent’s bounding box and a touch-line vector. Leveraging this touch-line prior, we further devise a geometric consistency loss that promotes co-linearity between referents and touch lines. Using the touch line as gestural information dramatically improves model performances: Experiments on the YouRefIt dataset demonstrate that our method yields a +25.0% accuracy improvement under the 0.75 IoU criterion, hence closing 63.6% of the performance difference between models and humans. Furthermore, we computationally validate prior human studies by demonstrating that computational models more accurately locate referents when employing the virtual touch line than when using the elbow-wrist line.",
         "['Computer Vision' 'Natural Language Processing'\n 'Human-Computer Interaction' 'Robotics']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11321.png"
        ],
        [
         "13",
         "ICML",
         "2022",
         "17303",
         "Path-Gradient Estimators for Continuous Normalizing Flows",
         "Recent work has established a path-gradient estimator for simple variational Gaussian distributions and has argued that the path-gradient is particularly beneficial in the regime in which the variational distribution approaches the exact target distribution. In many applications, this regime can however not be reached by a simple Gaussian variational distribution. In this work, we overcome this crucial limitation by proposing a path-gradient estimator for the considerably more expressive variational family of continuous normalizing flows. We outline an efficient algorithm to calculate this estimator and establish its superior performance empirically.",
         "['Variational Inference' 'Normalizing Flows' 'Computational Statistics']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/7417744a2bac776fabe5a09b21c707a2.png"
        ],
        [
         "14",
         "ICLR",
         "2023",
         "10802",
         "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation",
         "Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge\". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on a public dataset nor it deploys a generative model at the server. We analyze convergence of FedHKD and conduct extensive experiments on visual datasets in a variety of scenarios, demonstrating that FedHKD provides significant improvement in both personalized as well as global model performance compared to state-of-the-art FL methods designed for heterogeneous data settings.",
         "['Federated Learning' 'Personalized Federated Learning'\n 'Knowledge Distillation' 'Data Heterogeneity' 'Model Convergence'\n 'Visual Data Analysis']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/10802.png"
        ],
        [
         "15",
         "ICLR",
         "2023",
         "11174",
         "ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion",
         "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",
         "['Knowledge Graphs' 'Data Mining' 'Graph Embedding']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11174.png"
        ],
        [
         "16",
         "ICML",
         "2024",
         "33187",
         "Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency",
         "Catastrophic overfitting (CO) presents a significant challenge in single-step adversarial training (AT), manifesting as highly distorted deep neural networks (DNNs) that are vulnerable to multi-step adversarial attacks. However, the underlying factors that lead to the distortion of decision boundaries remain unclear. In this work, we delve into the specific changes within different DNN layers and discover that during CO, the former layers are more susceptible, experiencing earlier and greater distortion, while the latter layers show relative insensitivity. Our analysis further reveals that this increased sensitivity in former layers stems from the formation of $\\textit{pseudo-robust shortcuts}$, which alone can impeccably defend against single-step adversarial attacks but bypass genuine-robust learning, resulting in distorted decision boundaries. Eliminating these shortcuts can partially restore robustness in DNNs from the CO state, thereby verifying that dependence on them triggers the occurrence of CO. This understanding motivates us to implement adaptive weight perturbations across different layers to hinder the generation of $\\textit{pseudo-robust shortcuts}$, consequently mitigating CO. Extensive experiments demonstrate that our proposed method, $\\textbf{L}$ayer-$\\textbf{A}$ware Adversarial Weight $\\textbf{P}$erturbation (LAP), can effectively prevent CO and further enhance robustness.",
         "['Deep Learning' 'Adversarial Machine Learning' 'Neural Networks'\n 'Robustness in Machine Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/33187.png"
        ],
        [
         "17",
         "NeurIPS",
         "2023",
         "72160",
         "On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\\epsilon$-Greedy Exploration",
         "This paper provides a theoretical understanding of deep Q-Network (DQN) with the $\\varepsilon$-greedy exploration in deep reinforcement learning.Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored.First, the exploration strategy is either impractical or ignored in the existing analysis.  Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training  the Q-network. However,the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the  practical setting of DQNs with $\\epsilon$-greedy policy. We prove an iterative procedure with decaying $\\epsilon$ converges to the optimal Q-value function geometrically. Moreover, a higher level of $\\epsilon$ values enlarges the region of convergence but slows down the convergence, while the opposite holds for a lower level of $\\epsilon$ values. Experiments justify our established theoretical insights on DQNs.",
         "['Deep Reinforcement Learning' 'Machine Learning Theory'\n 'Convergence Analysis' 'Sample Complexity Analysis']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72160.png"
        ],
        [
         "18",
         "NeurIPS",
         "2023",
         "74880",
         "DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility",
         "When a small molecule binds to a protein, the 3D structure of the protein and its function change. Understanding this process, called molecular docking, can be crucial in areas such as drug design. Recent learning-based attempts have shown promising results at this task, yet lack features that traditional approaches support. In this work, we close this gap by proposing DiffDock-Pocket, a diffusion-based docking algorithm that is conditioned on a binding target to predict ligand poses only in a specific binding pocket. On top of this, our model supports receptor flexibility and predicts the position of sidechains close to the binding site. Empirically, we improve the state-of-the-art in site-specific-docking on the PDBBind benchmark. Especially when using in-silico generated structures, we achieve more than twice the performance of current methods while being more than 20 times faster than other flexible approaches. Although the model was not trained for cross-docking to different structures, it yields competitive results in this task.",
         "['Computational Biology' 'Molecular Docking' 'Drug Design'\n 'Structural Bioinformatics' 'Machine Learning in Biology']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/74880.png"
        ],
        [
         "19",
         "ICLR",
         "2023",
         "11225",
         "Consolidator: Mergable Adapter with Group Connections for Visual Adaptation",
         "Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting a well-trained transformer to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every downstream task and thus easily falls into an overfitting situation, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of all parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose consolidator to achieve efficient transfer learning for large vision models. Our consolidator modifies the pre-trained model with the addition of a small set of tunable parameters to temporarily store the task-specific knowledge while freezing the backbone model during adaptation. Motivated by the success of group-wise convolution, we adopt grouped connections across the features extracted by fully connected layers to construct tunable parts in a consolidator. To further enhance the model's capacity to transfer knowledge under a constrained storage budget and keep inference efficient, we consolidate the parameters in two stages: 1. between adaptation and storage, and 2. between loading and inference. On a series of downstream visual tasks, our consolidator can reach up to 7.56 better accuracy than full fine-tuning with merely 0.35% parameters, and outperform state-of-the-art parameter-efficient tuning methods by a clear margin. Code is available at github.",
         "['Computer Vision' 'Transfer Learning' 'Vision Transformers'\n 'Model Optimization' 'Deep Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11225.png"
        ],
        [
         "20",
         "ICML",
         "2023",
         "24636",
         "Differentially Private Optimization on Large Model at Small Cost",
         "Differentially private (DP) optimization is the  standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2$\\sim$1000$\\times$ more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA) accuracy with very small extra cost: on GPT2 and at almost the same memory cost (<1% overhead), BK has 1.03$\\times$ the time complexity of the standard training (0.83$\\times$ training speed in practice), and 0.61$\\times$ the time complexity of the most efficient DP implementation (1.36$\\times$ training speed in practice). We open-source the codebase for the BK algorithm at \\url{https://github.com/awslabs/fast-differential-privacy}.",
         "['Differential Privacy' 'Optimization' 'Deep Learning' 'Neural Networks'\n 'Privacy-Preserving Machine Learning' 'Computational Efficiency'\n 'Machine Learning Algorithms']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24636.png"
        ],
        [
         "21",
         "NeurIPS",
         "2023",
         "70120",
         "Deep Equilibrium Based Neural Operators for Steady-State PDEs",
         "Data-driven machine learning approaches are being increasingly used to solve partial differential equations (PDEs). They have shown particularly striking successes when training an operator, which takes as input a PDE in some family, and outputs its solution. However, the architectural design space, especially given structural knowledge of the PDE family of interest, is still poorly understood. We seek to remedy this gap by studying the benefits of weight-tied neural network architectures for steady-state PDEs. To achieve this, we first demonstrate that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator. Motivated by this observation, we propose FNO-DEQ, a deep equilibrium variant of the FNO architecture that directly solves for the solution of a steady-state PDE as the infinite-depth fixed point of an implicit operator layer using a black-box root solver and differentiates analytically through this fixed point resulting in $\\mathcal{O}(1)$ training memory. Our experiments indicate that FNO-DEQ-based architectures outperform FNO-based baselines with $4\\times$ the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when trained with datasets with more noisy observations than the FNO-based baselines, demonstrating the benefits of using appropriate inductive biases in architectural design for different neural network based PDE solvers. Further, we show a universal approximation result that demonstrates that FNO-DEQ can approximate the solution to any steady-state PDE that can be written as a fixed point equation.",
         "['Neural Networks' 'Computational Mathematics'\n 'Partial Differential Equations ' 'Scientific Computing']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70120.png"
        ],
        [
         "22",
         "ICLR",
         "2024",
         "17592",
         "Learning invariant representations of time-homogeneous stochastic dynamical systems",
         "We consider the general class of time-homogeneous stochastic dynamical systems, both discrete and continuous, and study the problem of learning a representation of the state that faithfully captures its dynamics. This is instrumental to learning the transfer operator or the generator of the system, which in turn can be used for numerous tasks, such as forecasting and interpreting the system dynamics. We show that the search for a good representation can be cast as an optimization problem over neural networks. Our approach is supported by recent results in statistical learning theory, highlighting the role of approximation error and metric distortion in the learning problem. The objective function we propose is associated with projection operators from the representation space to the data space, overcomes metric distortion, and can be empirically estimated from data. In the discrete-time setting, we further derive a relaxed objective function that is differentiable and numerically well-conditioned. We compare our method against state-of-the-art approaches on different datasets, showing better performance across the board.",
         "['Dynamical Systems' 'Stochastic Processes' 'Representation Learning'\n 'Neural Networks' 'Statistical Learning Theory']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17592.png"
        ],
        [
         "23",
         "NeurIPS",
         "2023",
         "72656",
         "DAC-DETR: Divide the Attention Layers and Conquer",
         "This paper reveals a characteristic of DEtection Transformer (DETR) that negatively impacts its training efficacy, i.e., the cross-attention and self-attention layers in DETR decoder have contrary impacts on the object queries (though both impacts are important). Specifically, we observe the cross-attention tends to gather multiple queries around the same object, while the self-attention disperses these queries far away. To improve the training efficacy, we propose a Divide-And-Conquer DETR (DAC-DETR) that divides the cross-attention out from this contrary for better conquering. During training, DAC-DETR employs an auxiliary decoder that focuses on learning the cross-attention layers. The auxiliary decoder, while sharing all the other parameters, has NO self-attention layers and employs one-to-many label assignment to improve the gathering effect. Experiments show that DAC-DETR brings remarkable improvement over popular DETRs. For example, under the 12 epochs training scheme on MS-COCO, DAC-DETR improves Deformable DETR (ResNet-50) by +3.4 AP and achieves 50.9 (ResNet-50) / 58.1 AP (Swin-Large) based on some popular methods (i.e., DINO and an IoU-related loss). Our code will be made available at https://github.com/huzhengdongcs/DAC-DETR.",
         "['Computer Vision' 'Object Detection' 'Deep Learning' 'Transformer Models']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72656.png"
        ],
        [
         "24",
         "ICML",
         "2023",
         "23710",
         "Active causal structure learning with advice",
         "We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) $G^*$ while minimizing the number of interventions made. In our setting, we are additionally given side information about $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on _algorithms with predictions_. When the advice is a DAG $G$, we design an adaptive search algorithm to recover $G^*$ whose intervention cost is at most $\\mathcal{O}(\\max\\{1, \\log \\psi\\})$ times the cost for verifying $G^*$; here, $\\psi$ is a distance measure between $G$ and $G^*$ that is upper bounded by the number of variables $n$, and is exactly 0 when $G=G^*$. Our approximation factor matches the state-of-the-art for the advice-less setting.",
         "['Causal Inference' 'Graph Theory' 'Algorithms with Predictions'\n 'Computational Learning Theory']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/23710.png"
        ],
        [
         "25",
         "NeurIPS",
         "2022",
         "64147",
         "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation",
         "Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks:1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re$^2$ consistently outperforms strong baselines and achieves significant improvement over both its Deep RL and EA components.",
         "['Reinforcement Learning' 'Evolutionary Algorithms'\n 'Optimization Techniques']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/64147.png"
        ],
        [
         "26",
         "NeurIPS",
         "2023",
         "72903",
         "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces",
         "Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces.  This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use — via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.",
         "['Human-Computer Interaction' 'Computer Vision']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72903.png"
        ],
        [
         "27",
         "ICML",
         "2022",
         "17457",
         "Scalable Deep Reinforcement Learning Algorithms for Mean Field Games",
         "Mean Field Games (MFGs) have been introduced to efficiently approximate games with very large populations of strategic agents. Recently, the question of learning equilibria in MFGs has gained momentum, particularly using model-free reinforcement learning (RL) methods. One limiting factor to further scale up using RL is that existing algorithms to solve MFGs require the mixing of approximated quantities such as strategies or $q$-values. This is far from being trivial in the case of non-linear function approximation that enjoy good generalization properties, \\textit{e.g.} neural networks. We propose two methods to address this shortcoming. The first one learns a mixed strategy from distillation of historical data into a neural network and is applied to the Fictitious Play algorithm. The second one is an online mixing method based on regularization that does not require memorizing historical data or previous estimates. It is used to extend Online Mirror Descent. We demonstrate numerically that these methods efficiently enable the use of Deep RL algorithms to solve various MFGs. In addition, we show that these methods outperform SotA baselines from the literature.",
         "['Deep Reinforcement Learning' 'Mean Field Games' 'Game Theory'\n 'Machine Learning Algorithms' 'Neural Networks']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/3016a447172f3045b65f5fc83e04b554.png"
        ],
        [
         "28",
         "NeurIPS",
         "2022",
         "53762",
         "Trade-off between Payoff and Model Rewards in Shapley-Fair Collaborative Machine Learning",
         "This paper investigates the problem of fairly trading off between payoff and model rewards in collaborative machine learning (ML) where parties aggregate their datasets together to obtain improved ML models over that of each party. Supposing parties can afford the optimal model trained on the aggregated dataset, we propose an allocation scheme that distributes the payoff fairly. Notably, the same scheme can be derived from two different approaches based on (a) desirable properties of the parties' payoffs or (b) that of the underlying payoff flows from one party to another. While the former is conceptually simpler, the latter can be used to handle the practical constraint on the budgets of parties. In particular, we propose desirable properties for achieving a fair adjustment of the payoff flows that can trade off between the model reward's performance and the payoff reward. We empirically demonstrate that our proposed scheme is a sensible solution in several scenarios of collaborative ML with different budget constraints.",
         "['Collaborative Machine Learning' 'Fairness in Machine Learning'\n 'Game Theory in Machine Learning'\n 'Resource Allocation in Machine Learning']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53762.png"
        ],
        [
         "29",
         "ICLR",
         "2024",
         "18386",
         "From Zero to Turbulence: Generative Modeling for 3D Flow Simulation",
         "Simulations of turbulent flows in 3D are one of the most expensive simulations in computational fluid dynamics (CFD). Many works have been written on surrogate models to replace numerical solvers for fluid flows with faster, learned, autoregressive models. However, the intricacies of turbulence in three dimensions necessitate training these models with very small time steps, while generating realistic flow states requires either long roll-outs with many steps and significant error accumulation or starting from a known, realistic flow state—something we aimed to avoid in the first place. Instead, we propose to approach turbulent flow simulation as a generative task directly learning the manifold of all possible turbulent flow states without relying on any initial flow state. For our experiments, we introduce a challenging 3D turbulence dataset of high-resolution flows and detailed vortex structures caused by various objects and derive two novel sample evaluation metrics for turbulent flows. On this dataset, we show that our generative model captures the distribution of turbulent flows caused by unseen objects and generates high-quality, realistic samples amenable for downstream applications without access to any initial state.",
         "['Computational Fluid Dynamics ' 'Generative Modeling'\n 'Turbulence Simulation' 'Machine Learning in Fluid Dynamics'\n 'Surrogate Modeling' '3D Flow Simulation']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18386.png"
        ],
        [
         "30",
         "NeurIPS",
         "2023",
         "72558",
         "Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control",
         "The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \\textbf{T}rajectories-awar\\textbf{E} \\textbf{E}nsemble exploratio\\textbf{N} (TEEN). The primary goal of TEEN is to  maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble policy compared to using sub-policies alone but also improves the performance over ensemble RL algorithms. On average, TEEN outperforms the baseline ensemble DRL algorithms by 41\\% in performance on the tested representative environments.",
         "['Deep Reinforcement Learning' 'Ensemble Methods' 'Continuous Control'\n 'Machine Learning Algorithms'\n 'Exploration Strategies in Reinforcement Learning']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72558.png"
        ],
        [
         "31",
         "ICLR",
         "2024",
         "19022",
         "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes",
         "Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.",
         "['Reinforcement Learning' 'Multi-Agent Systems'\n 'Machine Learning Algorithms' 'Control Systems']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19022.png"
        ],
        [
         "32",
         "ICML",
         "2023",
         "23569",
         "Optimizing the Collaboration Structure in Cross-Silo Federated Learning",
         "In federated learning (FL), multiple clients collaborate to train machine learning models together while keeping their data decentralized. Through utilizing more training data, FL suffers from the potential negative transfer problem: the global FL model may even perform worse than the models trained with local data only. In this paper, we propose FedCollab, a novel FL framework that alleviates negative transfer by clustering clients into non-overlapping coalitions based on their distribution distances and data quantities. As a result, each client only collaborates with the clients having similar data distributions, and tends to collaborate with more clients when it has less data. We evaluate our framework with a variety of datasets, models, and types of non-IIDness. Our results demonstrate that FedCollab effectively mitigates negative transfer across a wide range of FL algorithms and consistently outperforms other clustered FL algorithms.",
         "['Federated Learning' 'Data Privacy' 'Distributed Systems'\n 'Collaborative Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/23569.png"
        ],
        [
         "33",
         "ICLR",
         "2024",
         "17668",
         "Neural SDF Flow for 3D Reconstruction of Dynamic Scenes",
         "In this paper, we tackle the problem of 3D reconstruction of dynamic scenes from multi-view videos. Previous dynamic scene reconstruction works either attempt to model the motion of 3D points in space, which constrains them to handle a single articulated object or require depth maps as input. By contrast, we propose to directly estimate the change of Signed Distance Function (SDF), namely SDFflow, of the dynamic scene. We show that the SDF flow captures the evolution of the scene surface. We further derive the mathematical relation between the SDF flow and the scene flow, which allows us to calculate the scene flow from the SDF flow analytically by solving linear equations. Our experiments on real-world multi-view video datasets show that our reconstructions are better than those of the state-of-the-art methods. Our code is available at https://github.com/wei-mao-2019/SDFFlow.git.",
         "['Computer Vision' '3D Reconstruction' 'Dynamic Scene Analysis'\n 'Neural Networks' 'Multi-View Geometry']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17668.png"
        ],
        [
         "34",
         "NeurIPS",
         "2022",
         "57822",
         "Transformers generalize differently from information stored in context vs in weights",
         "Transformer models can use two fundamentally different kinds of information: information stored in weights during training, and information provided ``in-context'' at inference time. In this work, we show that transformers exhibit different inductive biases in how they represent and generalize from the information in these two sources. In particular, we characterize whether they generalize via parsimonious rules (rule-based generalization) or via direct comparison with observed examples (exemplar-based generalization). This is of important practical consequence, as it informs whether to encode information in weights or in context, depending on how we want models to use that information. In transformers trained on controlled stimuli, we find that generalization from weights is more rule-based whereas generalization from context is largely exemplar-based. In contrast, we find that in transformers pre-trained on natural language, in-context learning is significantly rule-based, with larger models showing more rule-basedness. We hypothesise that rule-based generalization from in-context information might be an emergent consequence of large-scale training on language, which has sparse rule-like structure. Using controlled stimuli, we verify that transformers pretrained on data containing sparse rule-like structure exhibit more rule-based generalization.",
         "['Natural Language Processing' 'Deep Learning' 'Neural Networks']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/57822.png"
        ],
        [
         "35",
         "NeurIPS",
         "2023",
         "72650",
         "New Bounds for Hyperparameter Tuning of Regression Problems Across Instances",
         "The task of tuning regularization coefficients in regularized regression models with provable guarantees across problem instances still poses a significant challenge in the literature. This paper investigates the sample complexity of tuning regularization parameters in linear and logistic regressions under $\\ell_1$ and $\\ell_2$-constraints in the data-driven setting. For the linear regression problem, by more carefully exploiting the structure of the dual function class, we provide a new upper bound for the pseudo-dimension of the validation loss function class, which significantly improves the best-known results on the problem. Remarkably, we also instantiate the first matching lower bound, proving our results are tight. For tuning the regularization parameters of logistic regression, we introduce a new approach to studying the learning guarantee via an approximation of the validation loss function class. We examine the pseudo-dimension of the approximation class and construct a uniform error bound between the validation loss function class and its approximation, which allows us to instantiate the first learning guarantee for the problem of tuning logistic regression regularization coefficients.",
         "['Hyperparameter Optimization' 'Regression Analysis'\n 'Statistical Learning Theory']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72650.png"
        ],
        [
         "36",
         "ICLR",
         "2023",
         "11153",
         "How robust is unsupervised representation learning to distribution shift?",
         "The robustness of machine learning algorithms to distributions shift is primarily discussed in the context of supervised learning (SL). As such, there is a lack of insight on the robustness of the representations learned from unsupervised methods, such as self-supervised learning (SSL) and auto-encoder based algorithms (AE), to distribution shift. We posit that the input-driven objectives of unsupervised algorithms lead to representations that are more robust to distribution shift than the target-driven objective of SL. We verify this by extensively evaluating the performance of SSL and AE on both synthetic and realistic distribution shift datasets. Following observations that the linear layer used for classification itself can be susceptible to spurious correlations, we evaluate the representations using a linearhead trained on a small amount of out-of-distribution (OOD) data, to isolate the robustness of the learned representations from that of the linear head. We also develop “controllable” versions of existing realistic domain generalisation datasets with adjustable degrees of distribution shifts. This allows us to study the robustness of different learning algorithms under versatile yet realistic distribution shiftconditions. Our experiments show that representations learned from unsupervised learning algorithms generalise better than SL under a wide variety of extreme as well as realistic distribution shifts.",
         "['Unsupervised Learning' 'Representation Learning' 'Distribution Shift'\n 'Machine Learning Robustness' 'Self-Supervised Learning' 'Auto-Encoders'\n 'Domain Generalization']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11153.png"
        ],
        [
         "37",
         "NeurIPS",
         "2023",
         "70685",
         "Generator Born from Classifier",
         "In this paper, we make a bold attempt toward an ambitious task: given a pre-trained classifier, we aim to reconstruct an image generator, without relying on any data samples. From a black-box perspective, this challenge seems intractable, since it inevitably involves identifying the inverse function for a classifier, which is, by nature, an information extraction process. As such, we resort to leveraging the knowledge encapsulated within the parameters of the neural network. Grounded on the theory of Maximum-Margin Bias of gradient descent, we propose a novel learning paradigm, in which the generator is trained to ensure that the convergence conditions of the network parameters are satisfied over the generated distribution of the samples. Empirical validation from various image generation tasks substantiates the efficacy of our strategy.",
         "['Neural Networks' 'Image Generation' 'Deep Learning' 'Generative Models']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70685.png"
        ],
        [
         "38",
         "ICLR",
         "2023",
         "12160",
         "Generating Diverse Cooperative Agents by Learning Incompatible Policies",
         "Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task's goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility—which has been used exclusively as a measure of robustness—can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust.See our project page at https://bit.ly/marl-lipo",
         "['Multi-Agent Systems' 'Reinforcement Learning' 'Cooperative AI']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/12160.png"
        ],
        [
         "39",
         "ICLR",
         "2023",
         "10822",
         "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning",
         "Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states---resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion---failure to consider even one of many factors can result in unexpected, undesirable behavior.",
         "['Reinforcement Learning' 'Causal Inference' 'Preference-Based Learning'\n 'Reward Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/10822.png"
        ],
        [
         "40",
         "ICML",
         "2022",
         "16427",
         "Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows",
         "We study approximation of probability measures supported on n-dimensional manifolds embedded in R^m by injective flows---neural networks composed of invertible flows and injective layers. We show that in general, injective flows between R^n and R^m universally approximate measures supported on images of extendable embeddings, which are a subset of standard embeddings: when the embedding dimension m is small, topological obstructions may preclude certain manifolds as admissible targets. When the embedding dimension is sufficiently large, m >= 3n+1, we use an argument from algebraic topology known as the clean trick to prove that the topological obstructions vanish and injective flows universally approximate any differentiable embedding. Along the way we show that the studied injective flows admit efficient projections on the range, and that their optimality can be established \"in reverse,\" resolving a conjecture made in Brehmer & Cranmer 2020.",
         "['Neural Networks' 'Manifold Learning' 'Algebraic Topology'\n 'Probability Theory' 'Approximation Theory']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/87475f2c1909e4e6d0d7f0e020a2ded3.png"
        ],
        [
         "41",
         "NeurIPS",
         "2023",
         "76120",
         "Graph-Theoretical Approaches for AI-Driven Discovery in Quantum Optics",
         "Emerging findings in the physical sciences frequently present new avenues for AI applications that can enhance its efficiency or broaden its scope, as we demonstrated in our study on quantum optics. We present a method that represents quantum optics experiments as abstract weighted graphs, converting problems that encompass both continuous and discrete elements into purely continuous optimization tasks. This allows efficient use of both gradient-based and neural network methods, circumventing the need for workarounds due to the discrete nature of the problems. The new representation not only simplifies the design process but also facilitates a deeper understanding and interpretation of strategies derived from neural networks.",
         "['Quantum Optics' 'Graph Theory' 'Computational Physics'\n 'Optimization Techniques']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76120.png"
        ],
        [
         "42",
         "NeurIPS",
         "2022",
         "56960",
         "FO-PINNs: A First-Order formulation for Physics~Informed Neural Networks",
         "We present FO-PINNs, physics-informed neural networks that are trained using the first-order formulation of the Partial Differential Equation (PDE) losses. We show that FO-PINNs offer significantly higher accuracy in solving parameterized systems compared to traditional PINNs, and reduce time-per-iteration by removing the extra backpropagations needed to compute the second or higher-order derivatives. Additionally, unlike standard PINNs, FO-PINNs can be used with exact imposition of boundary conditions using approximate distance functions, and can be trained using Automatic Mixed Precision (AMP) to further speed up the training. Through two Helmholtz and Navier-Stokes examples, we demonstrate the advantages of FO-PINNs over traditional PINNs in terms of accuracy and training speedup. FO-PINN has been developed using Modulus framework by NVIDIA and the source code for this is available in https://developer.nvidia.com/modulus.",
         "['Computational Physics' 'Numerical Analysis' 'Scientific Computing'\n 'Neural Networks' 'Partial Differential Equations ']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/56960.png"
        ],
        [
         "43",
         "NeurIPS",
         "2022",
         "55747",
         "ComMU: Dataset for Combinatorial Music Generation",
         "Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU/).",
         "['Music Information Retrieval' 'Computational Creativity'\n 'Symbolic Music Generation' 'Artificial Intelligence in Music'\n 'Music Data and Metadata' 'Music Composition and Production']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55747.png"
        ],
        [
         "44",
         "ICML",
         "2023",
         "24487",
         "GC-Flow: A Graph-Based Flow Network for Effective Clustering",
         "Graph convolutional networks (GCNs) are *discriminative models* that directly model the class posterior $p(y|\\mathbf{x})$ for semi-supervised classification of graph data. While being effective, as a representation learning approach, the node representations extracted from a GCN often miss useful information for effective clustering, because the objectives are different. In this work, we design normalizing flows that replace GCN layers, leading to a *generative model* that models both the class conditional likelihood $p(\\mathbf{x}|y)$ and the class prior $p(y)$. The resulting neural network, GC-Flow, retains the graph convolution operations while being equipped with a Gaussian mixture representation space. It enjoys two benefits: it not only maintains the predictive power of GCN, but also produces well-separated clusters, due to the structuring of the representation space. We demonstrate these benefits on a variety of benchmark data sets. Moreover, we show that additional parameterization, such as that on the adjacency matrix used for graph convolutions, yields additional improvement in clustering.",
         "['Graph Neural Networks' 'Clustering' 'Generative Models'\n 'Representation Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24487.png"
        ],
        [
         "45",
         "ICML",
         "2024",
         "37259",
         "Differentiable Local Intrinsic Dimension Estimation with Diffusion Models",
         "High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. While many estimation techniques exist, they are all either inaccurate or do not scale. In this work, we show that the Fokker-Planck equation associated with a diffusion model can provide the first LID estimator which scales to high dimensional data while outperforming existing baselines on LID estimation benchmarks.",
         "['High-Dimensional Data Analysis' 'Intrinsic Dimension Estimation'\n 'Diffusion Models' 'Neural Networks' 'Data Science']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/37259.png"
        ],
        [
         "46",
         "NeurIPS",
         "2023",
         "76960",
         "Physics-informed DeepONet for battery state prediction",
         "Electrification has emerged as a pivotal trend in the energy transition to address climate change, leading to a substantial surge in the demand for batteries. Accurately predicting the internal states and performance of batteries assumes paramount significance, as it ensures the safe and stable operation of batteries and informs decision-making processes, such as optimizing battery operation for arbitrage opportunities. However, current models struggle to strike a balance between precision and computational efficiency or are limited in their applicability to specific scenarios. We aim to adopt a physics-informed deep operator network (PI-DeepONet) for internal battery state estimation based on the rigorous P2D model, which can simultaneously achieve high precision and computational efficiency. Furthermore, it exhibits promising prospects for extension beyond lithium-ion batteries to encompass various battery technologies.",
         "['Battery Technology' 'Computational Modeling'\n 'Machine Learning in Energy Systems' 'Physics-informed Machine Learning'\n 'Energy Storage Systems']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76960.png"
        ],
        [
         "47",
         "ICML",
         "2023",
         "24321",
         "Traversing Between Modes in Function Space for Fast Ensembling",
         "Deep ensemble is a simple yet powerful way to improve the performance of deep neural networks. Under this motivation, recent works on mode connectivity have shown that parameters of ensembles are connected by low-loss subspaces, and one can efficiently collect ensemble parameters in those subspaces. While this provides a way to efficiently train ensembles, for inference, multiple forward passes should still be executed using all the ensemble parameters, which often becomes a serious bottleneck for real-world deployment. In this work, we propose a novel framework to reduce such costs. Given a low-loss subspace connecting two modes of a neural network, we build an additional neural network that predicts the output of the original neural network evaluated at a certain point in the low-loss subspace. The additional neural network, which we call a ``bridge'', is a lightweight network that takes minimal features from the original network and predicts outputs for the low-loss subspace without forward passes through the original network. We empirically demonstrate that we can indeed train such bridge networks and significantly reduce inference costs with the help of bridge networks.",
         "['Deep Learning' 'Neural Networks' 'Model Ensembling' 'Mode Connectivity']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24321.png"
        ],
        [
         "48",
         "ICML",
         "2023",
         "24799",
         "Regret-Minimizing Double Oracle for Extensive-Form Games",
         "By incorporating regret minimization, double oracle methods have demonstrated rapid convergence to Nash Equilibrium (NE) in normal-form games and extensive-form games, through algorithms such as online double oracle (ODO) and extensive-form double oracle (XDO), respectively. In this study, we further examine the theoretical convergence rate and sample complexity of such regret minimization-based double oracle methods, utilizing a unified framework called Regret-Minimizing Double Oracle. Based on this framework, we extend ODO to extensive-form games and determine its sample complexity. Moreover, we demonstrate that the sample complexity of XDO can be exponential in the number of information sets $|S|$, owing to the exponentially decaying stopping threshold of restricted games. To solve this problem, we propose the Periodic Double Oracle (PDO) method, which has the lowest sample complexity among regret minimization-based double oracle methods, being only polynomial in $|S|$. Empirical evaluations on multiple poker and board games show that PDO achieves significantly faster convergence than previous double oracle algorithms and reaches a competitive level with state-of-the-art regret minimization methods.",
         "['Game Theory' 'Algorithmic Game Theory' 'Computational Complexity']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24799.png"
        ],
        [
         "49",
         "NeurIPS",
         "2023",
         "75166",
         "Exploring Practitioner Perspectives On Training Data Attribution Explanations",
         "Explainable AI (XAI) aims to provide insight into opaque model reasoning to humans and as such is an interdisciplinary field by nature. In this paper, we interviewed 10 practitioners to understand the possible usability of training data attribution (TDA) explanations and to explore the design space of such an approach. We confirmed that training data quality is often the most important factor for high model performance in practice and model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model and do not necessarily prioritise but are open to training data as a means of explanation. Within our participants, we found that TDA explanations are not well-known and therefore not used. We urge the community to focus on the utility of TDA techniques from the human-machine collaboration perspective and broaden the TDA evaluation to reflect common use cases in practice.",
         "['Explainable AI ' 'Human-Computer Interaction ' 'Data Science'\n 'Interdisciplinary Research']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/75166.png"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10305
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conference</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topics</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19205</td>\n",
       "      <td>A Fast and Provable Algorithm for Sparse Phase...</td>\n",
       "      <td>We study the sparse phase retrieval problem, w...</td>\n",
       "      <td>[Signal Processing, Computational Mathematics,...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19234</td>\n",
       "      <td>Understanding Augmentation-based Self-Supervis...</td>\n",
       "      <td>Data augmentation is critical to the empirical...</td>\n",
       "      <td>[Self-Supervised Learning, Representation Lear...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>71333</td>\n",
       "      <td>Regularized Behavior Cloning for Blocking the ...</td>\n",
       "      <td>For partially observable environments, imitati...</td>\n",
       "      <td>[Imitation Learning, Reinforcement Learning, P...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>72466</td>\n",
       "      <td>Koopman Kernel Regression</td>\n",
       "      <td>Many machine learning approaches for decision ...</td>\n",
       "      <td>[Reinforcement Learning, Dynamical Systems, Pr...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2023</td>\n",
       "      <td>25261</td>\n",
       "      <td>Learning GFlowNets From Partial Episodes For I...</td>\n",
       "      <td>Generative flow networks (GFlowNets) are a fam...</td>\n",
       "      <td>[Reinforcement Learning, Probabilistic Modelin...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202023/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10300</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>71191</td>\n",
       "      <td>MarioGPT: Open-Ended Text2Level Generation thr...</td>\n",
       "      <td>Procedural Content Generation (PCG) is a techn...</td>\n",
       "      <td>[Procedural Content Generation , Large Languag...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10301</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2023</td>\n",
       "      <td>24254</td>\n",
       "      <td>Towards Better Graph Representation Learning w...</td>\n",
       "      <td>Proposing an effective and flexible matrix to ...</td>\n",
       "      <td>[Graph Representation Learning, Graph Neural N...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202023/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10302</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2022</td>\n",
       "      <td>6409</td>\n",
       "      <td>Graph-Guided Network for Irregularly Sampled M...</td>\n",
       "      <td>In many domains, including healthcare, biology...</td>\n",
       "      <td>[Graph Neural Networks, Time Series Analysis, ...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202022/7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10303</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>71085</td>\n",
       "      <td>Empowering Convolutional Neural Nets with Meta...</td>\n",
       "      <td>ReLU networks have remained the default choice...</td>\n",
       "      <td>[Neural Networks, Computer Vision, Image Proce...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2024</td>\n",
       "      <td>34864</td>\n",
       "      <td>Boximator: Generating Rich and Controllable Mo...</td>\n",
       "      <td>Generating rich and controllable motion is a p...</td>\n",
       "      <td>[Video Synthesis, Motion Control, Computer Vis...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202024/3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10305 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conference  year  paper_id  \\\n",
       "0           ICLR  2024     19205   \n",
       "1           ICLR  2024     19234   \n",
       "2        NeurIPS  2023     71333   \n",
       "3        NeurIPS  2023     72466   \n",
       "4           ICML  2023     25261   \n",
       "...          ...   ...       ...   \n",
       "10300    NeurIPS  2023     71191   \n",
       "10301       ICML  2023     24254   \n",
       "10302       ICLR  2022      6409   \n",
       "10303    NeurIPS  2023     71085   \n",
       "10304       ICML  2024     34864   \n",
       "\n",
       "                                                   title  \\\n",
       "0      A Fast and Provable Algorithm for Sparse Phase...   \n",
       "1      Understanding Augmentation-based Self-Supervis...   \n",
       "2      Regularized Behavior Cloning for Blocking the ...   \n",
       "3                              Koopman Kernel Regression   \n",
       "4      Learning GFlowNets From Partial Episodes For I...   \n",
       "...                                                  ...   \n",
       "10300  MarioGPT: Open-Ended Text2Level Generation thr...   \n",
       "10301  Towards Better Graph Representation Learning w...   \n",
       "10302  Graph-Guided Network for Irregularly Sampled M...   \n",
       "10303  Empowering Convolutional Neural Nets with Meta...   \n",
       "10304  Boximator: Generating Rich and Controllable Mo...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      We study the sparse phase retrieval problem, w...   \n",
       "1      Data augmentation is critical to the empirical...   \n",
       "2      For partially observable environments, imitati...   \n",
       "3      Many machine learning approaches for decision ...   \n",
       "4      Generative flow networks (GFlowNets) are a fam...   \n",
       "...                                                  ...   \n",
       "10300  Procedural Content Generation (PCG) is a techn...   \n",
       "10301  Proposing an effective and flexible matrix to ...   \n",
       "10302  In many domains, including healthcare, biology...   \n",
       "10303  ReLU networks have remained the default choice...   \n",
       "10304  Generating rich and controllable motion is a p...   \n",
       "\n",
       "                                                  topics  \\\n",
       "0      [Signal Processing, Computational Mathematics,...   \n",
       "1      [Self-Supervised Learning, Representation Lear...   \n",
       "2      [Imitation Learning, Reinforcement Learning, P...   \n",
       "3      [Reinforcement Learning, Dynamical Systems, Pr...   \n",
       "4      [Reinforcement Learning, Probabilistic Modelin...   \n",
       "...                                                  ...   \n",
       "10300  [Procedural Content Generation , Large Languag...   \n",
       "10301  [Graph Representation Learning, Graph Neural N...   \n",
       "10302  [Graph Neural Networks, Time Series Analysis, ...   \n",
       "10303  [Neural Networks, Computer Vision, Image Proce...   \n",
       "10304  [Video Synthesis, Motion Control, Computer Vis...   \n",
       "\n",
       "                                               image_url  \n",
       "0      https://iclr.cc/media/PosterPDFs/ICLR%202024/1...  \n",
       "1      https://iclr.cc/media/PosterPDFs/ICLR%202024/1...  \n",
       "2      https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "3      https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "4      https://icml.cc/media/PosterPDFs/ICML%202023/2...  \n",
       "...                                                  ...  \n",
       "10300  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "10301  https://icml.cc/media/PosterPDFs/ICML%202023/2...  \n",
       "10302  https://iclr.cc/media/PosterPDFs/ICLR%202022/7...  \n",
       "10303  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "10304  https://icml.cc/media/PosterPDFs/ICML%202024/3...  \n",
       "\n",
       "[10305 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2828a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "save_dir = \"images\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "local_paths=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51060eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(df):\n",
    "    for url in df['image_url']:\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            filename = os.path.basename(parsed.path)\n",
    "\n",
    "            if not filename:\n",
    "                filename = 'image_' + str(len(local_paths)) + '.png'\n",
    "\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            local_paths.append(filepath)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Failed to download url: {e}')\n",
    "            local_paths.append(None)\n",
    "\n",
    "    df['local_image_paths'] = local_paths\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa0350f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "topics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "image_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "local_image_paths",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b5c0d00b-e33b-4a9a-865f-6ccd5ea3a2c7",
       "rows": [
        [
         "0",
         "ICLR",
         "2024",
         "19205",
         "A Fast and Provable Algorithm for Sparse Phase Retrieval",
         "We study the sparse phase retrieval problem, which seeks to recover a sparse signal from a limited set of magnitude-only measurements. In contrast to prevalent sparse phase retrieval algorithms that primarily use first-order methods, we propose an innovative second-order algorithm that employs a Newton-type method with hard thresholding. This algorithm overcomes the linear convergence limitations of first-order methods while preserving their hallmark per-iteration computational efficiency. We provide theoretical guarantees that our algorithm converges to the $s$-sparse ground truth signal $\\boldsymbol{x}^{\\natural} \\in \\mathbb{R}^n$ (up to a global sign) at a quadratic convergence rate after at most $O(\\log (\\Vert\\boldsymbol{x}^{\\natural} \\Vert /x_{\\min}^{\\natural}))$ iterations, using $\\Omega(s^2\\log n)$ Gaussian random samples. Numerical experiments show that our algorithm achieves a significantly faster convergence rate than state-of-the-art methods.",
         "['Signal Processing' 'Computational Mathematics' 'Optimization'\n 'Algorithms']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19205.png",
         "images/19205.png"
        ],
        [
         "1",
         "ICLR",
         "2024",
         "19234",
         "Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression",
         "Data augmentation is critical to the empirical success of modern self-supervised representation learning, such as contrastive learning and masked language modeling.However, a theoretical understanding of the exact role of the augmentation remains limited.Recent work has built the connection between self-supervised learning and the approximation of the top eigenspace of a graph Laplacian operator, suggesting that learning a linear probe atop such representation can be connected to RKHS regression.Building on this insight, this work delves into a statistical analysis of augmentation-based pretraining.Starting from the isometry property, a geometric characterization of the target function given by the augmentation, we disentangle the effects of the model and the augmentation,and prove two generalization bounds that are free of model complexity.Our first bound works for an arbitrary encoder, and it is the sum of an estimation error bound incurred by fitting a linear probe, and an approximation error bound by RKHS approximation.Our second bound specifically addresses the casewhere the encoder extracts the top-d eigenspace of a finite-sample-based approximation of the underlying RKHS.A key ingredient in our analysis is theaugmentation complexity,which we use to quantitatively compare different augmentations and analyze their impact on downstream performance.",
         "['Self-Supervised Learning' 'Representation Learning' 'Data Augmentation'\n 'Theoretical Analysis in Machine Learning'\n 'Reproducing Kernel Hilbert Space ' 'Statistical Learning Theory']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19234.png",
         "images/19234.png"
        ],
        [
         "2",
         "NeurIPS",
         "2023",
         "71333",
         "Regularized Behavior Cloning for Blocking the Leakage of Past Action Information",
         "For partially observable environments, imitation learning with observation histories (ILOH) assumes that control-relevant information is sufficiently captured in the observation histories for imitating the expert actions. In the offline setting wherethe agent is required to learn to imitate without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet effective method for imitation learning. However, when the information about the actions executed in the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own past actions. In this paper, we address this catastrophic failure by proposing a principled regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind our approach is to leverage the classical notion of conditional independence to mitigate the leakage. We compare different instances of our framework with natural choices of conditional independence metric and its estimator. The result of our comparison advocates the use of a particular kernel-based estimator for the conditional independence metric. We conduct an extensive set of experiments on benchmark datasets in order to assess the effectiveness of our regularization method. The experimental results show that our method significantly outperforms prior related approaches, highlighting its potential to successfully imitate expert actions when the past action information leaks into the observation histories.",
         "['Imitation Learning' 'Reinforcement Learning'\n 'Partially Observable Environments' 'Behavior Cloning'\n 'Regularization Techniques']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/71333.png",
         "images/71333.png"
        ],
        [
         "3",
         "NeurIPS",
         "2023",
         "72466",
         "Koopman Kernel Regression",
         "Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging.Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear time-invariant (LTI) ODEs, turning multi-step forecasts into sparse matrix multiplication.Though there exists a variety of learning approaches, they usually lack crucial learning-theoretic guarantees, making the behavior of the obtained models with increasing data and dimensionality unclear.We address the aforementioned by deriving a universal Koopman-invariant reproducing kernel Hilbert space (RKHS) that solely spans transformations into LTI dynamical systems. The resulting Koopman Kernel Regression (KKR) framework enables the use of statistical learning tools from function approximation for novel convergence results and generalization error bounds under weaker assumptions than existing work. Our experiments demonstrate superior forecasting performance compared to Koopman operator and sequential data predictors in RKHS.",
         "['Reinforcement Learning' 'Dynamical Systems' 'Predictive Modeling'\n 'Kernel Methods' 'Statistical Learning' 'Optimization' 'Control Theory']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72466.png",
         "images/72466.png"
        ],
        [
         "4",
         "ICML",
         "2023",
         "25261",
         "Learning GFlowNets From Partial Episodes For Improved Convergence And Stability",
         "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\\lambda$) algorithm in reinforcement learning, we introduce *subtrajectory balance* or SubTB($\\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.",
         "['Reinforcement Learning' 'Probabilistic Modeling'\n 'Algorithmic Development']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/25261.png",
         "images/25261.png"
        ],
        [
         "5",
         "ICML",
         "2024",
         "33124",
         "Viewing Transformers Through the Lens of Long Convolutions Layers",
         "Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of longrange layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal modifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our theory and experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies.",
         "['Deep Learning' 'Natural Language Processing' 'Transformer Models'\n 'Long-Range Dependencies' 'Neural Network Architectures']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/33124.png",
         "images/33124.png"
        ],
        [
         "6",
         "ICLR",
         "2022",
         "6644",
         "Towards Model Agnostic Federated Learning Using Knowledge Distillation",
         "Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data at hand. Knowledge distillation (KD) is the obvious tool of choice to design such protocols. However, surprisingly, we show that most natural KD-based federated learning protocols have poor performance.        To investigate this, we propose a new theoretical framework, Federated Kernel ridge regression, which can capture both model heterogeneity as well as data heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further validate our framework by analyzing and designing new protocols based on KD. Their performance on real world experiments using neural networks, though still unsatisfactory, closely matches our theoretical predictions.",
         "['Federated Learning' 'Knowledge Distillation' 'Model Agnostic Methods'\n 'Data Heterogeneity' 'Neural Networks']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202022/28267ab848bcf807b2ed53c3a8f8fc8a_Qg0wg4t.png",
         "images/28267ab848bcf807b2ed53c3a8f8fc8a_Qg0wg4t.png"
        ],
        [
         "7",
         "ICML",
         "2023",
         "24401",
         "Local Optimization Achieves Global Optimality in Multi-Agent Reinforcement Learning",
         "Policy optimization methods with function approximation are widely used in multi-agent reinforcement learning. However, it remains elusive how to design such algorithms with statistical guarantees. Leveraging a multi-agent performance difference lemma that characterizes the landscape of multi-agent policy optimization, we find that the localized action value function serves as an ideal descent direction for each local policy. Motivated by the observation, we present a multi-agent PPO algorithm in which the local policy of each agent is updated similarly to vanilla PPO. We prove that with standard regularity conditions on the Markov game and problem-dependent quantities, our algorithm converges to the globally optimal policy at a sublinear rate. We extend our algorithm to the off-policy setting and introduce pessimism to policy evaluation, which aligns with experiments. To our knowledge, this is the first provably convergent multi-agent PPO algorithm in cooperative Markov games.",
         "['Multi-Agent Reinforcement Learning' 'Policy Optimization'\n 'Function Approximation' 'Cooperative Markov Games'\n 'Algorithm Design and Analysis']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24401.png",
         "images/24401.png"
        ],
        [
         "8",
         "NeurIPS",
         "2023",
         "79608",
         "Grounding Code Generation with Input-Output Specifications",
         "Large language models (LLMs) have demonstrated significant potential in code generation. However, the code generated by these models occasionally deviates from the user's intended outcome, resulting in executable but incorrect code. To mitigate this issue, we propose Gift4Code, a novel approach for the instruction fine-tuning of LLMs specifically tailored for code generation. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program input-output specifications, is provided to the LLM to facilitate fine-tuning. We evaluated our approach on two challenging data science benchmarks, Arcade and DS-1000. Our results suggest that the method enhances the LLM's alignment with user intentions, reducing the incidence of executable but incorrect outputs.",
         "['Code Generation' 'Natural Language Processing' 'Software Engineering']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/79608.png",
         "images/79608.png"
        ],
        [
         "9",
         "ICLR",
         "2024",
         "18178",
         "REFACTOR: Learning to Extract Theorems from Proofs",
         "Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6\\% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems. Code can be found at https://github.com/jinpz/refactor.",
         "['Automated Theorem Proving' 'Formal Methods' 'Computational Mathematics']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18178.png",
         "images/18178.png"
        ],
        [
         "10",
         "ICLR",
         "2024",
         "20866",
         "Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel",
         "Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. However, to compress further text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations with current technology. We push semantic compression as low as 100 μbpp (up to 10,000× smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 μbpp level represents a soft limit on semantic compression at standard image resolutions.",
         "['Image Compression' 'Semantic Compression' 'Computer Vision'\n 'Natural Language Processing']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/20866.png",
         "images/20866.png"
        ],
        [
         "11",
         "NeurIPS",
         "2023",
         "69905",
         "Automated Classification of Model Errors on ImageNet",
         "While the ImageNet dataset has been driving computer vision research over the past decade, significant label noise and ambiguity have made top-1 accuracy an insufficient measure of further progress. To address this, new label-sets and evaluation protocols have been proposed for ImageNet showing that state-of-the-art models already achieve over 95% accuracy and shifting the focus on investigating why the remaining errors persist.Recent work in this direction employed a panel of experts to manually categorize all remaining classification errors for two selected models. However, this process is time-consuming, prone to inconsistencies, and requires trained experts, making it unsuitable for regular model evaluation thus limiting its utility. To overcome these limitations, we propose the first automated error classification framework, a valuable tool to study how modeling choices affect error distributions. We use our framework to comprehensively evaluate the error distribution of over 900 models. Perhaps surprisingly, we find that across model architectures, scales, and pre-training corpora, top-1 accuracy is a strong predictor for theportionof all error types. In particular, we observe that the portion of severe errors drops significantly with top-1 accuracy indicating that, while it underreports a model's true performance, it remains a valuable performance metric.We release all our code at https://github.com/eth-sri/automated-error-analysis.",
         "['Computer Vision' 'Image Classification' 'Model Evaluation'\n 'Error Analysis']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/69905.png",
         "images/69905.png"
        ],
        [
         "12",
         "ICLR",
         "2023",
         "11321",
         "Understanding Embodied Reference with Touch-Line Transformer",
         "We study embodied reference understanding, the task of locating referents using embodied gestural signals and language references. Human studies have revealed that, contrary to popular belief, objects referred to or pointed to do not lie on the elbow-wrist line, but rather on the so-called virtual touch line. Nevertheless, contemporary human pose representations lack the virtual touch line. To tackle this problem, we devise the touch-line Transformer: It takes as input tokenized visual and textual features and simultaneously predicts the referent’s bounding box and a touch-line vector. Leveraging this touch-line prior, we further devise a geometric consistency loss that promotes co-linearity between referents and touch lines. Using the touch line as gestural information dramatically improves model performances: Experiments on the YouRefIt dataset demonstrate that our method yields a +25.0% accuracy improvement under the 0.75 IoU criterion, hence closing 63.6% of the performance difference between models and humans. Furthermore, we computationally validate prior human studies by demonstrating that computational models more accurately locate referents when employing the virtual touch line than when using the elbow-wrist line.",
         "['Computer Vision' 'Natural Language Processing'\n 'Human-Computer Interaction' 'Robotics']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11321.png",
         "images/11321.png"
        ],
        [
         "13",
         "ICML",
         "2022",
         "17303",
         "Path-Gradient Estimators for Continuous Normalizing Flows",
         "Recent work has established a path-gradient estimator for simple variational Gaussian distributions and has argued that the path-gradient is particularly beneficial in the regime in which the variational distribution approaches the exact target distribution. In many applications, this regime can however not be reached by a simple Gaussian variational distribution. In this work, we overcome this crucial limitation by proposing a path-gradient estimator for the considerably more expressive variational family of continuous normalizing flows. We outline an efficient algorithm to calculate this estimator and establish its superior performance empirically.",
         "['Variational Inference' 'Normalizing Flows' 'Computational Statistics']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/7417744a2bac776fabe5a09b21c707a2.png",
         "images/7417744a2bac776fabe5a09b21c707a2.png"
        ],
        [
         "14",
         "ICLR",
         "2023",
         "10802",
         "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation",
         "Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge\". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on a public dataset nor it deploys a generative model at the server. We analyze convergence of FedHKD and conduct extensive experiments on visual datasets in a variety of scenarios, demonstrating that FedHKD provides significant improvement in both personalized as well as global model performance compared to state-of-the-art FL methods designed for heterogeneous data settings.",
         "['Federated Learning' 'Personalized Federated Learning'\n 'Knowledge Distillation' 'Data Heterogeneity' 'Model Convergence'\n 'Visual Data Analysis']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/10802.png",
         "images/10802.png"
        ],
        [
         "15",
         "ICLR",
         "2023",
         "11174",
         "ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion",
         "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",
         "['Knowledge Graphs' 'Data Mining' 'Graph Embedding']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11174.png",
         "images/11174.png"
        ],
        [
         "16",
         "ICML",
         "2024",
         "33187",
         "Layer-Aware Analysis of Catastrophic Overfitting: Revealing the Pseudo-Robust Shortcut Dependency",
         "Catastrophic overfitting (CO) presents a significant challenge in single-step adversarial training (AT), manifesting as highly distorted deep neural networks (DNNs) that are vulnerable to multi-step adversarial attacks. However, the underlying factors that lead to the distortion of decision boundaries remain unclear. In this work, we delve into the specific changes within different DNN layers and discover that during CO, the former layers are more susceptible, experiencing earlier and greater distortion, while the latter layers show relative insensitivity. Our analysis further reveals that this increased sensitivity in former layers stems from the formation of $\\textit{pseudo-robust shortcuts}$, which alone can impeccably defend against single-step adversarial attacks but bypass genuine-robust learning, resulting in distorted decision boundaries. Eliminating these shortcuts can partially restore robustness in DNNs from the CO state, thereby verifying that dependence on them triggers the occurrence of CO. This understanding motivates us to implement adaptive weight perturbations across different layers to hinder the generation of $\\textit{pseudo-robust shortcuts}$, consequently mitigating CO. Extensive experiments demonstrate that our proposed method, $\\textbf{L}$ayer-$\\textbf{A}$ware Adversarial Weight $\\textbf{P}$erturbation (LAP), can effectively prevent CO and further enhance robustness.",
         "['Deep Learning' 'Adversarial Machine Learning' 'Neural Networks'\n 'Robustness in Machine Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/33187.png",
         "images/33187.png"
        ],
        [
         "17",
         "NeurIPS",
         "2023",
         "72160",
         "On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\\epsilon$-Greedy Exploration",
         "This paper provides a theoretical understanding of deep Q-Network (DQN) with the $\\varepsilon$-greedy exploration in deep reinforcement learning.Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored.First, the exploration strategy is either impractical or ignored in the existing analysis.  Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training  the Q-network. However,the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the  practical setting of DQNs with $\\epsilon$-greedy policy. We prove an iterative procedure with decaying $\\epsilon$ converges to the optimal Q-value function geometrically. Moreover, a higher level of $\\epsilon$ values enlarges the region of convergence but slows down the convergence, while the opposite holds for a lower level of $\\epsilon$ values. Experiments justify our established theoretical insights on DQNs.",
         "['Deep Reinforcement Learning' 'Machine Learning Theory'\n 'Convergence Analysis' 'Sample Complexity Analysis']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72160.png",
         "images/72160.png"
        ],
        [
         "18",
         "NeurIPS",
         "2023",
         "74880",
         "DiffDock-Pocket: Diffusion for Pocket-Level Docking with Sidechain Flexibility",
         "When a small molecule binds to a protein, the 3D structure of the protein and its function change. Understanding this process, called molecular docking, can be crucial in areas such as drug design. Recent learning-based attempts have shown promising results at this task, yet lack features that traditional approaches support. In this work, we close this gap by proposing DiffDock-Pocket, a diffusion-based docking algorithm that is conditioned on a binding target to predict ligand poses only in a specific binding pocket. On top of this, our model supports receptor flexibility and predicts the position of sidechains close to the binding site. Empirically, we improve the state-of-the-art in site-specific-docking on the PDBBind benchmark. Especially when using in-silico generated structures, we achieve more than twice the performance of current methods while being more than 20 times faster than other flexible approaches. Although the model was not trained for cross-docking to different structures, it yields competitive results in this task.",
         "['Computational Biology' 'Molecular Docking' 'Drug Design'\n 'Structural Bioinformatics' 'Machine Learning in Biology']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/74880.png",
         "images/74880.png"
        ],
        [
         "19",
         "ICLR",
         "2023",
         "11225",
         "Consolidator: Mergable Adapter with Group Connections for Visual Adaptation",
         "Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting a well-trained transformer to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every downstream task and thus easily falls into an overfitting situation, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of all parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose consolidator to achieve efficient transfer learning for large vision models. Our consolidator modifies the pre-trained model with the addition of a small set of tunable parameters to temporarily store the task-specific knowledge while freezing the backbone model during adaptation. Motivated by the success of group-wise convolution, we adopt grouped connections across the features extracted by fully connected layers to construct tunable parts in a consolidator. To further enhance the model's capacity to transfer knowledge under a constrained storage budget and keep inference efficient, we consolidate the parameters in two stages: 1. between adaptation and storage, and 2. between loading and inference. On a series of downstream visual tasks, our consolidator can reach up to 7.56 better accuracy than full fine-tuning with merely 0.35% parameters, and outperform state-of-the-art parameter-efficient tuning methods by a clear margin. Code is available at github.",
         "['Computer Vision' 'Transfer Learning' 'Vision Transformers'\n 'Model Optimization' 'Deep Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11225.png",
         "images/11225.png"
        ],
        [
         "20",
         "ICML",
         "2023",
         "24636",
         "Differentially Private Optimization on Large Model at Small Cost",
         "Differentially private (DP) optimization is the  standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2$\\sim$1000$\\times$ more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA) accuracy with very small extra cost: on GPT2 and at almost the same memory cost (<1% overhead), BK has 1.03$\\times$ the time complexity of the standard training (0.83$\\times$ training speed in practice), and 0.61$\\times$ the time complexity of the most efficient DP implementation (1.36$\\times$ training speed in practice). We open-source the codebase for the BK algorithm at \\url{https://github.com/awslabs/fast-differential-privacy}.",
         "['Differential Privacy' 'Optimization' 'Deep Learning' 'Neural Networks'\n 'Privacy-Preserving Machine Learning' 'Computational Efficiency'\n 'Machine Learning Algorithms']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24636.png",
         "images/24636.png"
        ],
        [
         "21",
         "NeurIPS",
         "2023",
         "70120",
         "Deep Equilibrium Based Neural Operators for Steady-State PDEs",
         "Data-driven machine learning approaches are being increasingly used to solve partial differential equations (PDEs). They have shown particularly striking successes when training an operator, which takes as input a PDE in some family, and outputs its solution. However, the architectural design space, especially given structural knowledge of the PDE family of interest, is still poorly understood. We seek to remedy this gap by studying the benefits of weight-tied neural network architectures for steady-state PDEs. To achieve this, we first demonstrate that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator. Motivated by this observation, we propose FNO-DEQ, a deep equilibrium variant of the FNO architecture that directly solves for the solution of a steady-state PDE as the infinite-depth fixed point of an implicit operator layer using a black-box root solver and differentiates analytically through this fixed point resulting in $\\mathcal{O}(1)$ training memory. Our experiments indicate that FNO-DEQ-based architectures outperform FNO-based baselines with $4\\times$ the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when trained with datasets with more noisy observations than the FNO-based baselines, demonstrating the benefits of using appropriate inductive biases in architectural design for different neural network based PDE solvers. Further, we show a universal approximation result that demonstrates that FNO-DEQ can approximate the solution to any steady-state PDE that can be written as a fixed point equation.",
         "['Neural Networks' 'Computational Mathematics'\n 'Partial Differential Equations ' 'Scientific Computing']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70120.png",
         "images/70120.png"
        ],
        [
         "22",
         "ICLR",
         "2024",
         "17592",
         "Learning invariant representations of time-homogeneous stochastic dynamical systems",
         "We consider the general class of time-homogeneous stochastic dynamical systems, both discrete and continuous, and study the problem of learning a representation of the state that faithfully captures its dynamics. This is instrumental to learning the transfer operator or the generator of the system, which in turn can be used for numerous tasks, such as forecasting and interpreting the system dynamics. We show that the search for a good representation can be cast as an optimization problem over neural networks. Our approach is supported by recent results in statistical learning theory, highlighting the role of approximation error and metric distortion in the learning problem. The objective function we propose is associated with projection operators from the representation space to the data space, overcomes metric distortion, and can be empirically estimated from data. In the discrete-time setting, we further derive a relaxed objective function that is differentiable and numerically well-conditioned. We compare our method against state-of-the-art approaches on different datasets, showing better performance across the board.",
         "['Dynamical Systems' 'Stochastic Processes' 'Representation Learning'\n 'Neural Networks' 'Statistical Learning Theory']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17592.png",
         "images/17592.png"
        ],
        [
         "23",
         "NeurIPS",
         "2023",
         "72656",
         "DAC-DETR: Divide the Attention Layers and Conquer",
         "This paper reveals a characteristic of DEtection Transformer (DETR) that negatively impacts its training efficacy, i.e., the cross-attention and self-attention layers in DETR decoder have contrary impacts on the object queries (though both impacts are important). Specifically, we observe the cross-attention tends to gather multiple queries around the same object, while the self-attention disperses these queries far away. To improve the training efficacy, we propose a Divide-And-Conquer DETR (DAC-DETR) that divides the cross-attention out from this contrary for better conquering. During training, DAC-DETR employs an auxiliary decoder that focuses on learning the cross-attention layers. The auxiliary decoder, while sharing all the other parameters, has NO self-attention layers and employs one-to-many label assignment to improve the gathering effect. Experiments show that DAC-DETR brings remarkable improvement over popular DETRs. For example, under the 12 epochs training scheme on MS-COCO, DAC-DETR improves Deformable DETR (ResNet-50) by +3.4 AP and achieves 50.9 (ResNet-50) / 58.1 AP (Swin-Large) based on some popular methods (i.e., DINO and an IoU-related loss). Our code will be made available at https://github.com/huzhengdongcs/DAC-DETR.",
         "['Computer Vision' 'Object Detection' 'Deep Learning' 'Transformer Models']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72656.png",
         "images/72656.png"
        ],
        [
         "24",
         "ICML",
         "2023",
         "23710",
         "Active causal structure learning with advice",
         "We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) $G^*$ while minimizing the number of interventions made. In our setting, we are additionally given side information about $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on _algorithms with predictions_. When the advice is a DAG $G$, we design an adaptive search algorithm to recover $G^*$ whose intervention cost is at most $\\mathcal{O}(\\max\\{1, \\log \\psi\\})$ times the cost for verifying $G^*$; here, $\\psi$ is a distance measure between $G$ and $G^*$ that is upper bounded by the number of variables $n$, and is exactly 0 when $G=G^*$. Our approximation factor matches the state-of-the-art for the advice-less setting.",
         "['Causal Inference' 'Graph Theory' 'Algorithms with Predictions'\n 'Computational Learning Theory']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/23710.png",
         "images/23710.png"
        ],
        [
         "25",
         "NeurIPS",
         "2022",
         "64147",
         "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation",
         "Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks:1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re$^2$ consistently outperforms strong baselines and achieves significant improvement over both its Deep RL and EA components.",
         "['Reinforcement Learning' 'Evolutionary Algorithms'\n 'Optimization Techniques']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/64147.png",
         "images/64147.png"
        ],
        [
         "26",
         "NeurIPS",
         "2023",
         "72903",
         "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces",
         "Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces.  This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use — via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.",
         "['Human-Computer Interaction' 'Computer Vision']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72903.png",
         "images/72903.png"
        ],
        [
         "27",
         "ICML",
         "2022",
         "17457",
         "Scalable Deep Reinforcement Learning Algorithms for Mean Field Games",
         "Mean Field Games (MFGs) have been introduced to efficiently approximate games with very large populations of strategic agents. Recently, the question of learning equilibria in MFGs has gained momentum, particularly using model-free reinforcement learning (RL) methods. One limiting factor to further scale up using RL is that existing algorithms to solve MFGs require the mixing of approximated quantities such as strategies or $q$-values. This is far from being trivial in the case of non-linear function approximation that enjoy good generalization properties, \\textit{e.g.} neural networks. We propose two methods to address this shortcoming. The first one learns a mixed strategy from distillation of historical data into a neural network and is applied to the Fictitious Play algorithm. The second one is an online mixing method based on regularization that does not require memorizing historical data or previous estimates. It is used to extend Online Mirror Descent. We demonstrate numerically that these methods efficiently enable the use of Deep RL algorithms to solve various MFGs. In addition, we show that these methods outperform SotA baselines from the literature.",
         "['Deep Reinforcement Learning' 'Mean Field Games' 'Game Theory'\n 'Machine Learning Algorithms' 'Neural Networks']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/3016a447172f3045b65f5fc83e04b554.png",
         "images/3016a447172f3045b65f5fc83e04b554.png"
        ],
        [
         "28",
         "NeurIPS",
         "2022",
         "53762",
         "Trade-off between Payoff and Model Rewards in Shapley-Fair Collaborative Machine Learning",
         "This paper investigates the problem of fairly trading off between payoff and model rewards in collaborative machine learning (ML) where parties aggregate their datasets together to obtain improved ML models over that of each party. Supposing parties can afford the optimal model trained on the aggregated dataset, we propose an allocation scheme that distributes the payoff fairly. Notably, the same scheme can be derived from two different approaches based on (a) desirable properties of the parties' payoffs or (b) that of the underlying payoff flows from one party to another. While the former is conceptually simpler, the latter can be used to handle the practical constraint on the budgets of parties. In particular, we propose desirable properties for achieving a fair adjustment of the payoff flows that can trade off between the model reward's performance and the payoff reward. We empirically demonstrate that our proposed scheme is a sensible solution in several scenarios of collaborative ML with different budget constraints.",
         "['Collaborative Machine Learning' 'Fairness in Machine Learning'\n 'Game Theory in Machine Learning'\n 'Resource Allocation in Machine Learning']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53762.png",
         "images/53762.png"
        ],
        [
         "29",
         "ICLR",
         "2024",
         "18386",
         "From Zero to Turbulence: Generative Modeling for 3D Flow Simulation",
         "Simulations of turbulent flows in 3D are one of the most expensive simulations in computational fluid dynamics (CFD). Many works have been written on surrogate models to replace numerical solvers for fluid flows with faster, learned, autoregressive models. However, the intricacies of turbulence in three dimensions necessitate training these models with very small time steps, while generating realistic flow states requires either long roll-outs with many steps and significant error accumulation or starting from a known, realistic flow state—something we aimed to avoid in the first place. Instead, we propose to approach turbulent flow simulation as a generative task directly learning the manifold of all possible turbulent flow states without relying on any initial flow state. For our experiments, we introduce a challenging 3D turbulence dataset of high-resolution flows and detailed vortex structures caused by various objects and derive two novel sample evaluation metrics for turbulent flows. On this dataset, we show that our generative model captures the distribution of turbulent flows caused by unseen objects and generates high-quality, realistic samples amenable for downstream applications without access to any initial state.",
         "['Computational Fluid Dynamics ' 'Generative Modeling'\n 'Turbulence Simulation' 'Machine Learning in Fluid Dynamics'\n 'Surrogate Modeling' '3D Flow Simulation']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18386.png",
         "images/18386.png"
        ],
        [
         "30",
         "NeurIPS",
         "2023",
         "72558",
         "Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control",
         "The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \\textbf{T}rajectories-awar\\textbf{E} \\textbf{E}nsemble exploratio\\textbf{N} (TEEN). The primary goal of TEEN is to  maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble policy compared to using sub-policies alone but also improves the performance over ensemble RL algorithms. On average, TEEN outperforms the baseline ensemble DRL algorithms by 41\\% in performance on the tested representative environments.",
         "['Deep Reinforcement Learning' 'Ensemble Methods' 'Continuous Control'\n 'Machine Learning Algorithms'\n 'Exploration Strategies in Reinforcement Learning']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72558.png",
         "images/72558.png"
        ],
        [
         "31",
         "ICLR",
         "2024",
         "19022",
         "REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes",
         "Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.",
         "['Reinforcement Learning' 'Multi-Agent Systems'\n 'Machine Learning Algorithms' 'Control Systems']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19022.png",
         "images/19022.png"
        ],
        [
         "32",
         "ICML",
         "2023",
         "23569",
         "Optimizing the Collaboration Structure in Cross-Silo Federated Learning",
         "In federated learning (FL), multiple clients collaborate to train machine learning models together while keeping their data decentralized. Through utilizing more training data, FL suffers from the potential negative transfer problem: the global FL model may even perform worse than the models trained with local data only. In this paper, we propose FedCollab, a novel FL framework that alleviates negative transfer by clustering clients into non-overlapping coalitions based on their distribution distances and data quantities. As a result, each client only collaborates with the clients having similar data distributions, and tends to collaborate with more clients when it has less data. We evaluate our framework with a variety of datasets, models, and types of non-IIDness. Our results demonstrate that FedCollab effectively mitigates negative transfer across a wide range of FL algorithms and consistently outperforms other clustered FL algorithms.",
         "['Federated Learning' 'Data Privacy' 'Distributed Systems'\n 'Collaborative Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/23569.png",
         "images/23569.png"
        ],
        [
         "33",
         "ICLR",
         "2024",
         "17668",
         "Neural SDF Flow for 3D Reconstruction of Dynamic Scenes",
         "In this paper, we tackle the problem of 3D reconstruction of dynamic scenes from multi-view videos. Previous dynamic scene reconstruction works either attempt to model the motion of 3D points in space, which constrains them to handle a single articulated object or require depth maps as input. By contrast, we propose to directly estimate the change of Signed Distance Function (SDF), namely SDFflow, of the dynamic scene. We show that the SDF flow captures the evolution of the scene surface. We further derive the mathematical relation between the SDF flow and the scene flow, which allows us to calculate the scene flow from the SDF flow analytically by solving linear equations. Our experiments on real-world multi-view video datasets show that our reconstructions are better than those of the state-of-the-art methods. Our code is available at https://github.com/wei-mao-2019/SDFFlow.git.",
         "['Computer Vision' '3D Reconstruction' 'Dynamic Scene Analysis'\n 'Neural Networks' 'Multi-View Geometry']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17668.png",
         "images/17668.png"
        ],
        [
         "34",
         "NeurIPS",
         "2022",
         "57822",
         "Transformers generalize differently from information stored in context vs in weights",
         "Transformer models can use two fundamentally different kinds of information: information stored in weights during training, and information provided ``in-context'' at inference time. In this work, we show that transformers exhibit different inductive biases in how they represent and generalize from the information in these two sources. In particular, we characterize whether they generalize via parsimonious rules (rule-based generalization) or via direct comparison with observed examples (exemplar-based generalization). This is of important practical consequence, as it informs whether to encode information in weights or in context, depending on how we want models to use that information. In transformers trained on controlled stimuli, we find that generalization from weights is more rule-based whereas generalization from context is largely exemplar-based. In contrast, we find that in transformers pre-trained on natural language, in-context learning is significantly rule-based, with larger models showing more rule-basedness. We hypothesise that rule-based generalization from in-context information might be an emergent consequence of large-scale training on language, which has sparse rule-like structure. Using controlled stimuli, we verify that transformers pretrained on data containing sparse rule-like structure exhibit more rule-based generalization.",
         "['Natural Language Processing' 'Deep Learning' 'Neural Networks']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/57822.png",
         "images/57822.png"
        ],
        [
         "35",
         "NeurIPS",
         "2023",
         "72650",
         "New Bounds for Hyperparameter Tuning of Regression Problems Across Instances",
         "The task of tuning regularization coefficients in regularized regression models with provable guarantees across problem instances still poses a significant challenge in the literature. This paper investigates the sample complexity of tuning regularization parameters in linear and logistic regressions under $\\ell_1$ and $\\ell_2$-constraints in the data-driven setting. For the linear regression problem, by more carefully exploiting the structure of the dual function class, we provide a new upper bound for the pseudo-dimension of the validation loss function class, which significantly improves the best-known results on the problem. Remarkably, we also instantiate the first matching lower bound, proving our results are tight. For tuning the regularization parameters of logistic regression, we introduce a new approach to studying the learning guarantee via an approximation of the validation loss function class. We examine the pseudo-dimension of the approximation class and construct a uniform error bound between the validation loss function class and its approximation, which allows us to instantiate the first learning guarantee for the problem of tuning logistic regression regularization coefficients.",
         "['Hyperparameter Optimization' 'Regression Analysis'\n 'Statistical Learning Theory']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/72650.png",
         "images/72650.png"
        ],
        [
         "36",
         "ICLR",
         "2023",
         "11153",
         "How robust is unsupervised representation learning to distribution shift?",
         "The robustness of machine learning algorithms to distributions shift is primarily discussed in the context of supervised learning (SL). As such, there is a lack of insight on the robustness of the representations learned from unsupervised methods, such as self-supervised learning (SSL) and auto-encoder based algorithms (AE), to distribution shift. We posit that the input-driven objectives of unsupervised algorithms lead to representations that are more robust to distribution shift than the target-driven objective of SL. We verify this by extensively evaluating the performance of SSL and AE on both synthetic and realistic distribution shift datasets. Following observations that the linear layer used for classification itself can be susceptible to spurious correlations, we evaluate the representations using a linearhead trained on a small amount of out-of-distribution (OOD) data, to isolate the robustness of the learned representations from that of the linear head. We also develop “controllable” versions of existing realistic domain generalisation datasets with adjustable degrees of distribution shifts. This allows us to study the robustness of different learning algorithms under versatile yet realistic distribution shiftconditions. Our experiments show that representations learned from unsupervised learning algorithms generalise better than SL under a wide variety of extreme as well as realistic distribution shifts.",
         "['Unsupervised Learning' 'Representation Learning' 'Distribution Shift'\n 'Machine Learning Robustness' 'Self-Supervised Learning' 'Auto-Encoders'\n 'Domain Generalization']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11153.png",
         "images/11153.png"
        ],
        [
         "37",
         "NeurIPS",
         "2023",
         "70685",
         "Generator Born from Classifier",
         "In this paper, we make a bold attempt toward an ambitious task: given a pre-trained classifier, we aim to reconstruct an image generator, without relying on any data samples. From a black-box perspective, this challenge seems intractable, since it inevitably involves identifying the inverse function for a classifier, which is, by nature, an information extraction process. As such, we resort to leveraging the knowledge encapsulated within the parameters of the neural network. Grounded on the theory of Maximum-Margin Bias of gradient descent, we propose a novel learning paradigm, in which the generator is trained to ensure that the convergence conditions of the network parameters are satisfied over the generated distribution of the samples. Empirical validation from various image generation tasks substantiates the efficacy of our strategy.",
         "['Neural Networks' 'Image Generation' 'Deep Learning' 'Generative Models']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70685.png",
         "images/70685.png"
        ],
        [
         "38",
         "ICLR",
         "2023",
         "12160",
         "Generating Diverse Cooperative Agents by Learning Incompatible Policies",
         "Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task's goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility—which has been used exclusively as a measure of robustness—can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust.See our project page at https://bit.ly/marl-lipo",
         "['Multi-Agent Systems' 'Reinforcement Learning' 'Cooperative AI']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/12160.png",
         "images/12160.png"
        ],
        [
         "39",
         "ICLR",
         "2023",
         "10822",
         "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning",
         "Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states---resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion---failure to consider even one of many factors can result in unexpected, undesirable behavior.",
         "['Reinforcement Learning' 'Causal Inference' 'Preference-Based Learning'\n 'Reward Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/10822.png",
         "images/10822.png"
        ],
        [
         "40",
         "ICML",
         "2022",
         "16427",
         "Universal Joint Approximation of Manifolds and Densities by Simple Injective Flows",
         "We study approximation of probability measures supported on n-dimensional manifolds embedded in R^m by injective flows---neural networks composed of invertible flows and injective layers. We show that in general, injective flows between R^n and R^m universally approximate measures supported on images of extendable embeddings, which are a subset of standard embeddings: when the embedding dimension m is small, topological obstructions may preclude certain manifolds as admissible targets. When the embedding dimension is sufficiently large, m >= 3n+1, we use an argument from algebraic topology known as the clean trick to prove that the topological obstructions vanish and injective flows universally approximate any differentiable embedding. Along the way we show that the studied injective flows admit efficient projections on the range, and that their optimality can be established \"in reverse,\" resolving a conjecture made in Brehmer & Cranmer 2020.",
         "['Neural Networks' 'Manifold Learning' 'Algebraic Topology'\n 'Probability Theory' 'Approximation Theory']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/87475f2c1909e4e6d0d7f0e020a2ded3.png",
         "images/87475f2c1909e4e6d0d7f0e020a2ded3.png"
        ],
        [
         "41",
         "NeurIPS",
         "2023",
         "76120",
         "Graph-Theoretical Approaches for AI-Driven Discovery in Quantum Optics",
         "Emerging findings in the physical sciences frequently present new avenues for AI applications that can enhance its efficiency or broaden its scope, as we demonstrated in our study on quantum optics. We present a method that represents quantum optics experiments as abstract weighted graphs, converting problems that encompass both continuous and discrete elements into purely continuous optimization tasks. This allows efficient use of both gradient-based and neural network methods, circumventing the need for workarounds due to the discrete nature of the problems. The new representation not only simplifies the design process but also facilitates a deeper understanding and interpretation of strategies derived from neural networks.",
         "['Quantum Optics' 'Graph Theory' 'Computational Physics'\n 'Optimization Techniques']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76120.png",
         "images/76120.png"
        ],
        [
         "42",
         "NeurIPS",
         "2022",
         "56960",
         "FO-PINNs: A First-Order formulation for Physics~Informed Neural Networks",
         "We present FO-PINNs, physics-informed neural networks that are trained using the first-order formulation of the Partial Differential Equation (PDE) losses. We show that FO-PINNs offer significantly higher accuracy in solving parameterized systems compared to traditional PINNs, and reduce time-per-iteration by removing the extra backpropagations needed to compute the second or higher-order derivatives. Additionally, unlike standard PINNs, FO-PINNs can be used with exact imposition of boundary conditions using approximate distance functions, and can be trained using Automatic Mixed Precision (AMP) to further speed up the training. Through two Helmholtz and Navier-Stokes examples, we demonstrate the advantages of FO-PINNs over traditional PINNs in terms of accuracy and training speedup. FO-PINN has been developed using Modulus framework by NVIDIA and the source code for this is available in https://developer.nvidia.com/modulus.",
         "['Computational Physics' 'Numerical Analysis' 'Scientific Computing'\n 'Neural Networks' 'Partial Differential Equations ']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/56960.png",
         "images/56960.png"
        ],
        [
         "43",
         "NeurIPS",
         "2022",
         "55747",
         "ComMU: Dataset for Combinatorial Music Generation",
         "Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU/).",
         "['Music Information Retrieval' 'Computational Creativity'\n 'Symbolic Music Generation' 'Artificial Intelligence in Music'\n 'Music Data and Metadata' 'Music Composition and Production']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/55747.png",
         "images/55747.png"
        ],
        [
         "44",
         "ICML",
         "2023",
         "24487",
         "GC-Flow: A Graph-Based Flow Network for Effective Clustering",
         "Graph convolutional networks (GCNs) are *discriminative models* that directly model the class posterior $p(y|\\mathbf{x})$ for semi-supervised classification of graph data. While being effective, as a representation learning approach, the node representations extracted from a GCN often miss useful information for effective clustering, because the objectives are different. In this work, we design normalizing flows that replace GCN layers, leading to a *generative model* that models both the class conditional likelihood $p(\\mathbf{x}|y)$ and the class prior $p(y)$. The resulting neural network, GC-Flow, retains the graph convolution operations while being equipped with a Gaussian mixture representation space. It enjoys two benefits: it not only maintains the predictive power of GCN, but also produces well-separated clusters, due to the structuring of the representation space. We demonstrate these benefits on a variety of benchmark data sets. Moreover, we show that additional parameterization, such as that on the adjacency matrix used for graph convolutions, yields additional improvement in clustering.",
         "['Graph Neural Networks' 'Clustering' 'Generative Models'\n 'Representation Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24487.png",
         "images/24487.png"
        ],
        [
         "45",
         "ICML",
         "2024",
         "37259",
         "Differentiable Local Intrinsic Dimension Estimation with Diffusion Models",
         "High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. While many estimation techniques exist, they are all either inaccurate or do not scale. In this work, we show that the Fokker-Planck equation associated with a diffusion model can provide the first LID estimator which scales to high dimensional data while outperforming existing baselines on LID estimation benchmarks.",
         "['High-Dimensional Data Analysis' 'Intrinsic Dimension Estimation'\n 'Diffusion Models' 'Neural Networks' 'Data Science']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/37259.png",
         "images/37259.png"
        ],
        [
         "46",
         "NeurIPS",
         "2023",
         "76960",
         "Physics-informed DeepONet for battery state prediction",
         "Electrification has emerged as a pivotal trend in the energy transition to address climate change, leading to a substantial surge in the demand for batteries. Accurately predicting the internal states and performance of batteries assumes paramount significance, as it ensures the safe and stable operation of batteries and informs decision-making processes, such as optimizing battery operation for arbitrage opportunities. However, current models struggle to strike a balance between precision and computational efficiency or are limited in their applicability to specific scenarios. We aim to adopt a physics-informed deep operator network (PI-DeepONet) for internal battery state estimation based on the rigorous P2D model, which can simultaneously achieve high precision and computational efficiency. Furthermore, it exhibits promising prospects for extension beyond lithium-ion batteries to encompass various battery technologies.",
         "['Battery Technology' 'Computational Modeling'\n 'Machine Learning in Energy Systems' 'Physics-informed Machine Learning'\n 'Energy Storage Systems']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76960.png",
         "images/76960.png"
        ],
        [
         "47",
         "ICML",
         "2023",
         "24321",
         "Traversing Between Modes in Function Space for Fast Ensembling",
         "Deep ensemble is a simple yet powerful way to improve the performance of deep neural networks. Under this motivation, recent works on mode connectivity have shown that parameters of ensembles are connected by low-loss subspaces, and one can efficiently collect ensemble parameters in those subspaces. While this provides a way to efficiently train ensembles, for inference, multiple forward passes should still be executed using all the ensemble parameters, which often becomes a serious bottleneck for real-world deployment. In this work, we propose a novel framework to reduce such costs. Given a low-loss subspace connecting two modes of a neural network, we build an additional neural network that predicts the output of the original neural network evaluated at a certain point in the low-loss subspace. The additional neural network, which we call a ``bridge'', is a lightweight network that takes minimal features from the original network and predicts outputs for the low-loss subspace without forward passes through the original network. We empirically demonstrate that we can indeed train such bridge networks and significantly reduce inference costs with the help of bridge networks.",
         "['Deep Learning' 'Neural Networks' 'Model Ensembling' 'Mode Connectivity']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24321.png",
         "images/24321.png"
        ],
        [
         "48",
         "ICML",
         "2023",
         "24799",
         "Regret-Minimizing Double Oracle for Extensive-Form Games",
         "By incorporating regret minimization, double oracle methods have demonstrated rapid convergence to Nash Equilibrium (NE) in normal-form games and extensive-form games, through algorithms such as online double oracle (ODO) and extensive-form double oracle (XDO), respectively. In this study, we further examine the theoretical convergence rate and sample complexity of such regret minimization-based double oracle methods, utilizing a unified framework called Regret-Minimizing Double Oracle. Based on this framework, we extend ODO to extensive-form games and determine its sample complexity. Moreover, we demonstrate that the sample complexity of XDO can be exponential in the number of information sets $|S|$, owing to the exponentially decaying stopping threshold of restricted games. To solve this problem, we propose the Periodic Double Oracle (PDO) method, which has the lowest sample complexity among regret minimization-based double oracle methods, being only polynomial in $|S|$. Empirical evaluations on multiple poker and board games show that PDO achieves significantly faster convergence than previous double oracle algorithms and reaches a competitive level with state-of-the-art regret minimization methods.",
         "['Game Theory' 'Algorithmic Game Theory' 'Computational Complexity']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/24799.png",
         "images/24799.png"
        ],
        [
         "49",
         "NeurIPS",
         "2023",
         "75166",
         "Exploring Practitioner Perspectives On Training Data Attribution Explanations",
         "Explainable AI (XAI) aims to provide insight into opaque model reasoning to humans and as such is an interdisciplinary field by nature. In this paper, we interviewed 10 practitioners to understand the possible usability of training data attribution (TDA) explanations and to explore the design space of such an approach. We confirmed that training data quality is often the most important factor for high model performance in practice and model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model and do not necessarily prioritise but are open to training data as a means of explanation. Within our participants, we found that TDA explanations are not well-known and therefore not used. We urge the community to focus on the utility of TDA techniques from the human-machine collaboration perspective and broaden the TDA evaluation to reflect common use cases in practice.",
         "['Explainable AI ' 'Human-Computer Interaction ' 'Data Science'\n 'Interdisciplinary Research']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/75166.png",
         "images/75166.png"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 10305
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conference</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topics</th>\n",
       "      <th>image_url</th>\n",
       "      <th>local_image_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19205</td>\n",
       "      <td>A Fast and Provable Algorithm for Sparse Phase...</td>\n",
       "      <td>We study the sparse phase retrieval problem, w...</td>\n",
       "      <td>[Signal Processing, Computational Mathematics,...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "      <td>images/19205.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19234</td>\n",
       "      <td>Understanding Augmentation-based Self-Supervis...</td>\n",
       "      <td>Data augmentation is critical to the empirical...</td>\n",
       "      <td>[Self-Supervised Learning, Representation Lear...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "      <td>images/19234.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>71333</td>\n",
       "      <td>Regularized Behavior Cloning for Blocking the ...</td>\n",
       "      <td>For partially observable environments, imitati...</td>\n",
       "      <td>[Imitation Learning, Reinforcement Learning, P...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/71333.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>72466</td>\n",
       "      <td>Koopman Kernel Regression</td>\n",
       "      <td>Many machine learning approaches for decision ...</td>\n",
       "      <td>[Reinforcement Learning, Dynamical Systems, Pr...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/72466.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2023</td>\n",
       "      <td>25261</td>\n",
       "      <td>Learning GFlowNets From Partial Episodes For I...</td>\n",
       "      <td>Generative flow networks (GFlowNets) are a fam...</td>\n",
       "      <td>[Reinforcement Learning, Probabilistic Modelin...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202023/2...</td>\n",
       "      <td>images/25261.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10300</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>71191</td>\n",
       "      <td>MarioGPT: Open-Ended Text2Level Generation thr...</td>\n",
       "      <td>Procedural Content Generation (PCG) is a techn...</td>\n",
       "      <td>[Procedural Content Generation , Large Languag...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/71191.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10301</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2023</td>\n",
       "      <td>24254</td>\n",
       "      <td>Towards Better Graph Representation Learning w...</td>\n",
       "      <td>Proposing an effective and flexible matrix to ...</td>\n",
       "      <td>[Graph Representation Learning, Graph Neural N...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202023/2...</td>\n",
       "      <td>images/24254.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10302</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2022</td>\n",
       "      <td>6409</td>\n",
       "      <td>Graph-Guided Network for Irregularly Sampled M...</td>\n",
       "      <td>In many domains, including healthcare, biology...</td>\n",
       "      <td>[Graph Neural Networks, Time Series Analysis, ...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202022/7...</td>\n",
       "      <td>images/728f206c2a01bf572b5940d7d9a8fa4c.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10303</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>71085</td>\n",
       "      <td>Empowering Convolutional Neural Nets with Meta...</td>\n",
       "      <td>ReLU networks have remained the default choice...</td>\n",
       "      <td>[Neural Networks, Computer Vision, Image Proce...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/71085.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2024</td>\n",
       "      <td>34864</td>\n",
       "      <td>Boximator: Generating Rich and Controllable Mo...</td>\n",
       "      <td>Generating rich and controllable motion is a p...</td>\n",
       "      <td>[Video Synthesis, Motion Control, Computer Vis...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202024/3...</td>\n",
       "      <td>images/34864.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10305 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conference  year  paper_id  \\\n",
       "0           ICLR  2024     19205   \n",
       "1           ICLR  2024     19234   \n",
       "2        NeurIPS  2023     71333   \n",
       "3        NeurIPS  2023     72466   \n",
       "4           ICML  2023     25261   \n",
       "...          ...   ...       ...   \n",
       "10300    NeurIPS  2023     71191   \n",
       "10301       ICML  2023     24254   \n",
       "10302       ICLR  2022      6409   \n",
       "10303    NeurIPS  2023     71085   \n",
       "10304       ICML  2024     34864   \n",
       "\n",
       "                                                   title  \\\n",
       "0      A Fast and Provable Algorithm for Sparse Phase...   \n",
       "1      Understanding Augmentation-based Self-Supervis...   \n",
       "2      Regularized Behavior Cloning for Blocking the ...   \n",
       "3                              Koopman Kernel Regression   \n",
       "4      Learning GFlowNets From Partial Episodes For I...   \n",
       "...                                                  ...   \n",
       "10300  MarioGPT: Open-Ended Text2Level Generation thr...   \n",
       "10301  Towards Better Graph Representation Learning w...   \n",
       "10302  Graph-Guided Network for Irregularly Sampled M...   \n",
       "10303  Empowering Convolutional Neural Nets with Meta...   \n",
       "10304  Boximator: Generating Rich and Controllable Mo...   \n",
       "\n",
       "                                                abstract  \\\n",
       "0      We study the sparse phase retrieval problem, w...   \n",
       "1      Data augmentation is critical to the empirical...   \n",
       "2      For partially observable environments, imitati...   \n",
       "3      Many machine learning approaches for decision ...   \n",
       "4      Generative flow networks (GFlowNets) are a fam...   \n",
       "...                                                  ...   \n",
       "10300  Procedural Content Generation (PCG) is a techn...   \n",
       "10301  Proposing an effective and flexible matrix to ...   \n",
       "10302  In many domains, including healthcare, biology...   \n",
       "10303  ReLU networks have remained the default choice...   \n",
       "10304  Generating rich and controllable motion is a p...   \n",
       "\n",
       "                                                  topics  \\\n",
       "0      [Signal Processing, Computational Mathematics,...   \n",
       "1      [Self-Supervised Learning, Representation Lear...   \n",
       "2      [Imitation Learning, Reinforcement Learning, P...   \n",
       "3      [Reinforcement Learning, Dynamical Systems, Pr...   \n",
       "4      [Reinforcement Learning, Probabilistic Modelin...   \n",
       "...                                                  ...   \n",
       "10300  [Procedural Content Generation , Large Languag...   \n",
       "10301  [Graph Representation Learning, Graph Neural N...   \n",
       "10302  [Graph Neural Networks, Time Series Analysis, ...   \n",
       "10303  [Neural Networks, Computer Vision, Image Proce...   \n",
       "10304  [Video Synthesis, Motion Control, Computer Vis...   \n",
       "\n",
       "                                               image_url  \\\n",
       "0      https://iclr.cc/media/PosterPDFs/ICLR%202024/1...   \n",
       "1      https://iclr.cc/media/PosterPDFs/ICLR%202024/1...   \n",
       "2      https://neurips.cc/media/PosterPDFs/NeurIPS%20...   \n",
       "3      https://neurips.cc/media/PosterPDFs/NeurIPS%20...   \n",
       "4      https://icml.cc/media/PosterPDFs/ICML%202023/2...   \n",
       "...                                                  ...   \n",
       "10300  https://neurips.cc/media/PosterPDFs/NeurIPS%20...   \n",
       "10301  https://icml.cc/media/PosterPDFs/ICML%202023/2...   \n",
       "10302  https://iclr.cc/media/PosterPDFs/ICLR%202022/7...   \n",
       "10303  https://neurips.cc/media/PosterPDFs/NeurIPS%20...   \n",
       "10304  https://icml.cc/media/PosterPDFs/ICML%202024/3...   \n",
       "\n",
       "                                 local_image_paths  \n",
       "0                                 images/19205.png  \n",
       "1                                 images/19234.png  \n",
       "2                                 images/71333.png  \n",
       "3                                 images/72466.png  \n",
       "4                                 images/25261.png  \n",
       "...                                            ...  \n",
       "10300                             images/71191.png  \n",
       "10301                             images/24254.png  \n",
       "10302  images/728f206c2a01bf572b5940d7d9a8fa4c.png  \n",
       "10303                             images/71085.png  \n",
       "10304                             images/34864.png  \n",
       "\n",
       "[10305 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_images(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03b31352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00271ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/8rlvtd2x063bt8wxd7bwsxjm0000gn/T/ipykernel_854/1460893844.py:5: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import requests\n",
    "import pandas as pd\n",
    "import imghdr\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.auto import tqdm  # works nicely in Jupyter\n",
    "\n",
    "\n",
    "def download_images(\n",
    "    df: pd.DataFrame,\n",
    "    url_col: str,\n",
    "    output_dir: str = \"images\",\n",
    "    path_col: str = \"local_image_path\",\n",
    "    max_workers: int = 8,\n",
    "    timeout: int = 10,\n",
    "    show_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download images from URLs in a DataFrame column and save them locally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing image URLs.\n",
    "    url_col : str\n",
    "        Name of the column in `df` that has the image URLs.\n",
    "    output_dir : str, optional\n",
    "        Directory to save images into, by default \"images\".\n",
    "    path_col : str, optional\n",
    "        Name of the column to store local file paths, by default \"local_image_path\".\n",
    "    max_workers : int, optional\n",
    "        Max number of threads for parallel downloads, by default 8.\n",
    "    timeout : int, optional\n",
    "        Timeout (seconds) for each HTTP request, by default 10.\n",
    "    show_progress : bool, optional\n",
    "        Whether to show a tqdm progress bar, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The same DataFrame with an extra column containing local image paths\n",
    "        (or None where download failed).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    urls = df[url_col].tolist()\n",
    "    results = [None] * len(urls)  # pre-allocate list for paths\n",
    "\n",
    "    def _safe_filename_from_url(url: str) -> str:\n",
    "        \"\"\"Create a base filename from the URL.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        name = os.path.basename(parsed.path)\n",
    "\n",
    "        # If URL doesn't end in a filename, fall back to a UUID\n",
    "        if not name or \".\" not in name:\n",
    "            name = str(uuid.uuid4())\n",
    "        else:\n",
    "            # Remove query parameters or fragments from the name, if any\n",
    "            name = name.split(\"?\")[0].split(\"#\")[0]\n",
    "\n",
    "        if not name:\n",
    "            name = str(uuid.uuid4())\n",
    "        return name\n",
    "\n",
    "    def _ensure_unique_path(base_path: str) -> str:\n",
    "        \"\"\"If base_path exists, append a counter to make it unique.\"\"\"\n",
    "        if not os.path.exists(base_path):\n",
    "            return base_path\n",
    "\n",
    "        root, ext = os.path.splitext(base_path)\n",
    "        counter = 1\n",
    "        candidate = f\"{root}_{counter}{ext}\"\n",
    "        while os.path.exists(candidate):\n",
    "            counter += 1\n",
    "            candidate = f\"{root}_{counter}{ext}\"\n",
    "        return candidate\n",
    "\n",
    "    def _detect_extension(content: bytes, fallback_ext: str = \".jpg\") -> str:\n",
    "        \"\"\"Detect image extension from bytes; fall back if unknown.\"\"\"\n",
    "        img_type = imghdr.what(None, h=content)\n",
    "        if img_type:\n",
    "            return f\".{img_type}\"\n",
    "        return fallback_ext\n",
    "\n",
    "    def _download_one(index_url: tuple[int, str]) -> tuple[int, str | None]:\n",
    "        \"\"\"Download a single image, return (index, local_path or None).\"\"\"\n",
    "        idx, url = index_url\n",
    "        if pd.isna(url) or not isinstance(url, str) or not url.strip():\n",
    "            return idx, None\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            content = response.content\n",
    "\n",
    "            # Base filename from URL\n",
    "            base_name = _safe_filename_from_url(url)\n",
    "            base_root, base_ext = os.path.splitext(base_name)\n",
    "\n",
    "            # Detect extension from content\n",
    "            ext = _detect_extension(content, fallback_ext=base_ext or \".jpg\")\n",
    "\n",
    "            # Rebuild filename with detected extension\n",
    "            filename = f\"{base_root}{ext}\"\n",
    "            full_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Ensure we don't overwrite existing files\n",
    "            full_path = _ensure_unique_path(full_path)\n",
    "\n",
    "            # Save file\n",
    "            with open(full_path, \"wb\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "            return idx, full_path\n",
    "\n",
    "        except Exception as e:\n",
    "            # You may want to log instead of print in real projects\n",
    "            print(f\"Failed to download {url} (index {idx}): {e}\")\n",
    "            return idx, None\n",
    "\n",
    "    # Parallel download with ThreadPoolExecutor + progress bar\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(_download_one, (i, url)): i\n",
    "            for i, url in enumerate(urls)\n",
    "        }\n",
    "\n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(futures), desc=\"Downloading images\")\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            idx, path = future.result()\n",
    "            results[idx] = path\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "\n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "\n",
    "    # Update DataFrame\n",
    "    df[path_col] = results\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a306d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_parquet(\"hf://datasets/rohitsaxena/PosterSum/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2717249d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "topics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "image_url",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0e6de529-b5e5-477d-bb68-71ccdb8dc5d9",
       "rows": [
        [
         "0",
         "NeurIPS",
         "2022",
         "58149",
         "Private and Robust Federated Learning using Private Information Retrieval and Norm Bounding",
         "Federated Learning (FL) is a distributed learning paradigm that enables mutually untrusting clients to collaboratively train a common machine learning model.  Client data privacy is paramount in FL.  At the same time, the model must be protected from poisoning attacks from adversarial clients.  Existing solutions address these two problems in isolation. We present FedPerm, a new FL algorithm that addresses both these problems by combining norm bounding for model robustness with a novel intra-model parameter shuffling technique that amplifies data privacy by means of Private Information Retrieval (PIR) based techniques that permit cryptographic aggregation of clients' model updates. The combination of these techniques helps the federation server constrain parameter updates from clients so as to curtail effects of model poisoning attacks by adversarial clients.  We further present FedPerm's unique hyperparameters that can be used effectively to trade off computation overheads with model utility.  Our empirical evaluation on the MNIST dataset demonstrates FedPerm's effectiveness over existing Differential Privacy (DP) enforcement solutions in FL.",
         "['Federated Learning' 'Privacy-Preserving Machine Learning'\n 'Adversarial Machine Learning' 'Distributed Machine Learning'\n 'Cryptography in Machine Learning']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/58149.png"
        ],
        [
         "1",
         "NeurIPS",
         "2023",
         "70289",
         "Private Federated Frequency Estimation: Adapting to the Hardness of the Instance",
         "In federated frequency estimation (FFE), multiple clients work together to estimate the frequency of their local data by communicating with a server, while maintaining the security constraint of $\\mathtt{secsum}$ where the server can only access the sum of client-held vectors. For FFE with a single communication round, it is known that count sketch is nearly information-theoretically optimal [Chen et al., 2022]. However, when multiple communication rounds are allowed, we propose a new sketch algorithm that is provably more accurate than a naive adaptation of count sketch. Furthermore, we show that both our sketch algorithm and count sketch can achieve better accuracy when the problem instance is simpler. Therefore, we propose a two-phase approach to enable the use of a smaller sketch size for simpler problems. Finally, we provide mechanisms to make our proposed algorithm differentially private. We verify the performance of our methods through experiments conducted on real datasets.",
         "['Federated Learning' 'Privacy-Preserving Machine Learning'\n 'Differential Privacy' 'Distributed Computing'\n 'Data Sketching Algorithms']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70289.png"
        ],
        [
         "2",
         "ICLR",
         "2024",
         "19285",
         "Time Fairness in Online Knapsack Problems",
         "The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive algorithms are known for this problem, they may be fundamentally unfair, i.e., individual items may be treated inequitably in different ways. We formalize a practically-relevant notion of time fairness which effectively models a trade off between static and dynamic pricing in a motivating application such as cloud resource allocation, and show that existing algorithms perform poorly under this metric.  We propose a parameterized deterministic algorithm where the parameter precisely captures the Pareto-optimal trade-off between fairness (static pricing) and competitiveness (dynamic pricing). We show that randomization is theoretically powerful enough to be simultaneously competitive and fair; however, it does not work well in experiments. To further improve the trade-off between fairness and competitiveness, we develop a nearly-optimal learning-augmented algorithm which is fair, consistent, and robust (competitive), showing substantial performance improvements in numerical experiments.",
         "['Online Algorithms' 'Knapsack Problems' 'Fairness in Algorithms'\n 'Resource Allocation' 'Algorithmic Game Theory'\n 'Learning-Augmented Algorithms']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19285.png"
        ],
        [
         "3",
         "ICLR",
         "2023",
         "13994",
         "Domain Generalization in Robust Invariant Representation",
         "Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a desirable property for training in resource-constrained settings.",
         "['Domain Generalization' 'Representation Learning' 'Robustness in AI'\n 'Unsupervised Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/13994.png"
        ],
        [
         "4",
         "NeurIPS",
         "2022",
         "62163",
         "SEIFER: Scalable Edge Inference for Deep Neural Networks",
         "Edge inference is becoming ever prevalent through its applications from retail to wearable technology. Clusters of networked resource-constrained edge devices are becoming common, yet there is no production-ready orchestration system for deploying deep learning models over such edge networks which adopts the robustness and scalability of the cloud. We present SEIFER, a framework utilizing a standalone Kubernetes cluster to partition a given DNN and place these partitions in a distributed manner across an edge network, with the goal of maximizing inference throughput. The system is node fault-tolerant and automatically updates deployments based on updates to the model's version. We provide a preliminary evaluation of a partitioning and placement algorithm that works within this framework, and show that we can improve the inference pipeline throughput by 200% by utilizing sufficient numbers of resource-constrained nodes. We have implemented SEIFER in open-source software that is publicly available to the research community.",
         "['Edge Computing' 'Deep Learning' 'Distributed Systems'\n 'Inference Systems' 'Cloud Computing' 'Internet of Things ']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/62163.png"
        ],
        [
         "5",
         "ICML",
         "2024",
         "33813",
         "FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler",
         "Federated learning (FL) enables collaborative machine learning across distributed data owners, but data heterogeneity poses a challenge for model calibration. While prior work focused on improving accuracy for non-iid data, calibration remains under-explored. This study reveals existing FL aggregation approaches lead to sub-optimal calibration, and theoretical analysis shows despite constraining variance in clients’ label distributions, global calibration error is still asymptotically lower bounded. To address this, we propose a novel Federated Calibration (FedCal) approach, emphasizing both local and global calibration. It leverages client-specific scalers for local calibration to effectively correct output misalignment without sacrificing prediction accuracy. These scalers are then aggregated via weight averaging to generate a global scaler, minimizing the global calibration error. Extensive experiments demonstrate that FedCal significantly outperforms the best-performing baseline, reducing global calibration error by 47.66% on average.",
         "['Federated Learning' 'Model Calibration' 'Distributed Systems'\n 'Data Heterogeneity']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/33813.png"
        ],
        [
         "6",
         "NeurIPS",
         "2023",
         "74839",
         "TS-DiffuGen: An equivariant diffusion model for reaction transition state conformation generation",
         "Molecular geometry optimization, particularly in the context of transition state generation, poses significant computational challenges that hinder its use within large-scale reaction workflows. Traditional methods rely on resource-intensive quantum mechanical approaches like density functional theory, demanding both computational resources and substantial prior reaction knowledge. Recent advancements in deep learning-based diffusion models have shown promise in predicting reaction transition state conformations. Current models rely on extensive architectures that capture a reaction's geometry and ensemble models. This work proposes an equivariant diffusion model, designed to address computational expenses and complex architectures. Our model demonstrates robust generalizability and efficiency in predicting transition state conformations, making it a valuable tool for a broader range of chemical reactions. Our approach is a step towards eliminating the computational barriers associated with classic transition state generation techniques, providing chemists with a powerful tool to rapidly propose transition state structures. Code and data can be found on https://figshare.com/s/cb10fda0c88f18d00baf.",
         "['Computational Chemistry' 'Machine Learning in Chemistry'\n 'Molecular Modeling' 'Reaction Dynamics' 'Quantum Chemistry']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/74839.png"
        ],
        [
         "7",
         "ICLR",
         "2024",
         "17435",
         "Retrieval meets Long Context Large Language Models",
         "Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.",
         "['Natural Language Processing' 'Information Retrieval'\n 'Large Language Models ' 'Artificial Intelligence ']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17435.png"
        ],
        [
         "8",
         "NeurIPS",
         "2022",
         "53317",
         "Benefits of Permutation-Equivariance in Auction Mechanisms",
         "Designing an incentive-compatible auction mechanism that maximizes the auctioneer's revenue while minimizes the bidders’ ex-post regret is an important yet intricate problem in economics. Remarkable progress has been achieved through learning the optimal auction mechanism by neural networks. In this paper, we consider the popular additive valuation and symmetric valuation setting; i.e., the valuation for a set of items is defined as the sum of all items’ valuations in the set, and the valuation distribution is invariant when the bidders and/or the items are permutated. We prove that permutation-equivariant neural networks have significant advantages: the permutation-equivariance decreases the expected ex-post regret, improves the model generalizability, while maintains the expected revenue invariant. This implies that the permutation-equivariance helps approach the theoretically optimal dominant strategy incentive compatible condition, and reduces the required sample complexity for desired generalization. Extensive experiments fully support our theory. To our best knowledge, this is the first work towards understanding the benefits of permutation-equivariance in auction mechanisms.",
         "['Economics' 'Auction Theory' 'Neural Networks' 'Game Theory']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53317.png"
        ],
        [
         "9",
         "ICLR",
         "2023",
         "10990",
         "Analogy-Forming Transformers for Few-Shot 3D Parsing",
         "We present Analogical Networks, a model that segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures in the input object 3D point cloud, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformer in many-shot settings and outperform them and existing paradigms of meta-learning and few-shot learning in few-shot scenarios. Our model successfully parses instances of novel object categories simply by expanding its memory, without any weight updates.",
         "['Computer Vision' '3D Object Recognition' 'Few-Shot Learning'\n 'Meta-Learning' 'Neural Networks' 'Transformers']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/10990.png"
        ],
        [
         "10",
         "ICML",
         "2022",
         "17309",
         "Achieving Fairness at No Utility Cost via Data Reweighing with Influence",
         "With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at https://github.com/brandeis-machine-learning/influence-fairness.",
         "['Fairness in AI' 'Algorithmic Governance' 'Data Preprocessing'\n 'Ethical AI']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/9c838d2e45b2ad1094d42f4ef36764f6.png"
        ],
        [
         "11",
         "ICML",
         "2024",
         "34978",
         "Provably Efficient Partially Observable Risk-sensitive Reinforcement Learning with Hindsight Observation",
         "This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\\tilde{O}\\left(\\frac{e^{|{\\gamma}|H}-1}{|{\\gamma}|H}H^2\\sqrt{KHS^2OA}\\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interest to the theoretical study of reinforcement learning.",
         "['Reinforcement Learning'\n 'Partially Observable Markov Decision Processes '\n 'Risk-sensitive Decision Making' 'Theoretical Machine Learning'\n 'Regret Analysis']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/34978.png"
        ],
        [
         "12",
         "ICLR",
         "2024",
         "21396",
         "AutoBasisEncoder: Pre-trained Neural Field Basis via Autoencoding for Operator Learning",
         "We introduce AutoBasisEncoder, a novel framework designed for operator learn-ing – the task of learning to map from one function to another. This approach au-tonomously discovers a basis of functions optimized for the target function spaceand utilizes this pre-trained basis for efficient operator learning. By introducingan intermediary auto-encoding task to the popular DeepONet framework, AutoBa-sisEncoder disentangles the learning of the basis functions and of the coefficients,simplifying the operator learning process. Initially, the framework learns basisfunctions through auto-encoding, followed by leveraging this basis to predict thecoefficients of the target function. Preliminary experiments indicate that Auto-BasisEncoder’s basis functions exhibit superior suitability for operator learningand function reconstruction compared to DeepONet. These findings underscorethe potential of AutoBasisEncoder to enhance the landscape of operator learningframeworks",
         "['Neural Networks' 'Operator Learning' 'Function Approximation'\n 'Computational Mathematics']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/21396.png"
        ],
        [
         "13",
         "ICML",
         "2024",
         "34619",
         "BiE: Bi-Exponent Block Floating-Point for Large Language Models Quantization",
         "Nowadays, Large Language Models (LLMs) mostly possess billions of parameters, bringing significant challenges to hardware platforms. Although quantization is an efficient approach to reduce computation and memory overhead for inference optimization, we stress the challenge that mainstream low-bit quantization approaches still suffer from either various data distribution outliers or a lack of hardware efficiency. We also find that low-bit data format has further potential expressiveness to cover the atypical language data distribution. In this paper, we propose a novel numerical representation, Bi-Exponent Block Floating Point (BiE), and a new quantization flow. BiE quantization shows accuracy superiority and hardware friendliness on various models and benchmarks.",
         "['Natural Language Processing' 'Quantization' 'Hardware Acceleration'\n 'Numerical Representation']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/34619.png"
        ],
        [
         "14",
         "ICLR",
         "2022",
         "6883",
         "Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design",
         "Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to bioactivity and drug discovery.",
         "['Computational Chemistry' 'Molecular Design' 'Synthesis Planning'\n 'Machine Learning in Chemistry' 'Drug Discovery'\n 'Artificial Intelligence in Chemistry']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202022/70ece1e1e0931919438fcfc6bd5f199c.png"
        ],
        [
         "15",
         "ICLR",
         "2024",
         "18815",
         "Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks",
         "Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems. The code is available at https://github.com/pkuxmq/HLOP-SNN.",
         "['Neuromorphic Computing' 'Spiking Neural Networks' 'Continual Learning'\n 'Hebbian Learning' 'Neuroscience']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18815.png"
        ],
        [
         "16",
         "ICML",
         "2024",
         "36505",
         "Towards diffusion models for large-scale sea-ice modelling",
         "We make the first steps towards diffusion models for unconditional generation of multivariate and Arctic-wide sea-ice states. While targeting to reduce the computational costs by diffusion in latent space, latent diffusion models also offer the possibility to integrate physical knowledge into the generation process. We tailor latent diffusion models to sea-ice physics with a censored Gaussian distribution in data space to generate data that follows the physical bounds of the modelled variables. Our latent diffusion models reach similar scores as the diffusion model trained in data space, but they smooth the generated fields as caused by the latent mapping. While enforcing physical bounds cannot reduce the smoothing, it improves the representation of the marginal ice zone. Therefore, for large-scale Earth system modelling, latent diffusion models can have many advantages compared to diffusion in data space if the significant barrier of smoothing can be resolved.",
         "['Climate Science' 'Earth System Modelling' 'Sea-Ice Modelling'\n 'Computational Physics' 'Machine Learning for Environmental Science']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/36505.png"
        ],
        [
         "17",
         "ICLR",
         "2024",
         "18852",
         "Transformer Fusion with Optimal Transport",
         "Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. This paper presents a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -- and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way to compress Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models.In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination. Code is available at https://github.com/graldij/transformer-fusion.",
         "['Neural Networks' 'Deep Learning' 'Natural Language Processing'\n 'Computer Vision' 'Model Compression' 'Model Fusion' 'Optimal Transport']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18852.png"
        ],
        [
         "18",
         "NeurIPS",
         "2022",
         "57011",
         "Diversity Balancing Generative Adversarial Networks for fast simulation of the Zero Degree Calorimeter in the ALICE experiment at CERN",
         "Generative Adversarial Networks (GANs) are powerful models able to synthesize data samples closely resembling the distribution of real data, yet the diversity of those generated samples is limited due to the so-called mode collapse phenomenon observed in GANs. Conditional GANs are especially prone to mode collapse, as they tend to ignore the input noise vector and focus on the conditional information. Recent methods proposed to mitigate this limitation increase the diversity of generated samples, yet they reduce the performance of the models when similarity of samples is required. To address this shortcoming, we propose a novel method to control the diversity of GAN-generated samples. By adding a simple, yet effective regularization to the training loss function we encourage the generator to discover new data modes for inputs related to diverse outputs while generating consistent samples for the remaining ones. More precisely, we reward or penalize the model for synthesising diverse images, matching the diversity of real and generated samples for a given conditional input. We show the superiority of our method on simulating data from the Zero Degree Calorimeter of the ALICE experiment in LHC, CERN.",
         "['Generative Adversarial Networks ' 'High Energy Physics'\n 'Simulation and Modeling' 'Particle Physics Experiments']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/57011.png"
        ],
        [
         "19",
         "NeurIPS",
         "2023",
         "76231",
         "Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds",
         "Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation initiation processes are alike despite variations in onset times.",
         "['Atmospheric Science' 'Cloud Microphysics' 'Climate Science'\n 'Computational Meteorology' 'Data Visualization'\n 'Machine Learning in Meteorology']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76231.png"
        ],
        [
         "20",
         "ICML",
         "2024",
         "33323",
         "Discovering Multiple Solutions from a Single Task in Offline Reinforcement Learning",
         "Recent studies on online reinforcement learning (RL) have demonstrated the advantages of learning multiple behaviors from a single task, as in the case of few-shot adaptation to a new environment. Although this approach is expected to yield similar benefits in offline RL, appropriate methods for learning multiple solutions have not been fully investigated in previous studies. In this study, we therefore addressed the problem of finding multiple solutions from a single task in offline RL. We propose algorithms that can learn multiple solutions in offline RL, and empirically investigate their performance. Our experimental results show that the proposed algorithm learns multiple qualitatively and quantitatively distinctive solutions in offline RL.",
         "['Reinforcement Learning' 'Offline Learning' 'Machine Learning Algorithms'\n 'Multi-Objective Optimization']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/33323.png"
        ],
        [
         "21",
         "NeurIPS",
         "2022",
         "53679",
         "Provably Efficient Model-Free Constrained RL with Linear Function Approximation",
         "We study the constrained reinforcement learning problem, in which an agent aims to maximize the expected cumulative reward subject to a constraint on the expected total value of a utility function.  In contrast to existing model-based approaches or model-free methods accompanied with a `simulator’, we aim to develop the first \\emph{model-free}, \\emph{simulator-free} algorithm that achieves a sublinear regret and a sublinear constraint violation even in \\emph{large-scale} systems. To this end, we consider the episodic constrained Markov decision processes with linear function approximation, where the transition dynamics and the reward function can be represented as a linear function of some known feature mapping. We show that $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ regret and  $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ constraint violation bounds can be achieved, where $d$ is the dimension of the feature mapping, $H$ is the length of the episode, and $T$ is the total number of steps. Our bounds are attained without explicitly estimating the unknown transition model or requiring a simulator, and they depend on the state space only through the dimension of the feature mapping. Hence our bounds hold even when the number of states goes to infinity. Our main results are achieved via novel adaptations of the standard LSVI-UCB algorithms. In particular, we first introduce primal-dual optimization into the LSVI-UCB algorithm to balance between regret and constraint violation. More importantly, we replace the standard greedy selection with respect to the state-action function with a soft-max policy. This turns out to be key in establishing uniform concentration (a critical step for provably efficient model-free exploration) for the constrained case via its approximation-smoothness trade-off. Finally, we also show that one can achieve an even zero constraint violation for large enough $T$ by trading the regret a little bit but still maintaining the same order with respect to $T$.",
         "['Reinforcement Learning' 'Constrained Optimization'\n 'Machine Learning Theory' 'Markov Decision Processes'\n 'Function Approximation']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53679.png"
        ],
        [
         "22",
         "ICLR",
         "2024",
         "17490",
         "Mixture of Weak and Strong Experts on Graphs",
         "Realistic graphs contain both (1) rich self-features of nodes and  (2) informative structures of neighborhoods, jointly handled by a Graph Neural Network (GNN) in the typical setup. We propose to decouple the two modalities byMixtureofweak andstrong experts (Mowst), where the weak expert is a light-weight Multi-layer Perceptron (MLP), and the strong expert is an off-the-shelf GNN. To adapt the experts' collaboration to different target nodes, we propose a \"confidence\" mechanism based on the dispersion of the weak expert's prediction logits. The strong expert is conditionally activated in the low-confidence region when either the node's classification relies on neighborhood information, or the weak expert has low model quality. We reveal interesting training dynamics by analyzing the influence of the confidence function on loss: our training algorithm encourages the specialization of each expert by effectively generating soft splitting of the graph. In addition, our \"confidence\" design imposes a desirable bias toward the strong expert to benefit from GNN's better generalization capability. Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN. Empirically, Mowst on 4 backbone GNN architectures show significant accuracy improvement on 6 standard node classification benchmarks, including both homophilous and heterophilous graphs (https://github.com/facebookresearch/mowst-gnn).",
         "['Graph Neural Networks ' 'Node Classification'\n 'Neural Network Architectures' 'Graph Theory']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17490.png"
        ],
        [
         "23",
         "ICLR",
         "2023",
         "11788",
         "Transformer Meets Boundary Value Inverse Problems",
         "A Transformer-based deep direct sampling method is proposed for electrical impedance tomography, a well-known severely ill-posed nonlinear boundary value inverse problem. A real-time reconstruction is achieved by evaluating the learned inverse operator between carefully designed data and the reconstructed images. An effort is made to give a specific example to a fundamental question: whether and how one can benefit from the theoretical structure of a mathematical problem to develop task-oriented and structure-conforming deep neural networks? Specifically, inspired by direct sampling methods for inverse problems, the 1D boundary data in different frequencies are preprocessed by a partial differential equation-based feature map to yield 2D harmonic extensions as different input channels. Then, by introducing learnable non-local kernels, the direct sampling is recast to a modified attention mechanism. The new method achieves superior accuracy over its predecessors and contemporary operator learners and shows robustness to noises in benchmarks. This research shall strengthen the insights that, despite being invented for natural language processing tasks, the attention mechanism offers great flexibility to be modified in conformity with the a priori mathematical knowledge, which ultimately leads to the design of more physics-compatible neural architectures.",
         "['Inverse Problems' 'Electrical Impedance Tomography' 'Deep Learning'\n 'Computational Mathematics' 'Applied Mathematics' 'Neural Networks'\n 'Signal Processing']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/11788.png"
        ],
        [
         "24",
         "NeurIPS",
         "2023",
         "70248",
         "What’s Left? Concept Grounding with Logic-Enhanced Foundation Models",
         "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning—using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like “left” can also be grounded in 3D, temporal, and action data, as in moving to yourleft. This limited generalization stems from these inference-only methods’ inability to learn or adapt pre-trained models to a new domain. We propose theLogic-EnhancedFoundaTion Model (LEFT), a unified framework thatlearnsto ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT’s executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.",
         "['Computer Vision' 'Natural Language Processing' 'Robotics'\n 'Multimodal Learning' 'Logic Programming']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70248.png"
        ],
        [
         "25",
         "ICLR",
         "2024",
         "19342",
         "Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces",
         "Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three state-of-the-art approaches designed for large discrete action spaces across three distinct environments. Our results show that DNC matches or outperforms state-of-the-art approaches while being computationally more  efficient. Furthermore, our method scales to action spaces that so far remained computationally intractable for existing methodologies.",
         "['Reinforcement Learning' 'Combinatorial Optimization'\n 'Operations Research']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19342.png"
        ],
        [
         "26",
         "NeurIPS",
         "2023",
         "76128",
         "Improving dispersive readout of a superconducting qubit by machine learning on path signature",
         "One major challenge that arises from quantum computing is to implement fast, high-accuracy quantum state readout. For superconducting circuits, this problem reduces to a time series classification problem on readout signals. We propose that using path signature methods to extract features can enhance existing techniques for quantum state discrimination. We demonstrate the superior performance of our proposed approach over conventional methods in distinguishing three different quantum states on real experimental data from a superconducting transmon qubit.",
         "['Quantum Computing' 'Superconducting Qubits' 'Quantum State Readout'\n 'Time Series Classification']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76128.png"
        ],
        [
         "27",
         "ICLR",
         "2022",
         "6604",
         "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
         "In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for the lottery ticket hypothesis, distributed training, and ensemble methods. The source code is available at \\url{https://github.com/rahimentezari/PermutationInvariance}.",
         "['Neural Networks' 'Optimization' 'Deep Learning Theory'\n 'Ensemble Methods' 'Distributed Computing']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202022/4b26dc4663ccf960c8538d595d0a1d3a.png"
        ],
        [
         "28",
         "NeurIPS",
         "2022",
         "56860",
         "Training physical networks like neural networks: deep physical neural networks",
         "Deep neural networks (DNNs) are increasingly used to predict physical processes. Here, we invert this relationship, and show that physical processes with adjustable physical parameters (e.g., geometry, voltages) can be trained to emulate DNNs, i.e., to perform machine learning inference tasks. We call these trainable processes \\textit{physical neural networks} (PNNs). We train experimental PNNs based on broadband optical pulses propagating in a nonlinear crystal, a nonlinear electronic oscillator, and an oscillating metal plate. As an extension of these laboratory proof-of-concepts, we train (in simulation) a network of coupled oscillators to perform Fashion MNIST classification. Since one cannot apply autodifferentiation directly to physical processes, we introduce a technique that uses a simulation model to efficiently estimate the gradients of the physical system, allowing us to use backpropagation to train PNNs. Using this technique, we train each system's physical transformations (which do not necessarily resemble typical DNN layers) directly to perform inference calculations. Our work may help inspire novel neural network architectures, including ones that can be efficiently realized with particular physical processes, and presents a route to training complex physical systems to take on desired physical functionalities, such as computational sensing. This article is intended as a summary of the previously published work [Wright, Onodera et al., 2022] for the NeurIPS 2022 Machine Learning and the Physical Sciences workshop.",
         "['Neural Networks' 'Physical Systems' 'Computational Physics'\n 'Optical Computing' 'Nonlinear Dynamics' 'Computational Sensing']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/56860.png"
        ],
        [
         "29",
         "NeurIPS",
         "2022",
         "63656",
         "MKA-Net: Multi-Kernel Attention Conv-Network",
         "Current Convolutional Neural Networks (CNNs) does not explicitly capture and select diverse image features. Rather follow an indirect approach of increasing the networks' depth or width, which significantly increase the computational cost of the models. Inspired by biological visual system, this paper proposes a Multi-Kernel Attention Convolutional Network (MKA-Net ), which enables any feed-forward CNNs to explicitly capture and select diverse informative features to efficiently boost CNNs' performance. MKA-Net  infers attention from the intermediate feature map by first using multiple sizes of kernels to capture diverse features then exploit neighboring feature-map relationship to adaptively select the most informative features. MKA-Net incurs negligible computational overhead and is designed to be easily integrated with any CNN architecture. We extensively evaluated the proposed MKA-Net module on benchmark datasets, including CIFAR100, SVHN, and ImageNet, with various CNN architectures. The experimental results show our approach provides a significant performance improvement with very minimal computational overhead.",
         "['Computer Vision' 'Deep Learning' 'Neural Networks' 'Image Processing']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/63656.png"
        ],
        [
         "30",
         "ICLR",
         "2024",
         "17905",
         "DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness",
         "Machine Learning (ML) models have been utilized for malware detection for over two decades. Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks. However, most if not all existing defenses against evasion attacks suffer from sizable performance degradation and/or can defend against only specific attacks, which makes them less practical in real-world settings. In this work, we develop a certified defense, DRSM (De-Randomized Smoothed MalConv), by redesigning the *de-randomized smoothing* technique for the domain of malware detection. Specifically, we propose a *window ablation* scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of the executables. After showing how DRSM is theoretically robust against attacks with contiguous adversarial bytes, we verify its performance and certified robustness experimentally, where we observe only marginal accuracy drops as the cost of robustness. To our knowledge, we are the first to offer certified robustness in the realm of static detection of malware executables. More surprisingly, through evaluating DRSM against $9$ empirical attacks of different types, we observe that the proposed defense is empirically robust to some extent against a diverse set of attacks, some of which even fall out of the scope of its original threat model. In addition, we collected $15.5K$ recent benign raw executables from diverse sources, which will be made public as a dataset called PACE (Publicly Accessible Collection(s) of Executables) to alleviate the scarcity of publicly available benign datasets for studying malware detection and provide future research with more representative data of the time. Our code and dataset are available at - https://github.com/ShoumikSaha/DRSM",
         "['Cybersecurity' 'Malware Detection' 'Adversarial Machine Learning'\n 'Robustness in Machine Learning' 'Software Security']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/17905.png"
        ],
        [
         "31",
         "ICML",
         "2023",
         "23989",
         "Quantile Credit Assignment",
         "In reinforcement learning, the credit assignment problem is to distinguish luck from skill, that is, separate the inherent randomness in the environment from the controllable effects of the agent's actions. This paper proposes two novel algorithms, Quantile Credit Assignment (QCA) and Hindsight QCA (HQCA), which incorporate distributional value estimation to perform credit assignment. QCA uses a network that predicts the quantiles of the return distribution, whereas HQCA additionally incorporates information about the future. Both QCA and HQCA have the appealing interpretation of leveraging an estimate of the quantile level of the return (interpreted as the level of \"luck\") in order to derive a \"luck-dependent\" baseline for policy gradient methods. We show theoretically that this approach gives an unbiased policy gradient estimate that can yield significant variance reductions over a standard value estimate baseline. QCA and HQCA significantly outperform prior state-of-the-art methods on a range of extremely difficult credit assignment problems.",
         "['Reinforcement Learning' 'Credit Assignment'\n 'Distributional Reinforcement Learning' 'Machine Learning Algorithms']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/23989.png"
        ],
        [
         "32",
         "ICML",
         "2024",
         "32845",
         "Improving Antibody Humanness Prediction using Patent Data",
         "We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the learned model consistently outperforms the alternative baselines and establishes new state-of-the-art on five out of six inference tasks, irrespective of the used metric.",
         "['Bioinformatics' 'Computational Biology' 'Drug Discovery'\n 'Machine Learning in Healthcare' 'Immunology' 'Biotechnology']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/32845.png"
        ],
        [
         "33",
         "ICLR",
         "2024",
         "18587",
         "Sliced Wasserstein Estimation with Control Variates",
         "The sliced Wasserstein (SW) distances between two probability measures are defined as the expectation of the Wasserstein distance between two one-dimensional projections of the two measures. The randomness comes from a projecting direction that is used to project the two input measures to one dimension. Due to the intractability of the expectation, Monte Carlo integration is performed to estimate the value of the SW distance. Despite having various variants, there has been no prior work that improves the Monte Carlo estimation scheme for the SW distance in terms of controlling its variance. To bridge the literature on variance reduction and the literature on the SW distance, we propose computationally efficient control variates to reduce the variance of the empirical estimation of the SW distance. The key idea is to first find Gaussian approximations of projected one-dimensional measures, then we utilize the closed-form of the Wasserstein-2 distance between two Gaussian distributions to design the control variates. In particular, we propose using a lower bound and an upper bound of the Wasserstein-2 distance between two fitted Gaussians as two computationally efficient control variates. We empirically show that the proposed control variate estimators can help to reduce the variance considerably when comparing measures over images and point-clouds. Finally, we demonstrate the favorable performance of the proposed control variate estimators in gradient flows to interpolate between two point-clouds and in deep generative modeling on standard image datasets, such as CIFAR10 and CelebA.",
         "['Probability Theory' 'Computational Statistics'\n 'Variance Reduction Techniques' 'Optimal Transport' 'Monte Carlo Methods'\n 'Deep Generative Models']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18587.png"
        ],
        [
         "34",
         "NeurIPS",
         "2022",
         "53218",
         "Better SGD using Second-order Momentum",
         "We develop a new algorithm for non-convex stochastic optimization that finds an $\\epsilon$-critical point in the optimal $O(\\epsilon^{-3})$ stochastic gradient and Hessian-vector product computations. Our algorithm uses Hessian-vector products to \"correct'' a bias term in the momentum of SGD with momentum. This leads to better gradient estimates in a manner analogous to variance reduction methods. In contrast to prior work, we do not require excessively large batch sizes and are able to provide an adaptive algorithm whose convergence rate automatically improves with decreasing variance in the gradient estimates. We validate our results on a variety of large-scale deep learning architectures and benchmarks tasks.",
         "['Optimization' 'Stochastic Optimization' 'Deep Learning' 'Algorithms']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/53218.png"
        ],
        [
         "35",
         "NeurIPS",
         "2023",
         "71920",
         "Imbalanced Mixed Linear Regression",
         "We consider the problem of mixed linear regression (MLR), where each observed sample belongs to one of $K$ unknown linear models. In practical applications, the mixture of the $K$ models may be imbalanced with a significantly different number of samples from each model. Unfortunately, most MLR methods do not perform well in such settings. Motivated by this practical challenge, in this work we propose Mix-IRLS, a novel, simple and fast algorithm for MLR with excellent performance on both balanced and imbalanced mixtures.In contrast to popular approaches that recover the $K$ models simultaneously, Mix-IRLS does it sequentially using tools from robust regression. Empirically, beyond imbalanced mixtures, Mix-IRLS succeeds in a broad range of additional settings where other methods fail, including small sample sizes, presence of outliers, and an unknown number of models $K$. Furthermore, Mix-IRLS outperforms competing methods on several real-world datasets, in some cases by a large margin. We complement our empirical results by deriving a recovery guarantee for Mix-IRLS, which highlights its advantage on imbalanced mixtures.",
         "['Statistical Learning' 'Regression Analysis' 'Robust Statistics'\n 'Algorithm Development']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/71920.png"
        ],
        [
         "36",
         "ICLR",
         "2022",
         "6994",
         "Constructing Orthogonal Convolutions in an Explicit Manner",
         "Convolutions with orthogonal input-output Jacobian matrix, i.e., orthogonal convolution,  have recently attracted substantial attention.  A convolution layer with an orthogonal Jacobian matrix is 1-Lipschitz  in the  2-norm, making the output robust to the perturbation in input. Meanwhile, an orthogonal Jacobian matrix preserves the gradient norm in back-propagation, which is critical for stable training deep networks. Nevertheless,  existing orthogonal convolutions are burdened by high computational costs for preserving orthogonality.In this work, we exploit the relation between the singular values of the convolution layer's  Jacobian and the structure of the convolution kernel.  To achieve orthogonality, we explicitly construct the convolution kernel for enforcing all singular values of the convolution layer's Jacobian to be $1$s.   After training,  the explicitly constructed orthogonal (ECO) convolution is constructed only once, and their weights are stored. Then,  in evaluation, we only need to load the stored weights of the trained  ECO convolution, and the computational cost of ECO convolution is the same as the standard dilated convolution. It is more efficient than the recent state-of-the-art approach, skew orthogonal convolution (SOC) in evaluation.    Experiments on CIFAR-10 and CIFAR-100  demonstrate that the proposed ECO convolution is faster than SOC in evaluation while leading to competitive standard and certified robust accuracies.",
         "['Deep Learning' 'Neural Networks' 'Convolutional Neural Networks '\n 'Optimization in Machine Learning' 'Robustness in Machine Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202022/d86ea612dec96096c5e0fcc8dd42ab6d.png"
        ],
        [
         "37",
         "ICLR",
         "2022",
         "6759",
         "Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations",
         "Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph neural networks (GNNs) designed for molecular property prediction at best use atomic labels to naïvely treat chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To enable representation learning on molecules with defined stereochemistry, we design an SE(3)-invariant model that processes torsion angles of a 3D molecular conformer. We explicitly model conformational flexibility by integrating a novel type of invariance to rotations about internal molecular bonds into the architecture, mitigating the need for multi-conformer data augmentation. We test our model on four benchmarks: contrastive learning to distinguish conformers of different stereoisomers in a learned latent space, classification of chiral centers as R/S, prediction of how enantiomers rotate circularly polarized light, and ranking enantiomers by their docking scores in an enantiosensitive protein pocket. We compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs to demonstrate that our model achieves state of the art performance when learning chiral-sensitive functions from molecular structures.",
         "['Computational Chemistry' 'Machine Learning in Chemistry'\n 'Molecular Modeling' 'Stereochemistry' 'Cheminformatics']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202022/a19acd7d2689207f9047f8cb01357370.png"
        ],
        [
         "38",
         "NeurIPS",
         "2023",
         "77410",
         "Sampling Protein Language Models for Functional Protein Design",
         "Protein language models have emerged as powerful ways to learn complex representations of proteins, thereby improving their performance on several downstream tasks, from structure prediction to fitness prediction, property prediction, homology detection, and more. By learning a distribution over protein sequences, they are also very promising tools for designing novel and functional proteins, with broad applications in healthcare, new material, or sustainability. Given the vastness of the corresponding sample space, efficient exploration methods are critical to the success of protein engineering efforts. However, the methodologies for adequately sampling these models to achieve core protein design objectives remain underexplored and have predominantly leaned on techniques developed for Natural Language Processing. In this work, we first develop a holistic in silico protein design evaluation framework, to comprehensively compare different sampling methods. After performing a thorough review of sampling methods for language models, we introduce several sampling strategies tailored to protein design. Lastly, we compare the various strategies on our in silico benchmark, investigating the effects of key hyperparameters and highlighting practical guidance on the relative strengths of different methods.",
         "['Computational Biology' 'Protein Engineering' 'Bioinformatics'\n 'Machine Learning in Biology' 'Structural Biology']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/77410.png"
        ],
        [
         "39",
         "NeurIPS",
         "2022",
         "62821",
         "Incentivizing Intelligence: The Bittensor Approach",
         "Inspired by the efficiency of financial markets, we propose that a market system can be used to effectively produce machine intelligence. This paper introduces a mechanism in which machine intelligence is valued by other intelligence systems peer-to-peer across the internet. Peers rank each other by training neural networks that are able to learn the value of their neighbours, while scores accumulate on a digital ledger. High-ranking peers are rewarded with additional weight in the network. In addition, the network features an incentive mechanism designed to resist collusion. The result is a collectively run machine intelligence market that continually produces newly trained models and rewards participants who contribute information-theoretic value to the system.",
         "['Decentralized Systems' 'Blockchain Technology' 'Incentive Mechanisms']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/62821.png"
        ],
        [
         "40",
         "ICLR",
         "2023",
         "10958",
         "On the duality between contrastive and non-contrastive self-supervised learning",
         "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.",
         "['Self-Supervised Learning' 'Computer Vision' 'Representation Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/10958.png"
        ],
        [
         "41",
         "ICML",
         "2023",
         "25115",
         "Learning to Boost Training by Periodic Nowcasting Near Future Weights",
         "Recent complicated problems require large-scale datasets and complex model architectures, however, it is difficult to train such large networks due to high computational issues. Significant efforts have been made to make the training more efficient such as momentum, learning rate scheduling, weight regularization, and meta-learning. Based on our observations on 1) high correlation between past eights and future weights, 2) conditions for beneficial weight prediction, and 3) feasibility of weight prediction, we propose a more general framework by intermittently skipping a handful of epochs by periodically forecasting near future weights, i.e., a Weight Nowcaster Network (WNN). As an add-on module, WNN predicts the future weights to make the learning process faster regardless of tasks and architectures. Experimental results show that WNN can significantly save actual time cost for training with an additional marginal time to train WNN. We validate the generalization capability of WNN under various tasks, and demonstrate that it works well even for unseen tasks. The code and pre-trained model are available at https://github.com/jjh6297/WNN.",
         "['Deep Learning' 'Neural Networks' 'Model Optimization'\n 'Computational Efficiency']",
         "https://icml.cc/media/PosterPDFs/ICML%202023/25115.png"
        ],
        [
         "42",
         "NeurIPS",
         "2023",
         "76944",
         "ALAS: Active Learning for Autoconversion Rates Prediction from Satellite Data",
         "High-resolution simulations, such as the ICOsahedral Non-hydrostatic Large-Eddy Model (ICON-LEM), provide valuable insights into the complex interactions among aerosols, clouds, and precipitation, which are the major contributors to climate change uncertainty. However, due to its exorbitant computational costs, it can only be employed for a limited period and geographical area. To address this, we propose a more cost-effective method powered by emerging machine learning approach -- leveraging high-resolution climate simulation as the oracle and abundant unlabeled data drawn from satellite data -- to better understand the intricate dynamics of the climate system. Our approach involves active learning techniques to predict autoconversion rates, a crucial step in precipitation formation, while significantly reducing the need for a large number of labeled instances. In this study, we present novel methods: custom query strategy fusion for labeling instances, WiFi and MeFi, along with active feature selection based on SHAP, designed to tackle real-world challenges due to its simplicity and practicality in application, specifically focusing on the prediction of autoconversion rates.",
         "['Climate Science' 'Remote Sensing' 'Atmospheric Science'\n 'Environmental Science']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/76944.png"
        ],
        [
         "43",
         "ICLR",
         "2024",
         "18891",
         "Cascading Reinforcement Learning",
         "Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with  large attraction probabilities but also leading to good successor states. This imposes a huge computational challenge due to the combinatorial action space. To tackle this challenge, we delve into the properties of value functions, and design an oracle BestPerm to efficiently find the optimal item list. Equipped with BestPerm, we develop two algorithms CascadingVI and CascadingBPI, which are both computationally-efficient and sample-efficient, and provide near-optimal regret and sample complexity guarantees. Furthermore, we present experiments to show the improved computational and sample efficiencies of our algorithms compared to straightforward adaptations of existing RL algorithms in practice.",
         "['Reinforcement Learning' 'Recommendation Systems' 'Online Advertising'\n 'Computational Efficiency' 'Algorithm Design']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18891.png"
        ],
        [
         "44",
         "ICML",
         "2024",
         "34743",
         "On the Weight Dynamics of Deep Normalized Networks",
         "Recent studies have shown that high disparities in effective learning rates (ELRs) across layers in deep neural networks can negatively affect trainability. We formalize how these disparities evolve over time by modeling weight dynamics (evolution of expected gradient and weight norms) of networks with normalization layers, predicting the evolution of layer-wise ELR ratios. We prove that when training with any constant learning rate, ELR ratios converge to 1, despite initial gradient explosion. We identify a \"critical learning rate\" beyond which ELR disparities widen, which only depends on current ELRs. To validate our findings, we devise a hyper-parameter-free warm-up method that successfully minimizes ELR spread quickly in theory and practice. Our experiments link ELR spread with trainability, a relationship that is most evident in very deep networks with significant gradient magnitude excursions.",
         "['Deep Learning' 'Neural Networks' 'Machine Learning Theory'\n 'Optimization in Machine Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/34743.png"
        ],
        [
         "45",
         "ICML",
         "2024",
         "38382",
         "Prompt Optimization with EASE? Efficient Ordering-aware Automated Selection of Exemplars",
         "Large language models (LLMs) have shown impressive capabilities in real-world applications. The capability of *in-context learning* (ICL) allows us to adapt an LLM to downstream tasks by including input-label exemplars in the prompt without model fine-tuning. However, the quality of these exemplars in the prompt greatly impacts performance, highlighting the need for an effective automated exemplar selection method. Recent studies have explored retrieval-based approaches to select exemplars tailored to individual test queries, which can be undesirable due to extra test-time computation and an increased risk of data exposure. Moreover, existing methods fail to adequately account for the impact of exemplar ordering on the performance. On the other hand, the impact of the *instruction*, another essential component in the prompt given to the LLM, is often overlooked in existing exemplar selection methods. To address these challenges, we propose a novel method named $\\texttt{EASE}$, which leverages the hidden embedding from a pre-trained language model to represent ordered sets of exemplars and uses a neural bandit algorithm to optimize the sets of exemplars *while accounting for exemplar ordering*. Our $\\texttt{EASE}$ can efficiently find an ordered set of exemplars that *performs well for all test queries* from a given task, thereby eliminating test-time computation. Importantly, $\\texttt{EASE}$ can be readily extended to *jointly optimize both the exemplars and the instruction*. Through extensive empirical evaluations (including novel tasks), we demonstrate the superiority of $\\texttt{EASE}$ over existing methods, and reveal practical insights about the impact of exemplar selection on ICL, which may be of independent interest.",
         "['Natural Language Processing' 'Prompt Engineering' 'Language Models'\n 'In-Context Learning']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/38382.png"
        ],
        [
         "46",
         "ICML",
         "2022",
         "17789",
         "Bayesian Optimization under Stochastic Delayed Feedback",
         "Bayesian optimization (BO) is a widely-used sequential method for zeroth-order optimization of complex and expensive-to-compute black-box functions. The existing BO methods assume that the function evaluation (feedback) is available to the learner immediately or after a fixed delay. Such assumptions may not be practical in many real-life problems like online recommendations, clinical trials, and hyperparameter tuning where feedback is available after a random delay. To benefit from the experimental parallelization in these problems, the learner needs to start new function evaluations without waiting for delayed feedback. In this paper, we consider the BO under stochastic delayed feedback problem. We propose algorithms with sub-linear regret guarantees that efficiently address the dilemma of selecting new function queries while waiting for randomly delayed feedback. Building on our results, we also make novel contributions to batch BO and contextual Gaussian process bandits. Experiments on synthetic and real-life datasets verify the  performance of our algorithms.",
         "['Bayesian Optimization' 'Stochastic Processes'\n 'Sequential Decision Making' 'Optimization Algorithms'\n 'Gaussian Processes' 'Bandit Algorithms']",
         "https://icml.cc/media/PosterPDFs/ICML%202022/54b2b21af94108d83c2a909d5b0a6a50.png"
        ],
        [
         "47",
         "ICML",
         "2024",
         "32614",
         "Rethinking Independent Cross-Entropy Loss For Graph-Structured Data",
         "Graph neural networks (GNNs) have exhibited prominent performance in learning graph-structured data. Considering node classification task, based on the i.i.d assumption among node labels, the traditional supervised learning simply sums up cross-entropy losses of the independent training nodes and applies the average loss to optimize GNNs' weights. But different from other data formats, the nodes are naturally connected. It is found that the independent distribution modeling of node labels restricts GNNs' capability to generalize over the entire graph and defend adversarial attacks. In this work, we propose a new framework, termed joint-cluster supervised learning, to model the joint distribution of each node with its corresponding cluster. We learn the joint distribution of node and cluster labels conditioned on their representations, and train GNNs with the obtained joint loss. In this way, the data-label reference signals extracted from the local cluster explicitly strengthen the discrimination ability on the target node. The extensive experiments demonstrate that our joint-cluster supervised learning can effectively bolster GNNs' node classification accuracy. Furthermore, being benefited from the reference signals which may be free from spiteful interference, our learning paradigm significantly protects the node classification from being affected by the adversarial attack.",
         "['Graph Neural Networks' 'Node Classification'\n 'Adversarial Machine Learning' 'Supervised Learning' 'Graph Theory']",
         "https://icml.cc/media/PosterPDFs/ICML%202024/32614.png"
        ],
        [
         "48",
         "ICLR",
         "2024",
         "18405",
         "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
         "Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.",
         "['Natural Language Processing' 'Evolutionary Algorithms'\n 'Prompt Engineering' 'Optimization Techniques']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/18405.png"
        ],
        [
         "49",
         "NeurIPS",
         "2023",
         "73404",
         "Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT",
         "Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \\$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate its dynamics to provide new observations and insights. Specifically, through downloading and parsing the NFT transaction activities, we obtain a temporal graph with more than 4.5 million nodes and 124 million edges. Then, a series of measurements are presented to understand the properties of the NFT ecosystem. Through comparisons with social, citation, and web networks, our analyses give intriguing findings and point out potential directions for future exploration. Finally, we also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community. The source codes and dataset are available at https://livegraphlab.github.io.",
         "['Blockchain Technology' 'Temporal Graph Analysis' 'Non-fungible Tokens '\n 'Data Privacy and Security' 'Machine Learning in Graphs']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/73404.png"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3000
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conference</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topics</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>58149</td>\n",
       "      <td>Private and Robust Federated Learning using Pr...</td>\n",
       "      <td>Federated Learning (FL) is a distributed learn...</td>\n",
       "      <td>[Federated Learning, Privacy-Preserving Machin...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>70289</td>\n",
       "      <td>Private Federated Frequency Estimation: Adapti...</td>\n",
       "      <td>In federated frequency estimation (FFE), multi...</td>\n",
       "      <td>[Federated Learning, Privacy-Preserving Machin...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19285</td>\n",
       "      <td>Time Fairness in Online Knapsack Problems</td>\n",
       "      <td>The online knapsack problem is a classic probl...</td>\n",
       "      <td>[Online Algorithms, Knapsack Problems, Fairnes...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2023</td>\n",
       "      <td>13994</td>\n",
       "      <td>Domain Generalization in Robust Invariant Repr...</td>\n",
       "      <td>Unsupervised approaches for learning represent...</td>\n",
       "      <td>[Domain Generalization, Representation Learnin...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202023/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>62163</td>\n",
       "      <td>SEIFER: Scalable Edge Inference for Deep Neura...</td>\n",
       "      <td>Edge inference is becoming ever prevalent thro...</td>\n",
       "      <td>[Edge Computing, Deep Learning, Distributed Sy...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>57023</td>\n",
       "      <td>Addressing out-of-distribution data for flow-b...</td>\n",
       "      <td>Simulation-based inference and normalizing flo...</td>\n",
       "      <td>[Gravitational Wave Astronomy, Machine Learnin...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>18996</td>\n",
       "      <td>Efficiently Computing Similarities to Private ...</td>\n",
       "      <td>Many methods in differentially private model t...</td>\n",
       "      <td>[Differential Privacy, Algorithmic Privacy, Ke...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2023</td>\n",
       "      <td>23928</td>\n",
       "      <td>Kernel Logistic Regression Approximation of an...</td>\n",
       "      <td>This paper proposes an understandable neural n...</td>\n",
       "      <td>[Neural Networks, Kernel Methods, Logistic Reg...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202023/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2023</td>\n",
       "      <td>25860</td>\n",
       "      <td>Elephant Neural Networks: Born to Be a Continu...</td>\n",
       "      <td>Catastrophic forgetting remains a great challe...</td>\n",
       "      <td>[Neural Networks, Continual Learning, Catastro...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202023/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>ICML</td>\n",
       "      <td>2024</td>\n",
       "      <td>32629</td>\n",
       "      <td>DSD-DA: Distillation-based Source Debiasing fo...</td>\n",
       "      <td>Though feature-alignment based Domain Adaptive...</td>\n",
       "      <td>[Domain Adaptation, Object Detection, Computer...</td>\n",
       "      <td>https://icml.cc/media/PosterPDFs/ICML%202024/3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conference  year  paper_id  \\\n",
       "0       NeurIPS  2022     58149   \n",
       "1       NeurIPS  2023     70289   \n",
       "2          ICLR  2024     19285   \n",
       "3          ICLR  2023     13994   \n",
       "4       NeurIPS  2022     62163   \n",
       "...         ...   ...       ...   \n",
       "2995    NeurIPS  2022     57023   \n",
       "2996       ICLR  2024     18996   \n",
       "2997       ICML  2023     23928   \n",
       "2998       ICML  2023     25860   \n",
       "2999       ICML  2024     32629   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Private and Robust Federated Learning using Pr...   \n",
       "1     Private Federated Frequency Estimation: Adapti...   \n",
       "2             Time Fairness in Online Knapsack Problems   \n",
       "3     Domain Generalization in Robust Invariant Repr...   \n",
       "4     SEIFER: Scalable Edge Inference for Deep Neura...   \n",
       "...                                                 ...   \n",
       "2995  Addressing out-of-distribution data for flow-b...   \n",
       "2996  Efficiently Computing Similarities to Private ...   \n",
       "2997  Kernel Logistic Regression Approximation of an...   \n",
       "2998  Elephant Neural Networks: Born to Be a Continu...   \n",
       "2999  DSD-DA: Distillation-based Source Debiasing fo...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Federated Learning (FL) is a distributed learn...   \n",
       "1     In federated frequency estimation (FFE), multi...   \n",
       "2     The online knapsack problem is a classic probl...   \n",
       "3     Unsupervised approaches for learning represent...   \n",
       "4     Edge inference is becoming ever prevalent thro...   \n",
       "...                                                 ...   \n",
       "2995  Simulation-based inference and normalizing flo...   \n",
       "2996  Many methods in differentially private model t...   \n",
       "2997  This paper proposes an understandable neural n...   \n",
       "2998  Catastrophic forgetting remains a great challe...   \n",
       "2999  Though feature-alignment based Domain Adaptive...   \n",
       "\n",
       "                                                 topics  \\\n",
       "0     [Federated Learning, Privacy-Preserving Machin...   \n",
       "1     [Federated Learning, Privacy-Preserving Machin...   \n",
       "2     [Online Algorithms, Knapsack Problems, Fairnes...   \n",
       "3     [Domain Generalization, Representation Learnin...   \n",
       "4     [Edge Computing, Deep Learning, Distributed Sy...   \n",
       "...                                                 ...   \n",
       "2995  [Gravitational Wave Astronomy, Machine Learnin...   \n",
       "2996  [Differential Privacy, Algorithmic Privacy, Ke...   \n",
       "2997  [Neural Networks, Kernel Methods, Logistic Reg...   \n",
       "2998  [Neural Networks, Continual Learning, Catastro...   \n",
       "2999  [Domain Adaptation, Object Detection, Computer...   \n",
       "\n",
       "                                              image_url  \n",
       "0     https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "1     https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "2     https://iclr.cc/media/PosterPDFs/ICLR%202024/1...  \n",
       "3     https://iclr.cc/media/PosterPDFs/ICLR%202023/1...  \n",
       "4     https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "...                                                 ...  \n",
       "2995  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  \n",
       "2996  https://iclr.cc/media/PosterPDFs/ICLR%202024/1...  \n",
       "2997  https://icml.cc/media/PosterPDFs/ICML%202023/2...  \n",
       "2998  https://icml.cc/media/PosterPDFs/ICML%202023/2...  \n",
       "2999  https://icml.cc/media/PosterPDFs/ICML%202024/3...  \n",
       "\n",
       "[3000 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7256fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a49984bdc174b1ca7d6848eeb84ae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = download_images(\n",
    "    df_validation,\n",
    "    url_col=\"image_url\",\n",
    "    output_dir=\"images\",\n",
    "    path_col=\"local_image_path\",\n",
    "    max_workers=8,\n",
    "    timeout=10,\n",
    "    show_progress=True,  # turn off if you don’t want it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39b8080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9f3517f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conference",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "topics",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "image_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "local_image_path",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a2340ab7-977d-43f8-930b-61237ce5b333",
       "rows": [
        [
         "0",
         "NeurIPS",
         "2022",
         "58149",
         "Private and Robust Federated Learning using Private Information Retrieval and Norm Bounding",
         "Federated Learning (FL) is a distributed learning paradigm that enables mutually untrusting clients to collaboratively train a common machine learning model.  Client data privacy is paramount in FL.  At the same time, the model must be protected from poisoning attacks from adversarial clients.  Existing solutions address these two problems in isolation. We present FedPerm, a new FL algorithm that addresses both these problems by combining norm bounding for model robustness with a novel intra-model parameter shuffling technique that amplifies data privacy by means of Private Information Retrieval (PIR) based techniques that permit cryptographic aggregation of clients' model updates. The combination of these techniques helps the federation server constrain parameter updates from clients so as to curtail effects of model poisoning attacks by adversarial clients.  We further present FedPerm's unique hyperparameters that can be used effectively to trade off computation overheads with model utility.  Our empirical evaluation on the MNIST dataset demonstrates FedPerm's effectiveness over existing Differential Privacy (DP) enforcement solutions in FL.",
         "['Federated Learning' 'Privacy-Preserving Machine Learning'\n 'Adversarial Machine Learning' 'Distributed Machine Learning'\n 'Cryptography in Machine Learning']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/58149.png",
         "images/58149.png"
        ],
        [
         "1",
         "NeurIPS",
         "2023",
         "70289",
         "Private Federated Frequency Estimation: Adapting to the Hardness of the Instance",
         "In federated frequency estimation (FFE), multiple clients work together to estimate the frequency of their local data by communicating with a server, while maintaining the security constraint of $\\mathtt{secsum}$ where the server can only access the sum of client-held vectors. For FFE with a single communication round, it is known that count sketch is nearly information-theoretically optimal [Chen et al., 2022]. However, when multiple communication rounds are allowed, we propose a new sketch algorithm that is provably more accurate than a naive adaptation of count sketch. Furthermore, we show that both our sketch algorithm and count sketch can achieve better accuracy when the problem instance is simpler. Therefore, we propose a two-phase approach to enable the use of a smaller sketch size for simpler problems. Finally, we provide mechanisms to make our proposed algorithm differentially private. We verify the performance of our methods through experiments conducted on real datasets.",
         "['Federated Learning' 'Privacy-Preserving Machine Learning'\n 'Differential Privacy' 'Distributed Computing'\n 'Data Sketching Algorithms']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70289.png",
         "images/70289.png"
        ],
        [
         "2",
         "ICLR",
         "2024",
         "19285",
         "Time Fairness in Online Knapsack Problems",
         "The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive algorithms are known for this problem, they may be fundamentally unfair, i.e., individual items may be treated inequitably in different ways. We formalize a practically-relevant notion of time fairness which effectively models a trade off between static and dynamic pricing in a motivating application such as cloud resource allocation, and show that existing algorithms perform poorly under this metric.  We propose a parameterized deterministic algorithm where the parameter precisely captures the Pareto-optimal trade-off between fairness (static pricing) and competitiveness (dynamic pricing). We show that randomization is theoretically powerful enough to be simultaneously competitive and fair; however, it does not work well in experiments. To further improve the trade-off between fairness and competitiveness, we develop a nearly-optimal learning-augmented algorithm which is fair, consistent, and robust (competitive), showing substantial performance improvements in numerical experiments.",
         "['Online Algorithms' 'Knapsack Problems' 'Fairness in Algorithms'\n 'Resource Allocation' 'Algorithmic Game Theory'\n 'Learning-Augmented Algorithms']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202024/19285.png",
         "images/19285.png"
        ],
        [
         "3",
         "ICLR",
         "2023",
         "13994",
         "Domain Generalization in Robust Invariant Representation",
         "Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a desirable property for training in resource-constrained settings.",
         "['Domain Generalization' 'Representation Learning' 'Robustness in AI'\n 'Unsupervised Learning']",
         "https://iclr.cc/media/PosterPDFs/ICLR%202023/13994.png",
         "images/13994.png"
        ],
        [
         "4",
         "NeurIPS",
         "2022",
         "62163",
         "SEIFER: Scalable Edge Inference for Deep Neural Networks",
         "Edge inference is becoming ever prevalent through its applications from retail to wearable technology. Clusters of networked resource-constrained edge devices are becoming common, yet there is no production-ready orchestration system for deploying deep learning models over such edge networks which adopts the robustness and scalability of the cloud. We present SEIFER, a framework utilizing a standalone Kubernetes cluster to partition a given DNN and place these partitions in a distributed manner across an edge network, with the goal of maximizing inference throughput. The system is node fault-tolerant and automatically updates deployments based on updates to the model's version. We provide a preliminary evaluation of a partitioning and placement algorithm that works within this framework, and show that we can improve the inference pipeline throughput by 200% by utilizing sufficient numbers of resource-constrained nodes. We have implemented SEIFER in open-source software that is publicly available to the research community.",
         "['Edge Computing' 'Deep Learning' 'Distributed Systems'\n 'Inference Systems' 'Cloud Computing' 'Internet of Things ']",
         "https://neurips.cc/media/PosterPDFs/NeurIPS%202022/62163.png",
         "images/62163.png"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conference</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topics</th>\n",
       "      <th>image_url</th>\n",
       "      <th>local_image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>58149</td>\n",
       "      <td>Private and Robust Federated Learning using Pr...</td>\n",
       "      <td>Federated Learning (FL) is a distributed learn...</td>\n",
       "      <td>[Federated Learning, Privacy-Preserving Machin...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/58149.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>70289</td>\n",
       "      <td>Private Federated Frequency Estimation: Adapti...</td>\n",
       "      <td>In federated frequency estimation (FFE), multi...</td>\n",
       "      <td>[Federated Learning, Privacy-Preserving Machin...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/70289.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19285</td>\n",
       "      <td>Time Fairness in Online Knapsack Problems</td>\n",
       "      <td>The online knapsack problem is a classic probl...</td>\n",
       "      <td>[Online Algorithms, Knapsack Problems, Fairnes...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "      <td>images/19285.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ICLR</td>\n",
       "      <td>2023</td>\n",
       "      <td>13994</td>\n",
       "      <td>Domain Generalization in Robust Invariant Repr...</td>\n",
       "      <td>Unsupervised approaches for learning represent...</td>\n",
       "      <td>[Domain Generalization, Representation Learnin...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202023/1...</td>\n",
       "      <td>images/13994.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>62163</td>\n",
       "      <td>SEIFER: Scalable Edge Inference for Deep Neura...</td>\n",
       "      <td>Edge inference is becoming ever prevalent thro...</td>\n",
       "      <td>[Edge Computing, Deep Learning, Distributed Sy...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/62163.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conference  year  paper_id  \\\n",
       "0    NeurIPS  2022     58149   \n",
       "1    NeurIPS  2023     70289   \n",
       "2       ICLR  2024     19285   \n",
       "3       ICLR  2023     13994   \n",
       "4    NeurIPS  2022     62163   \n",
       "\n",
       "                                               title  \\\n",
       "0  Private and Robust Federated Learning using Pr...   \n",
       "1  Private Federated Frequency Estimation: Adapti...   \n",
       "2          Time Fairness in Online Knapsack Problems   \n",
       "3  Domain Generalization in Robust Invariant Repr...   \n",
       "4  SEIFER: Scalable Edge Inference for Deep Neura...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Federated Learning (FL) is a distributed learn...   \n",
       "1  In federated frequency estimation (FFE), multi...   \n",
       "2  The online knapsack problem is a classic probl...   \n",
       "3  Unsupervised approaches for learning represent...   \n",
       "4  Edge inference is becoming ever prevalent thro...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  [Federated Learning, Privacy-Preserving Machin...   \n",
       "1  [Federated Learning, Privacy-Preserving Machin...   \n",
       "2  [Online Algorithms, Knapsack Problems, Fairnes...   \n",
       "3  [Domain Generalization, Representation Learnin...   \n",
       "4  [Edge Computing, Deep Learning, Distributed Sy...   \n",
       "\n",
       "                                           image_url  local_image_path  \n",
       "0  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  images/58149.png  \n",
       "1  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  images/70289.png  \n",
       "2  https://iclr.cc/media/PosterPDFs/ICLR%202024/1...  images/19285.png  \n",
       "3  https://iclr.cc/media/PosterPDFs/ICLR%202023/1...  images/13994.png  \n",
       "4  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  images/62163.png  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fb658e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docling\n",
      "  Downloading docling-2.64.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (2.11.5)\n",
      "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading docling_core-2.55.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
      "  Downloading docling_parse-4.7.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (10 kB)\n",
      "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
      "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (1.2.0)\n",
      "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m710.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-settings<3.0.0,>=2.3.0 (from docling)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (0.34.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (2.32.5)\n",
      "Collecting ocrmac<2.0.0,>=1.0.0 (from docling)\n",
      "  Downloading ocrmac-1.0.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
      "  Downloading rapidocr-3.4.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (2025.8.3)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
      "  Downloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typer<0.20.0,>=0.12.5 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (0.16.0)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (4.13.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (2.2.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
      "  Downloading marko-2.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (3.1.5)\n",
      "Collecting lxml<7.0.0,>=4.0.0 (from docling)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (10.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (1.0.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m707.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (1.15.3)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling) (1.9.0)\n",
      "Collecting polyfactory>=2.22.2 (from docling)\n",
      "  Downloading polyfactory-3.1.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/miniconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling) (2.7.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling) (0.5.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.13.2)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.19.2)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading tree_sitter-0.25.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Collecting tree-sitter-java<1.0.0,>=0.23.5 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading tree_sitter_java-0.23.5-cp39-abi3-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/miniconda3/lib/python3.12/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.54.0)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /opt/miniconda3/lib/python3.12/site-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.22.1)\n",
      "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (2025.5.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling) (1.1.5)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/miniconda3/lib/python3.12/site-packages (from ocrmac<2.0.0,>=1.0.0->docling) (8.2.1)\n",
      "Collecting pyobjc-framework-Vision (from ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_vision-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: et-xmlfile in /opt/miniconda3/lib/python3.12/site-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
      "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
      "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/miniconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.1.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading pyclipper-1.4.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (8.6 kB)\n",
      "Collecting opencv_python>=4.5.1.48 (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/miniconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling) (1.16.0)\n",
      "Collecting Shapely!=2.0.4,>=1.7.1 (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading shapely-2.1.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting omegaconf (from rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorlog in /opt/miniconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling) (6.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling) (2.1.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/lib/python3.12/site-packages (from typer<0.20.0,>=0.12.5->docling) (14.1.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/miniconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.10.6)\n",
      "Collecting numpy<3.0.0,>=1.17 (from accelerate<2,>=1.0.0->docling)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m703.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/miniconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.1)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
      "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (68.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/miniconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.21.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->rapidocr<4.0.0,>=3.3->docling)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m609.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyobjc-core>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_core-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.8 kB)\n",
      "Collecting pyobjc-framework-Cocoa>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_cocoa-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-Quartz>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_quartz-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting pyobjc-framework-CoreML>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling)\n",
      "  Downloading pyobjc_framework_coreml-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/miniconda3/lib/python3.12/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/miniconda3/lib/python3.12/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.4.0)\n",
      "Downloading docling-2.64.1-py3-none-any.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.8/274.8 kB\u001b[0m \u001b[31m625.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docling_core-2.55.0-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m878.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docling_parse-4.7.2-cp312-cp312-macosx_14_0_arm64.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-6.0.2-cp312-cp312-macosx_10_13_universal2.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading marko-2.2.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ocrmac-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading polyfactory-3.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidocr-3.4.3-py3-none-any.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.3/436.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl (37.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.4.0-cp312-cp312-macosx_10_13_universal2.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading shapely-2.1.2-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter-0.25.2-cp312-cp312-macosx_11_0_arm64.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.3/86.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_java-0.23.5-cp39-abi3-macosx_11_0_arm64.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_vision-12.1-cp312-cp312-macosx_10_13_universal2.whl (16 kB)\n",
      "Downloading pyobjc_core-12.1-cp312-cp312-macosx_10_13_universal2.whl (678 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m678.3/678.3 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_cocoa-12.1-cp312-cp312-macosx_10_13_universal2.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.6/384.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_coreml-12.1-cp312-cp312-macosx_10_13_universal2.whl (11 kB)\n",
      "Downloading pyobjc_framework_quartz-12.1-cp312-cp312-macosx_10_13_universal2.whl (218 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pylatexenc, antlr4-python3-runtime\n",
      "  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136818 sha256=acaa865c3b03c0ad0ae12a4b1d50f4424858453df69ff00d50962f37d62865fc\n",
      "  Stored in directory: /Users/majdshammout/Library/Caches/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=475c8f7079fe075a7d1a0a475efd5833a454b2fbb0aea8269ee8a47c1209a579\n",
      "  Stored in directory: /Users/majdshammout/Library/Caches/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built pylatexenc antlr4-python3-runtime\n",
      "Installing collected packages: pylatexenc, antlr4-python3-runtime, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-java, tree-sitter-c, tree-sitter, rtree, pypdfium2, pyobjc-core, pyclipper, omegaconf, numpy, mpire, marko, lxml, latex2mathml, jsonref, jsonlines, faker, Shapely, python-pptx, python-docx, pyobjc-framework-Cocoa, polyfactory, opencv_python, semchunk, rapidocr, pyobjc-framework-Quartz, pyobjc-framework-CoreML, pydantic-settings, pyobjc-framework-Vision, docling-core, ocrmac, docling-parse, docling-ibm-models, docling\n",
      "  Attempting uninstall: pyobjc-core\n",
      "    Found existing installation: pyobjc-core 11.0\n",
      "    Uninstalling pyobjc-core-11.0:\n",
      "      Successfully uninstalled pyobjc-core-11.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.2\n",
      "    Uninstalling numpy-2.3.2:\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "  Attempting uninstall: pyobjc-framework-Cocoa\n",
      "    Found existing installation: pyobjc-framework-Cocoa 11.0\n",
      "    Uninstalling pyobjc-framework-Cocoa-11.0:\n",
      "      Successfully uninstalled pyobjc-framework-Cocoa-11.0\n",
      "Successfully installed Shapely-2.1.2 XlsxWriter-3.2.9 antlr4-python3-runtime-4.9.3 docling-2.64.1 docling-core-2.55.0 docling-ibm-models-3.10.3 docling-parse-4.7.2 faker-38.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 lxml-6.0.2 marko-2.2.1 mpire-2.10.2 numpy-2.2.6 ocrmac-1.0.0 omegaconf-2.3.0 opencv_python-4.12.0.88 polyfactory-3.1.0 pyclipper-1.4.0 pydantic-settings-2.12.0 pylatexenc-2.10 pyobjc-core-12.1 pyobjc-framework-Cocoa-12.1 pyobjc-framework-CoreML-12.1 pyobjc-framework-Quartz-12.1 pyobjc-framework-Vision-12.1 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.4.3 rtree-1.4.1 semchunk-2.2.2 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-java-0.23.5 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43edf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc151bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/58149.png'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6b08a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = df.iloc[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cebbe260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 17:34:52,539 - INFO - detected formats: [<InputFormat.IMAGE: 'image'>]\n",
      "2025-12-10 17:34:52,612 - INFO - Going to convert document batch...\n",
      "2025-12-10 17:34:52,613 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-10 17:34:52,623 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-10 17:34:52,625 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-10 17:34:52,634 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-10 17:34:52,636 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-10 17:34:58,473 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-12-10 17:34:58,484 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-10 17:34:58,486 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-10 17:34:58,491 - INFO - Accelerator device: 'mps'\n",
      "2025-12-10 17:35:08,665 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-10 17:35:08,666 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-10 17:35:35,878 - INFO - Accelerator device: 'mps'\n",
      "2025-12-10 17:35:36,214 - INFO - Processing document 58149.png\n",
      "2025-12-10 17:35:55,442 - INFO - Finished converting document 58149.png in 62.91 sec.\n"
     ]
    }
   ],
   "source": [
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "908ba21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/58149.png'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ab4704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Private and Robust Federated Learning using Private Information Retrieval and Norm Bounding\n",
      "\n",
      "## Introduction and Motivation\n",
      "\n",
      "- Federated Learning (FL) is a distributed learning paradigm that enables mutually untrusting clients to collaboratively train a common machine learning model.\n",
      "- At the same time, the model must be protected from poisoning attacks from adversarial clients.\n",
      "- Client data privacy is paramount in FL.\n",
      "- We present FedPerm, a new FL algorithm that addresses both these problems by combining\n",
      "- with a novel intra-model parameter shuffling technique that amplifies data privacy by means of computational Private Information Retrieval (cPIR) based techniques that permit secure aggregation of clients' model updates.\n",
      "- norm bounding for model robustness\n",
      "\n",
      "## Differential Privacy in FL\n",
      "\n",
      "- (a) Central differential privacy in FL (CDP-FL)\n",
      "- (b) Local differential privacy in FL (LDP-FL)\n",
      "- (c) Privacy amplification by shuffling clients' updates\n",
      "- (d) Shuffling the parameters (FedPerm)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(b) Local Privacy\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## (d) Shuffling Parameters (FedPerm)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## FedPerm Design\n",
      "\n",
      "FedPerm training involves following steps (Algorithm 1):\n",
      "\n",
      "- Key distribution (before FedPerm starts)\n",
      "- FedPerm requires an independent key server that generates a pair of public and secret homomorphic keys (Pk, Sk). This key pair is distributed to all federation clients, and just the public key Pk is sent to the federation server (for aggregation).\n",
      "- Client: Randomizing update parameters (line 6-8)\n",
      "- Client: Local training (line 5)\n",
      "- Client: Shuffling the noisy parameters(line 9-10)\n",
      "- Server: norm bounding (line 15)\n",
      "- Client: Generating PIR queries (line 11-12)\n",
      "- Note that unlike other robust AGRs, norm bounding is the only robust AGR scheme that does not require the true position of the parameters.\n",
      "- Server: secure agregation (line 16)\n",
      "- This aggregation is unshuffling and averaging the update parameters for the collected updates.\n",
      "- Client: Updating the global model (line 17-19)\n",
      "- The result of server aggregation is encrypted, so it needs to be decrypted at the clients.\n",
      "\n",
      "Algorithm 1 FedPerm where green and blue colors show execution by server and client respectively.\n",
      "\n",
      "Innut: number of FL rounds T. number of local enochs F. number of number of model parameters d. parameter update clipping threshold C Output: 07\n",
      "\n",
      "```\n",
      "1: 0° + Initialize weights 2: for each iteration t € [T) do U + set of n randomly selected clients out of N total clients for u in U do O\"+ LOCALUPDATE (Og, n, E) oi - CLP(Ot, -C,C) ôt + (õt + C)/(2C) Yu + RANDOMIZE(Ot,, Ed) 9: Tu + Shuffling pattern RANDOMPERMUTATIONS € [1, d) 10: 11: 12: 13: b* + BINARYMASK (Tre) ct + ENCpk (bl) Client u sends (It, Ct) to the server end for 15: norm bounding: Yu + Yu · min(1, TI foru E U 18: normalize = + C. (22 - 1) update model oi+1 += 20: end for 21: return o]\n",
      "```\n",
      "\n",
      "Hamid Mozaffari, Virendra J. Marathe, Dave Dice\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "LABS\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Computation/Communication and Utility Tradeoff in FedPerm\n",
      "\n",
      "We propose two ways to adjust the computation/communication vs. utility trade off in FedPerm:\n",
      "\n",
      "- FedPerm with Smaller Shuffling Pattern (k1)\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "redperm example with k1=3 and d=12. Ine red boxes are showing the windows that the parameters inside them are coina to be shuffled with the same shufflina nattern.\n",
      "\n",
      "- FedPerm with Multiple Shuffling Patterns (k2)\n",
      "\n",
      "FedPerm examole with k1=3 and k2=2. We have two shufilina batterns shown with red and blue boxes\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Empirical Results\n",
      "\n",
      "We compare the test accuracy of the model trained using different FL algorithms running for T=50 rounds on MNIST.\n",
      "\n",
      "Here, we have two versions of FedPerm:\n",
      "\n",
      "- Light FedPerm (k1=400, k2=1): requires 54.6 seconds and 21 minutes for client and server time.\n",
      "- Heavy FedPerm (k1=800, k2=10): requires 32.1 minutes and 16.4 hours for client and server time.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n"
     ]
    }
   ],
   "source": [
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5f104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiofiles\n",
      "  Downloading aiofiles-25.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading aiofiles-25.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: aiofiles\n",
      "Successfully installed aiofiles-25.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1483b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utilities\n",
    "# =============================================================================\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Thread-safe rate limiter.\"\"\"\n",
    "    \n",
    "    def __init__(self, calls_per_second: float = 0.5):\n",
    "        self.min_interval = 1.0 / calls_per_second\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_call = 0.0\n",
    "    \n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            elapsed = now - self.last_call\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_call = time.time()\n",
    "\n",
    "\n",
    "def _make_session(\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    "    status_forcelist: tuple = (429, 500, 502, 503, 504),\n",
    ") -> requests.Session:\n",
    "    \"\"\"Create a session with automatic retry on failures.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retry))\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "    return session\n",
    "\n",
    "\n",
    "def _is_valid_pdf(content: bytes) -> bool:\n",
    "    \"\"\"Check if content is actually a PDF.\"\"\"\n",
    "    return len(content) > 4 and content[:4] == b'%PDF'\n",
    "\n",
    "\n",
    "def _safe_filename(title: str, paper_id: str, source: str) -> str:\n",
    "    \"\"\"Create a safe filename from title and ID.\"\"\"\n",
    "    safe_title = re.sub(r'[^\\w\\s-]', '', title)[:50].strip()\n",
    "    safe_id = paper_id.replace('/', '_')\n",
    "    return f\"{source}_{safe_id}_{safe_title}.pdf\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Threaded Version (with rate limiting and retries)\n",
    "# =============================================================================\n",
    "\n",
    "def download_papers(\n",
    "    df: pd.DataFrame,\n",
    "    title_col: str,\n",
    "    output_dir: str = \"papers\",\n",
    "    path_col: str = \"local_pdf_path\",\n",
    "    source_col: str = \"source\",\n",
    "    error_col: str | None = \"error\",\n",
    "    max_workers: int = 2,\n",
    "    timeout: int = 30,\n",
    "    show_progress: bool = True,\n",
    "    # Rate limiting options\n",
    "    rate_limit: bool = True,\n",
    "    calls_per_second: float = 0.5,\n",
    "    # Retry options\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    "    # Source options\n",
    "    try_arxiv: bool = True,\n",
    "    try_openreview: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download papers from arXiv and OpenReview by searching title.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing paper titles.\n",
    "    title_col : str\n",
    "        Name of the column with paper titles.\n",
    "    output_dir : str, optional\n",
    "        Directory to save PDFs into, by default \"papers\".\n",
    "    path_col : str, optional\n",
    "        Name of the column to store local file paths, by default \"local_pdf_path\".\n",
    "    source_col : str, optional\n",
    "        Name of the column to store the source (arxiv/openreview), by default \"source\".\n",
    "    error_col : str | None, optional\n",
    "        Name of the column to store error messages, by default \"error\".\n",
    "        Set to None to disable error logging.\n",
    "    max_workers : int, optional\n",
    "        Max number of threads for parallel downloads, by default 2.\n",
    "    timeout : int, optional\n",
    "        Timeout (seconds) for each HTTP request, by default 30.\n",
    "    show_progress : bool, optional\n",
    "        Whether to show a tqdm progress bar, by default True.\n",
    "    rate_limit : bool, optional\n",
    "        Whether to enable rate limiting, by default True.\n",
    "    calls_per_second : float, optional\n",
    "        Max API calls per second when rate limiting, by default 0.5.\n",
    "    retries : int, optional\n",
    "        Number of retries on failure, by default 3.\n",
    "    backoff_factor : float, optional\n",
    "        Backoff multiplier between retries, by default 2.0.\n",
    "    try_arxiv : bool, optional\n",
    "        Whether to search arXiv, by default True.\n",
    "    try_openreview : bool, optional\n",
    "        Whether to search OpenReview, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The same DataFrame with extra columns for local paths, source, and errors.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = [None] * len(df)\n",
    "    sources = [None] * len(df)\n",
    "    errors = [None] * len(df)\n",
    "\n",
    "    # Set up rate limiter and session\n",
    "    limiter = RateLimiter(calls_per_second) if rate_limit else None\n",
    "    session = _make_session(retries=retries, backoff_factor=backoff_factor)\n",
    "\n",
    "    def _search_arxiv(title: str) -> tuple[bytes | None, str | None, str | None]:\n",
    "        \"\"\"Returns (content, paper_id, error_message).\"\"\"\n",
    "        if limiter:\n",
    "            limiter.wait()\n",
    "        \n",
    "        base_url = \"http://export.arxiv.org/api/query\"\n",
    "        clean_title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "        \n",
    "        params = {'search_query': f'ti:\"{clean_title}\"', 'max_results': 1}\n",
    "        resp = session.get(base_url, params=params, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        id_match = re.search(r'<id>http://arxiv.org/abs/([^<]+)</id>', resp.text)\n",
    "        if not id_match:\n",
    "            return None, None, \"not found on arxiv\"\n",
    "        \n",
    "        arxiv_id = id_match.group(1)\n",
    "        \n",
    "        if limiter:\n",
    "            limiter.wait()\n",
    "        \n",
    "        pdf_resp = session.get(f\"https://arxiv.org/pdf/{arxiv_id}.pdf\", timeout=timeout)\n",
    "        pdf_resp.raise_for_status()\n",
    "        \n",
    "        if not _is_valid_pdf(pdf_resp.content):\n",
    "            return None, None, f\"arxiv returned invalid PDF for {arxiv_id}\"\n",
    "        \n",
    "        return pdf_resp.content, arxiv_id, None\n",
    "\n",
    "    def _search_openreview(title: str) -> tuple[bytes | None, str | None, str | None]:\n",
    "        \"\"\"Returns (content, paper_id, error_message).\"\"\"\n",
    "        if limiter:\n",
    "            limiter.wait()\n",
    "        \n",
    "        base_url = \"https://api.openreview.net/notes/search\"\n",
    "        resp = session.get(base_url, params={'query': title, 'limit': 5}, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        \n",
    "        if not data.get('notes'):\n",
    "            return None, None, \"not found on openreview\"\n",
    "        \n",
    "        # Find matching title\n",
    "        for note in data['notes']:\n",
    "            note_title = note.get('content', {}).get('title', '')\n",
    "            if isinstance(note_title, dict):\n",
    "                note_title = note_title.get('value', '')\n",
    "            \n",
    "            if note_title.lower().strip() == title.lower().strip():\n",
    "                note_id = note['id']\n",
    "                \n",
    "                if limiter:\n",
    "                    limiter.wait()\n",
    "                \n",
    "                pdf_resp = session.get(\n",
    "                    f\"https://openreview.net/pdf?id={note_id}\",\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                \n",
    "                if pdf_resp.status_code == 200 and _is_valid_pdf(pdf_resp.content):\n",
    "                    return pdf_resp.content, note_id, None\n",
    "                else:\n",
    "                    return None, None, f\"openreview returned invalid PDF for {note_id}\"\n",
    "        \n",
    "        return None, None, \"title mismatch on openreview\"\n",
    "\n",
    "    def _download_one(idx: int, title: str) -> tuple[int, str | None, str | None, str | None]:\n",
    "        \"\"\"Returns (idx, path, source, error).\"\"\"\n",
    "        if pd.isna(title) or not isinstance(title, str) or not title.strip():\n",
    "            return idx, None, None, \"invalid title\"\n",
    "\n",
    "        error_messages = []\n",
    "\n",
    "        # Try arXiv\n",
    "        if try_arxiv:\n",
    "            try:\n",
    "                content, paper_id, err = _search_arxiv(title)\n",
    "                if content:\n",
    "                    filename = _safe_filename(title, paper_id, 'arxiv')\n",
    "                    full_path = os.path.join(output_dir, filename)\n",
    "                    with open(full_path, 'wb') as f:\n",
    "                        f.write(content)\n",
    "                    return idx, full_path, 'arxiv', None\n",
    "                if err:\n",
    "                    error_messages.append(err)\n",
    "            except Exception as e:\n",
    "                error_messages.append(f\"arxiv error: {e}\")\n",
    "\n",
    "        # Try OpenReview\n",
    "        if try_openreview:\n",
    "            try:\n",
    "                content, paper_id, err = _search_openreview(title)\n",
    "                if content:\n",
    "                    filename = _safe_filename(title, paper_id, 'openreview')\n",
    "                    full_path = os.path.join(output_dir, filename)\n",
    "                    with open(full_path, 'wb') as f:\n",
    "                        f.write(content)\n",
    "                    return idx, full_path, 'openreview', None\n",
    "                if err:\n",
    "                    error_messages.append(err)\n",
    "            except Exception as e:\n",
    "                error_messages.append(f\"openreview error: {e}\")\n",
    "\n",
    "        return idx, None, None, \"; \".join(error_messages) if error_messages else \"no sources enabled\"\n",
    "\n",
    "    # Parallel download\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(_download_one, idx, row[title_col]): idx\n",
    "            for idx, row in df.iterrows()\n",
    "        }\n",
    "\n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(futures), desc=\"Downloading papers\")\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            idx, path, source, error = future.result()\n",
    "            results[idx] = path\n",
    "            sources[idx] = source\n",
    "            errors[idx] = error\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "\n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "\n",
    "    df = df.copy()\n",
    "    df[path_col] = results\n",
    "    df[source_col] = sources\n",
    "    if error_col:\n",
    "        df[error_col] = errors\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Async Version (faster for large datasets)\n",
    "# =============================================================================\n",
    "\n",
    "async def download_papers_async(\n",
    "    df: pd.DataFrame,\n",
    "    title_col: str,\n",
    "    output_dir: str = \"papers\",\n",
    "    path_col: str = \"local_pdf_path\",\n",
    "    source_col: str = \"source\",\n",
    "    error_col: str | None = \"error\",\n",
    "    max_concurrent: int = 10,\n",
    "    delay: float = 0.2,\n",
    "    timeout: int = 30,\n",
    "    show_progress: bool = True,\n",
    "    try_arxiv: bool = True,\n",
    "    try_openreview: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Async version of paper downloader - faster for large datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing paper titles.\n",
    "    title_col : str\n",
    "        Name of the column with paper titles.\n",
    "    output_dir : str, optional\n",
    "        Directory to save PDFs into, by default \"papers\".\n",
    "    path_col : str, optional\n",
    "        Name of the column to store local file paths, by default \"local_pdf_path\".\n",
    "    source_col : str, optional\n",
    "        Name of the column to store the source, by default \"source\".\n",
    "    error_col : str | None, optional\n",
    "        Name of the column to store error messages, by default \"error\".\n",
    "    max_concurrent : int, optional\n",
    "        Max concurrent downloads, by default 10.\n",
    "    delay : float, optional\n",
    "        Delay between requests in seconds, by default 0.2.\n",
    "    timeout : int, optional\n",
    "        Timeout for each request in seconds, by default 30.\n",
    "    show_progress : bool, optional\n",
    "        Whether to show progress bar, by default True.\n",
    "    try_arxiv : bool, optional\n",
    "        Whether to search arXiv, by default True.\n",
    "    try_openreview : bool, optional\n",
    "        Whether to search OpenReview, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added columns for paths, sources, and errors.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    results = {}\n",
    "    client_timeout = aiohttp.ClientTimeout(total=timeout)\n",
    "\n",
    "    async def _search_arxiv(\n",
    "        session: aiohttp.ClientSession, title: str\n",
    "    ) -> tuple[bytes | None, str | None, str | None]:\n",
    "        clean_title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "        params = {'search_query': f'ti:\"{clean_title}\"', 'max_results': 1}\n",
    "        \n",
    "        async with session.get(\n",
    "            \"http://export.arxiv.org/api/query\", params=params\n",
    "        ) as resp:\n",
    "            text = await resp.text()\n",
    "        \n",
    "        id_match = re.search(r'<id>http://arxiv.org/abs/([^<]+)</id>', text)\n",
    "        if not id_match:\n",
    "            return None, None, \"not found on arxiv\"\n",
    "        \n",
    "        arxiv_id = id_match.group(1)\n",
    "        await asyncio.sleep(delay)\n",
    "        \n",
    "        async with session.get(f\"https://arxiv.org/pdf/{arxiv_id}.pdf\") as pdf_resp:\n",
    "            content = await pdf_resp.read()\n",
    "        \n",
    "        if _is_valid_pdf(content):\n",
    "            return content, arxiv_id, None\n",
    "        return None, None, f\"arxiv returned invalid PDF for {arxiv_id}\"\n",
    "\n",
    "    async def _search_openreview(\n",
    "        session: aiohttp.ClientSession, title: str\n",
    "    ) -> tuple[bytes | None, str | None, str | None]:\n",
    "        params = {'query': title, 'limit': 5}\n",
    "        \n",
    "        async with session.get(\n",
    "            \"https://api.openreview.net/notes/search\", params=params\n",
    "        ) as resp:\n",
    "            data = await resp.json()\n",
    "        \n",
    "        if not data.get('notes'):\n",
    "            return None, None, \"not found on openreview\"\n",
    "        \n",
    "        for note in data['notes']:\n",
    "            note_title = note.get('content', {}).get('title', '')\n",
    "            if isinstance(note_title, dict):\n",
    "                note_title = note_title.get('value', '')\n",
    "            \n",
    "            if note_title.lower().strip() == title.lower().strip():\n",
    "                note_id = note['id']\n",
    "                await asyncio.sleep(delay)\n",
    "                \n",
    "                async with session.get(\n",
    "                    f\"https://openreview.net/pdf?id={note_id}\"\n",
    "                ) as pdf_resp:\n",
    "                    content = await pdf_resp.read()\n",
    "                \n",
    "                if _is_valid_pdf(content):\n",
    "                    return content, note_id, None\n",
    "                return None, None, f\"openreview returned invalid PDF for {note_id}\"\n",
    "        \n",
    "        return None, None, \"title mismatch on openreview\"\n",
    "\n",
    "    async def _download_one(\n",
    "        session: aiohttp.ClientSession, idx: int, title: str\n",
    "    ) -> tuple[int, str | None, str | None, str | None]:\n",
    "        if pd.isna(title) or not isinstance(title, str) or not title.strip():\n",
    "            return idx, None, None, \"invalid title\"\n",
    "        \n",
    "        async with semaphore:\n",
    "            await asyncio.sleep(delay)\n",
    "            error_messages = []\n",
    "            \n",
    "            # Try arXiv\n",
    "            if try_arxiv:\n",
    "                try:\n",
    "                    content, paper_id, err = await _search_arxiv(session, title)\n",
    "                    if content:\n",
    "                        filename = _safe_filename(title, paper_id, 'arxiv')\n",
    "                        full_path = os.path.join(output_dir, filename)\n",
    "                        async with aiofiles.open(full_path, 'wb') as f:\n",
    "                            await f.write(content)\n",
    "                        return idx, full_path, 'arxiv', None\n",
    "                    if err:\n",
    "                        error_messages.append(err)\n",
    "                except Exception as e:\n",
    "                    error_messages.append(f\"arxiv error: {e}\")\n",
    "            \n",
    "            # Try OpenReview\n",
    "            if try_openreview:\n",
    "                try:\n",
    "                    content, paper_id, err = await _search_openreview(session, title)\n",
    "                    if content:\n",
    "                        filename = _safe_filename(title, paper_id, 'openreview')\n",
    "                        full_path = os.path.join(output_dir, filename)\n",
    "                        async with aiofiles.open(full_path, 'wb') as f:\n",
    "                            await f.write(content)\n",
    "                        return idx, full_path, 'openreview', None\n",
    "                    if err:\n",
    "                        error_messages.append(err)\n",
    "                except Exception as e:\n",
    "                    error_messages.append(f\"openreview error: {e}\")\n",
    "            \n",
    "            return idx, None, None, \"; \".join(error_messages) if error_messages else \"no sources enabled\"\n",
    "\n",
    "    async with aiohttp.ClientSession(\n",
    "        timeout=client_timeout,\n",
    "        headers={'User-Agent': 'PaperDownloader/1.0 (Academic Research; mailto:your@email.edu)'}\n",
    "    ) as session:\n",
    "        tasks = [\n",
    "            _download_one(session, idx, row[title_col])\n",
    "            for idx, row in df.iterrows()\n",
    "        ]\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(tasks), desc=\"Downloading papers (async)\")\n",
    "        \n",
    "        for coro in asyncio.as_completed(tasks):\n",
    "            idx, path, source, error = await coro\n",
    "            results[idx] = (path, source, error)\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "\n",
    "    df = df.copy()\n",
    "    df[path_col] = [results[i][0] for i in range(len(df))]\n",
    "    df[source_col] = [results[i][1] for i in range(len(df))]\n",
    "    if error_col:\n",
    "        df[error_col] = [results[i][2] for i in range(len(df))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Checkpoint wrapper (for large datasets)\n",
    "# =============================================================================\n",
    "\n",
    "def download_with_checkpoints(\n",
    "    df: pd.DataFrame,\n",
    "    title_col: str,\n",
    "    batch_size: int = 500,\n",
    "    checkpoint_path: str = \"checkpoint.csv\",\n",
    "    use_async: bool = True,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download papers in batches with checkpointing for fault tolerance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing paper titles.\n",
    "    title_col : str\n",
    "        Name of the column with paper titles.\n",
    "    batch_size : int, optional\n",
    "        Number of papers per batch, by default 500.\n",
    "    checkpoint_path : str, optional\n",
    "        Path to save checkpoint CSV, by default \"checkpoint.csv\".\n",
    "    use_async : bool, optional\n",
    "        Whether to use async version, by default True.\n",
    "    **kwargs\n",
    "        Additional arguments passed to download function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Complete DataFrame with all results.\n",
    "    \"\"\"\n",
    "    # Resume from checkpoint if exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        done_df = pd.read_csv(checkpoint_path)\n",
    "        done_titles = set(done_df[title_col].tolist())\n",
    "        remaining_df = df[~df[title_col].isin(done_titles)].copy()\n",
    "        print(f\"Resuming: {len(done_df)} done, {len(remaining_df)} remaining\")\n",
    "    else:\n",
    "        done_df = pd.DataFrame()\n",
    "        remaining_df = df.copy()\n",
    "    \n",
    "    if len(remaining_df) == 0:\n",
    "        print(\"All papers already downloaded!\")\n",
    "        return done_df\n",
    "    \n",
    "    # Process in batches\n",
    "    batches = np.array_split(remaining_df, max(1, len(remaining_df) // batch_size))\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Batch {i+1}/{len(batches)} ({len(batch)} papers)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        if use_async:\n",
    "            result = asyncio.run(download_papers_async(batch, title_col=title_col, **kwargs))\n",
    "        else:\n",
    "            result = download_papers(batch, title_col=title_col, **kwargs)\n",
    "        \n",
    "        # Append to checkpoint\n",
    "        done_df = pd.concat([done_df, result], ignore_index=True)\n",
    "        done_df.to_csv(checkpoint_path, index=False)\n",
    "        \n",
    "        # Print batch summary\n",
    "        success = result[result['local_pdf_path'].notna()]\n",
    "        print(f\"\\nBatch complete: {len(success)}/{len(batch)} successful\")\n",
    "        print(f\"Total progress: {len(done_df)}/{len(df)} ({100*len(done_df)/len(df):.1f}%)\")\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    return done_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Usage Examples\n",
    "# =============================================================================\n",
    "\n",
    "    # Example DataFrame\n",
    "    # df = pd.read_csv('papers.csv')\n",
    "    \n",
    "    # --- Option 1: Simple threaded download ---\n",
    "    # df = download_papers(df, title_col='title')\n",
    "    \n",
    "    # --- Option 2: Faster async download ---\n",
    "    # df = asyncio.run(download_papers_async(df, title_col='title', max_concurrent=10))\n",
    "    \n",
    "    # --- Option 3: Large dataset with checkpoints (recommended for 16k papers) ---\n",
    "    # df = download_with_checkpoints(\n",
    "    #     df,\n",
    "    #     title_col='title',\n",
    "    #     batch_size=500,\n",
    "    #     checkpoint_path='download_checkpoint.csv',\n",
    "    #     use_async=True,\n",
    "    #     max_concurrent=10,\n",
    "    #     delay=0.2,\n",
    "    # )\n",
    "    \n",
    "    # --- Option 4: Only arXiv, more aggressive ---\n",
    "    # df = download_with_checkpoints(\n",
    "    #     df,\n",
    "    #     title_col='title',\n",
    "    #     try_openreview=False,\n",
    "    #     max_concurrent=15,\n",
    "    #     delay=0.1,\n",
    "    # )\n",
    "    \n",
    "    # Save final results\n",
    "    # df.to_csv('papers_downloaded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b041c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Batch 1/20 (516 papers)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45adc1b5ee840fa86896d6b752a46a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers (async):   0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() \n",
    "\n",
    "df = download_with_checkpoints(\n",
    "    df,\n",
    "    title_col='title',\n",
    "    batch_size=500,\n",
    "    checkpoint_path='download_checkpoint.csv',\n",
    "    use_async=True,\n",
    "    max_concurrent=10,\n",
    ")\n",
    "df.to_csv('papers_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe650b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/majdshammout/Documents/Projects/paper-poster-finetuning\n",
      "True\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Your current directory\n",
    "print(os.path.exists('papers'))  # Does the folder exist?\n",
    "print(os.listdir('papers') if os.path.exists('papers') else 'No papers folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e04a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_session(\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    "    status_forcelist: tuple = (429, 500, 502, 503, 504),\n",
    ") -> requests.Session:\n",
    "    \"\"\"Create a session with automatic retry on failures.\"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'PaperDownloader/1.0 (Academic Research; mailto:your@email.edu)'\n",
    "    })\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retry))\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47325fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Batch 1/103 (101 papers)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b59334131f4acab8484f2417933504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = download_with_checkpoints(\n",
    "    df,\n",
    "    title_col='title',\n",
    "    batch_size=100,          # Smaller batches\n",
    "    use_async=False,         # Threaded is easier to throttle\n",
    "    max_workers=1,           # Sequential\n",
    "    calls_per_second=0.2,    # 1 request per 5 seconds\n",
    "    try_openreview=False,    # Test arXiv alone first\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c759e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<?xml version='1.0' encoding='UTF-8'?>\n",
      "<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <id>https://arxiv.org/api/Of4nuPKF5/ZylXkWdgoIdNFRiLU</id>\n",
      "  <title>arXiv Query: search_query=ti:\"A Fast and Provable Algorithm for Sparse Phase Retrieval\"&amp;id_list=&amp;start=0&amp;max_results=1</title>\n",
      "  <updated>2025-12-12T01:11:33Z</updated>\n",
      "  <link href=\"https://arxiv.org/api/query?search_query=ti:%22A+Fast+and+Provable+Algorithm+for+Sparse+Phase+Retrieval%22&amp;start=0&amp;max_results=1&amp;id_list=\" type=\"application/atom+xml\"/>\n",
      "  <opensearch:itemsPerPage>1</opensearch:itemsPerPage>\n",
      "  <opensearch:totalResults>1</opensearch:totalResults>\n",
      "  <opensearch:startIndex>0</opensearch:startIndex>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2309.02046v2</id>\n",
      "    <title>A Fast and Provable Algorithm for Sparse Phase Retrieval</title>\n",
      "    <updated>2024-03-19T13:59:56Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2309.02046v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2309.02046v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We study the sparse phase retrieval problem, which seeks to recover a sparse signal from a limited set of magnitude-only measurements. In contrast to prevalent sparse phase retrieval algorithms that primarily use first-order methods, we propose an innovative second-order algorithm that employs a Newton-type method with hard thres\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "headers = {'User-Agent': 'PaperDownloader/1.0 (Academic Research; mailto:your@email.edu)'}\n",
    "title = df['title'].iloc[0]\n",
    "clean_title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "\n",
    "resp = requests.get(\n",
    "    \"http://export.arxiv.org/api/query\",\n",
    "    params={'search_query': f'ti:\"{clean_title}\"', 'max_results': 1},\n",
    "    headers=headers\n",
    ")\n",
    "print(resp.status_code)\n",
    "print(resp.text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94c2c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# =============================================================================\n",
    "# Config\n",
    "# =============================================================================\n",
    "\n",
    "USER_AGENT = 'PaperDownloader/1.0 (Academic Research; mailto:your@email.edu)'\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utilities\n",
    "# =============================================================================\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Thread-safe rate limiter.\"\"\"\n",
    "    \n",
    "    def __init__(self, calls_per_second: float = 0.5):\n",
    "        self.min_interval = 1.0 / calls_per_second\n",
    "        self.lock = threading.Lock()\n",
    "        self.last_call = 0.0\n",
    "    \n",
    "    def wait(self):\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            elapsed = now - self.last_call\n",
    "            if elapsed < self.min_interval:\n",
    "                time.sleep(self.min_interval - elapsed)\n",
    "            self.last_call = time.time()\n",
    "\n",
    "\n",
    "def _make_session(\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    "    status_forcelist: tuple = (429, 500, 502, 503, 504),\n",
    ") -> requests.Session:\n",
    "    \"\"\"Create a session with automatic retry on failures.\"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': USER_AGENT})\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retry))\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "    return session\n",
    "\n",
    "\n",
    "def _is_valid_pdf(content: bytes) -> bool:\n",
    "    \"\"\"Check if content is actually a PDF.\"\"\"\n",
    "    return len(content) > 4 and content[:4] == b'%PDF'\n",
    "\n",
    "\n",
    "def _safe_filename(title: str, paper_id: str, source: str) -> str:\n",
    "    \"\"\"Create a safe filename from title and ID.\"\"\"\n",
    "    safe_title = re.sub(r'[^\\w\\s-]', '', title)[:50].strip()\n",
    "    safe_id = paper_id.replace('/', '_')\n",
    "    return f\"{source}_{safe_id}_{safe_title}.pdf\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Threaded Version\n",
    "# =============================================================================\n",
    "\n",
    "def download_papers(\n",
    "    df: pd.DataFrame,\n",
    "    title_col: str,\n",
    "    output_dir: str = \"papers\",\n",
    "    path_col: str = \"local_pdf_path\",\n",
    "    source_col: str = \"source\",\n",
    "    error_col: str | None = \"error\",\n",
    "    max_workers: int = 2,\n",
    "    timeout: int = 30,\n",
    "    show_progress: bool = True,\n",
    "    rate_limit: bool = True,\n",
    "    calls_per_second: float = 0.5,\n",
    "    retries: int = 3,\n",
    "    backoff_factor: float = 2.0,\n",
    "    try_arxiv: bool = True,\n",
    "    try_openreview: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Download papers from arXiv and OpenReview by searching title.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = [None] * len(df)\n",
    "    sources = [None] * len(df)\n",
    "    errors = [None] * len(df)\n",
    "\n",
    "    limiter = RateLimiter(calls_per_second) if rate_limit else None\n",
    "    session = _make_session(retries=retries, backoff_factor=backoff_factor)\n",
    "\n",
    "    def _search_arxiv(title: str) -> tuple[bytes | None, str | None, str | None]:\n",
    "        if limiter:\n",
    "            limiter.wait()\n",
    "        \n",
    "        clean_title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "        params = {'search_query': f'ti:\"{clean_title}\"', 'max_results': 1}\n",
    "        resp = session.get(\n",
    "            \"http://export.arxiv.org/api/query\",\n",
    "            params=params,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        id_match = re.search(r'<id>http://arxiv.org/abs/([^<]+)</id>', resp.text)\n",
    "        if not id_match:\n",
    "            return None, None, \"not found on arxiv\"\n",
    "        \n",
    "        arxiv_id = id_match.group(1)\n",
    "        \n",
    "        if limiter:\n",
    "            limiter.wait()\n",
    "        \n",
    "        pdf_resp = session.get(f\"https://arxiv.org/pdf/{arxiv_id}.pdf\", timeout=timeout)\n",
    "        pdf_resp.raise_for_status()\n",
    "        \n",
    "        if not _is_valid_pdf(pdf_resp.content):\n",
    "            return None, None, f\"arxiv returned invalid PDF for {arxiv_id}\"\n",
    "        \n",
    "        return pdf_resp.content, arxiv_id, None\n",
    "\n",
    "    def _search_openreview(title: str) -> tuple[bytes | None, str | None, str | None]:\n",
    "        if limiter:\n",
    "            limiter.wait()\n",
    "        \n",
    "        resp = session.get(\n",
    "            \"https://api.openreview.net/notes/search\",\n",
    "            params={'query': title, 'limit': 5},\n",
    "            timeout=timeout\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        \n",
    "        if not data.get('notes'):\n",
    "            return None, None, \"not found on openreview\"\n",
    "        \n",
    "        for note in data['notes']:\n",
    "            note_title = note.get('content', {}).get('title', '')\n",
    "            if isinstance(note_title, dict):\n",
    "                note_title = note_title.get('value', '')\n",
    "            \n",
    "            if note_title.lower().strip() == title.lower().strip():\n",
    "                note_id = note['id']\n",
    "                \n",
    "                if limiter:\n",
    "                    limiter.wait()\n",
    "                \n",
    "                pdf_resp = session.get(\n",
    "                    f\"https://openreview.net/pdf?id={note_id}\",\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                \n",
    "                if pdf_resp.status_code == 200 and _is_valid_pdf(pdf_resp.content):\n",
    "                    return pdf_resp.content, note_id, None\n",
    "                else:\n",
    "                    return None, None, f\"openreview returned invalid PDF for {note_id}\"\n",
    "        \n",
    "        return None, None, \"title mismatch on openreview\"\n",
    "\n",
    "    def _download_one(idx: int, title: str) -> tuple[int, str | None, str | None, str | None]:\n",
    "        if pd.isna(title) or not isinstance(title, str) or not title.strip():\n",
    "            return idx, None, None, \"invalid title\"\n",
    "\n",
    "        error_messages = []\n",
    "\n",
    "        if try_arxiv:\n",
    "            try:\n",
    "                content, paper_id, err = _search_arxiv(title)\n",
    "                if content:\n",
    "                    filename = _safe_filename(title, paper_id, 'arxiv')\n",
    "                    full_path = os.path.join(output_dir, filename)\n",
    "                    with open(full_path, 'wb') as f:\n",
    "                        f.write(content)\n",
    "                    return idx, full_path, 'arxiv', None\n",
    "                if err:\n",
    "                    error_messages.append(err)\n",
    "            except Exception as e:\n",
    "                error_messages.append(f\"arxiv error: {e}\")\n",
    "\n",
    "        if try_openreview:\n",
    "            try:\n",
    "                content, paper_id, err = _search_openreview(title)\n",
    "                if content:\n",
    "                    filename = _safe_filename(title, paper_id, 'openreview')\n",
    "                    full_path = os.path.join(output_dir, filename)\n",
    "                    with open(full_path, 'wb') as f:\n",
    "                        f.write(content)\n",
    "                    return idx, full_path, 'openreview', None\n",
    "                if err:\n",
    "                    error_messages.append(err)\n",
    "            except Exception as e:\n",
    "                error_messages.append(f\"openreview error: {e}\")\n",
    "\n",
    "        return idx, None, None, \"; \".join(error_messages) if error_messages else \"no sources enabled\"\n",
    "\n",
    "    # Use iloc for reliable indexing\n",
    "    indices = list(range(len(df)))\n",
    "    titles = df[title_col].tolist()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(_download_one, idx, titles[idx]): idx\n",
    "            for idx in indices\n",
    "        }\n",
    "\n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(futures), desc=\"Downloading papers\")\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            idx, path, source, error = future.result()\n",
    "            results[idx] = path\n",
    "            sources[idx] = source\n",
    "            errors[idx] = error\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "\n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "\n",
    "    df = df.copy()\n",
    "    df[path_col] = results\n",
    "    df[source_col] = sources\n",
    "    if error_col:\n",
    "        df[error_col] = errors\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Async Version\n",
    "# =============================================================================\n",
    "\n",
    "async def download_papers_async(\n",
    "    df: pd.DataFrame,\n",
    "    title_col: str,\n",
    "    output_dir: str = \"papers\",\n",
    "    path_col: str = \"local_pdf_path\",\n",
    "    source_col: str = \"source\",\n",
    "    error_col: str | None = \"error\",\n",
    "    max_concurrent: int = 10,\n",
    "    delay: float = 0.2,\n",
    "    timeout: int = 30,\n",
    "    show_progress: bool = True,\n",
    "    try_arxiv: bool = True,\n",
    "    try_openreview: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Async version - faster for large datasets.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    results = {}\n",
    "    client_timeout = aiohttp.ClientTimeout(total=timeout)\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "\n",
    "    async def _search_arxiv(\n",
    "        session: aiohttp.ClientSession, title: str\n",
    "    ) -> tuple[bytes | None, str | None, str | None]:\n",
    "        clean_title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "        params = {'search_query': f'ti:\"{clean_title}\"', 'max_results': 1}\n",
    "        \n",
    "        async with session.get(\n",
    "            \"http://export.arxiv.org/api/query\", params=params\n",
    "        ) as resp:\n",
    "            text = await resp.text()\n",
    "        \n",
    "        id_match = re.search(r'<id>http://arxiv.org/abs/([^<]+)</id>', text)\n",
    "        if not id_match:\n",
    "            return None, None, \"not found on arxiv\"\n",
    "        \n",
    "        arxiv_id = id_match.group(1)\n",
    "        await asyncio.sleep(delay)\n",
    "        \n",
    "        async with session.get(f\"https://arxiv.org/pdf/{arxiv_id}.pdf\") as pdf_resp:\n",
    "            content = await pdf_resp.read()\n",
    "        \n",
    "        if _is_valid_pdf(content):\n",
    "            return content, arxiv_id, None\n",
    "        return None, None, f\"arxiv returned invalid PDF for {arxiv_id}\"\n",
    "\n",
    "    async def _search_openreview(\n",
    "        session: aiohttp.ClientSession, title: str\n",
    "    ) -> tuple[bytes | None, str | None, str | None]:\n",
    "        params = {'query': title, 'limit': 5}\n",
    "        \n",
    "        async with session.get(\n",
    "            \"https://api.openreview.net/notes/search\", params=params\n",
    "        ) as resp:\n",
    "            data = await resp.json()\n",
    "        \n",
    "        if not data.get('notes'):\n",
    "            return None, None, \"not found on openreview\"\n",
    "        \n",
    "        for note in data['notes']:\n",
    "            note_title = note.get('content', {}).get('title', '')\n",
    "            if isinstance(note_title, dict):\n",
    "                note_title = note_title.get('value', '')\n",
    "            \n",
    "            if note_title.lower().strip() == title.lower().strip():\n",
    "                note_id = note['id']\n",
    "                await asyncio.sleep(delay)\n",
    "                \n",
    "                async with session.get(\n",
    "                    f\"https://openreview.net/pdf?id={note_id}\"\n",
    "                ) as pdf_resp:\n",
    "                    content = await pdf_resp.read()\n",
    "                \n",
    "                if _is_valid_pdf(content):\n",
    "                    return content, note_id, None\n",
    "                return None, None, f\"openreview returned invalid PDF for {note_id}\"\n",
    "        \n",
    "        return None, None, \"title mismatch on openreview\"\n",
    "\n",
    "    async def _download_one(\n",
    "        session: aiohttp.ClientSession, idx: int, title: str\n",
    "    ) -> tuple[int, str | None, str | None, str | None]:\n",
    "        if pd.isna(title) or not isinstance(title, str) or not title.strip():\n",
    "            return idx, None, None, \"invalid title\"\n",
    "        \n",
    "        async with semaphore:\n",
    "            await asyncio.sleep(delay)\n",
    "            error_messages = []\n",
    "            \n",
    "            if try_arxiv:\n",
    "                try:\n",
    "                    content, paper_id, err = await _search_arxiv(session, title)\n",
    "                    if content:\n",
    "                        filename = _safe_filename(title, paper_id, 'arxiv')\n",
    "                        full_path = os.path.join(output_dir, filename)\n",
    "                        async with aiofiles.open(full_path, 'wb') as f:\n",
    "                            await f.write(content)\n",
    "                        return idx, full_path, 'arxiv', None\n",
    "                    if err:\n",
    "                        error_messages.append(err)\n",
    "                except Exception as e:\n",
    "                    error_messages.append(f\"arxiv error: {e}\")\n",
    "            \n",
    "            if try_openreview:\n",
    "                try:\n",
    "                    content, paper_id, err = await _search_openreview(session, title)\n",
    "                    if content:\n",
    "                        filename = _safe_filename(title, paper_id, 'openreview')\n",
    "                        full_path = os.path.join(output_dir, filename)\n",
    "                        async with aiofiles.open(full_path, 'wb') as f:\n",
    "                            await f.write(content)\n",
    "                        return idx, full_path, 'openreview', None\n",
    "                    if err:\n",
    "                        error_messages.append(err)\n",
    "                except Exception as e:\n",
    "                    error_messages.append(f\"openreview error: {e}\")\n",
    "            \n",
    "            return idx, None, None, \"; \".join(error_messages) if error_messages else \"no sources enabled\"\n",
    "\n",
    "    titles = df[title_col].tolist()\n",
    "    \n",
    "    async with aiohttp.ClientSession(timeout=client_timeout, headers=headers) as session:\n",
    "        tasks = [\n",
    "            _download_one(session, idx, titles[idx])\n",
    "            for idx in range(len(df))\n",
    "        ]\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(tasks), desc=\"Downloading papers (async)\")\n",
    "        \n",
    "        for coro in asyncio.as_completed(tasks):\n",
    "            idx, path, source, error = await coro\n",
    "            results[idx] = (path, source, error)\n",
    "            if show_progress:\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "\n",
    "    df = df.copy()\n",
    "    df[path_col] = [results[i][0] for i in range(len(df))]\n",
    "    df[source_col] = [results[i][1] for i in range(len(df))]\n",
    "    if error_col:\n",
    "        df[error_col] = [results[i][2] for i in range(len(df))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Checkpoint Wrapper\n",
    "# =============================================================================\n",
    "\n",
    "def download_with_checkpoints(\n",
    "    df: pd.DataFrame,\n",
    "    title_col: str,\n",
    "    batch_size: int = 500,\n",
    "    checkpoint_path: str = \"checkpoint.csv\",\n",
    "    use_async: bool = True,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Download in batches with checkpointing.\"\"\"\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        done_df = pd.read_csv(checkpoint_path)\n",
    "        done_titles = set(done_df[title_col].tolist())\n",
    "        remaining_df = df[~df[title_col].isin(done_titles)].copy().reset_index(drop=True)\n",
    "        print(f\"Resuming: {len(done_df)} done, {len(remaining_df)} remaining\")\n",
    "    else:\n",
    "        done_df = pd.DataFrame()\n",
    "        remaining_df = df.copy().reset_index(drop=True)\n",
    "    \n",
    "    if len(remaining_df) == 0:\n",
    "        print(\"All papers already downloaded!\")\n",
    "        return done_df\n",
    "    \n",
    "    batches = np.array_split(remaining_df, max(1, len(remaining_df) // batch_size))\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Batch {i+1}/{len(batches)} ({len(batch)} papers)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        batch = batch.reset_index(drop=True)\n",
    "        \n",
    "        if use_async:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            result = loop.run_until_complete(\n",
    "                download_papers_async(batch, title_col=title_col, **kwargs)\n",
    "            )\n",
    "        else:\n",
    "            result = download_papers(batch, title_col=title_col, **kwargs)\n",
    "        \n",
    "        done_df = pd.concat([done_df, result], ignore_index=True)\n",
    "        done_df.to_csv(checkpoint_path, index=False)\n",
    "        \n",
    "        success = result[result['local_pdf_path'].notna()]\n",
    "        print(f\"\\nBatch complete: {len(success)}/{len(batch)} successful\")\n",
    "        print(f\"Total progress: {len(done_df)}/{len(df)} ({100*len(done_df)/len(df):.1f}%)\")\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    return done_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e3e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Batch 1/20 (516 papers)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa1a866419e41e99c9049efac273baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading papers (async):   0%|          | 0/516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_checkpoint.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      4\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload_checkpoint.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_with_checkpoints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownload_checkpoint.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Conservative\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Half second between requests\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 438\u001b[0m, in \u001b[0;36mdownload_with_checkpoints\u001b[0;34m(df, title_col, batch_size, checkpoint_path, use_async, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_async:\n\u001b[1;32m    437\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m--> 438\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_papers_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     result \u001b[38;5;241m=\u001b[39m download_papers(batch, title_col\u001b[38;5;241m=\u001b[39mtitle_col, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/selectors.py:566\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Delete old checkpoint to start fresh\n",
    "import os\n",
    "if os.path.exists('download_checkpoint.csv'):\n",
    "    os.remove('download_checkpoint.csv')\n",
    "\n",
    "df = download_with_checkpoints(\n",
    "    df,\n",
    "    title_col='title',\n",
    "    batch_size=500,\n",
    "    checkpoint_path='download_checkpoint.csv',\n",
    "    use_async=True,\n",
    "    max_concurrent=5,   # Conservative\n",
    "    delay=0.5,          # Half second between requests\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23586230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Fast and Provable Algorithm for Sparse Phase Retrieval\n",
      "\n",
      "Step 1 - Search status: 200\n",
      "Step 2 - Found ID: 2309.02046v2\n",
      "Step 3 - PDF download status: 200\n",
      "Step 3 - Content length: 1853\n",
      "Step 3 - Is valid PDF: False\n",
      "Step 4 - Saved to: papers/test_paper.pdf\n",
      "Step 4 - File exists: True\n",
      "Step 4 - File size: 1853 bytes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Test the full flow manually\n",
    "title = df['title'].iloc[0]\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Step 1: Search\n",
    "headers = {'User-Agent': 'PaperDownloader/1.0 (Academic Research; mailto:your@email.edu)'}\n",
    "clean_title = re.sub(r'[^\\w\\s]', ' ', title)\n",
    "params = {'search_query': f'ti:\"{clean_title}\"', 'max_results': 1}\n",
    "\n",
    "resp = requests.get(\n",
    "    \"http://export.arxiv.org/api/query\",\n",
    "    params=params,\n",
    "    headers=headers\n",
    ")\n",
    "print(f\"Step 1 - Search status: {resp.status_code}\")\n",
    "\n",
    "# Step 2: Extract ID\n",
    "id_match = re.search(r'<id>http://arxiv.org/abs/([^<]+)</id>', resp.text)\n",
    "if id_match:\n",
    "    arxiv_id = id_match.group(1)\n",
    "    print(f\"Step 2 - Found ID: {arxiv_id}\")\n",
    "else:\n",
    "    print(\"Step 2 - FAILED: No ID found\")\n",
    "    print(resp.text[:500])\n",
    "\n",
    "# Step 3: Download PDF\n",
    "pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "pdf_resp = requests.get(pdf_url, headers=headers)\n",
    "print(f\"Step 3 - PDF download status: {pdf_resp.status_code}\")\n",
    "print(f\"Step 3 - Content length: {len(pdf_resp.content)}\")\n",
    "print(f\"Step 3 - Is valid PDF: {pdf_resp.content[:4] == b'%PDF'}\")\n",
    "\n",
    "# Step 4: Save\n",
    "os.makedirs('papers', exist_ok=True)\n",
    "test_path = 'papers/test_paper.pdf'\n",
    "with open(test_path, 'wb') as f:\n",
    "    f.write(pdf_resp.content)\n",
    "print(f\"Step 4 - Saved to: {test_path}\")\n",
    "print(f\"Step 4 - File exists: {os.path.exists(test_path)}\")\n",
    "print(f\"Step 4 - File size: {os.path.getsize(test_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a26e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have any checkpoint data\n",
    "if os.path.exists('download_checkpoint.csv'):\n",
    "    check = pd.read_csv('download_checkpoint.csv')\n",
    "    print(check[['title', 'source', 'error']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd9fa329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n       <html>\\n     <head>\\n       <title>arXiv reCAPTCHA</title>\\n       <link rel=\"stylesheet\" type=\"text/css\" media=\"screen\" href=\"https://static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215\" />\\n       <script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\\n       <script>\\n         var submitForm = function () {\\n             document.forms[\\'rrr\\'].submit();\\n         }\\n       </script>\\n     </head>\\n\\n     <body class=\"with-cu-identity\">\\n\\n       <div id=\"cu-identity\"'\n",
      "{'Connection': 'close', 'Content-Length': '1853', 'Server': 'Varnish', 'Retry-After': '0', 'Cache-Control': 'private, no-store', 'Content-Type': 'text/html', 'Accept-Ranges': 'bytes', 'Date': 'Fri, 12 Dec 2025 02:12:42 GMT', 'Via': '1.1 varnish', 'X-Served-By': 'cache-lga21934-LGA', 'X-Cache': 'HIT', 'X-Timer': 'S1765505563.839085,VS0,VE0'}\n"
     ]
    }
   ],
   "source": [
    "print(pdf_resp.content[:500])\n",
    "print(pdf_resp.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fef738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /opt/miniconda3/lib/python3.12/site-packages (from arxiv) (2.32.5)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
      "Downloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ee91af06e9f0ac2e395e5dc2d5cb624eda99a9ebed032ff15585e51ae62ca1fa\n",
      "  Stored in directory: /Users/majdshammout/Library/Caches/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.3.1 feedparser-6.0.12 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd75f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def download_papers_arxiv_package(\n",
    "    df,\n",
    "    title_col,\n",
    "    output_dir=\"papers\",\n",
    "    delay=3.0\n",
    "):\n",
    "    \"\"\"Use official arxiv package - handles rate limits properly.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    client = arxiv.Client(\n",
    "        page_size=1,\n",
    "        delay_seconds=delay,\n",
    "        num_retries=3\n",
    "    )\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        title = row[title_col]\n",
    "        result_data = {'title': title, 'path': None, 'arxiv_id': None, 'error': None}\n",
    "        \n",
    "        try:\n",
    "            search = arxiv.Search(query=f'ti:\"{title}\"', max_results=1)\n",
    "            paper = next(client.results(search), None)\n",
    "            \n",
    "            if paper:\n",
    "                filename = f\"{paper.get_short_id()}_{title[:50]}.pdf\"\n",
    "                filename = \"\".join(c for c in filename if c.isalnum() or c in ' ._-')\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                paper.download_pdf(filename=filepath)\n",
    "                \n",
    "                result_data['path'] = filepath\n",
    "                result_data['arxiv_id'] = paper.get_short_id()\n",
    "            else:\n",
    "                result_data['error'] = 'not found'\n",
    "                \n",
    "        except Exception as e:\n",
    "            result_data['error'] = str(e)\n",
    "        \n",
    "        results.append(result_data)\n",
    "        time.sleep(delay)  # Extra safety\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    result_df['local_pdf_path'] = [r['path'] for r in results]\n",
    "    result_df['arxiv_id'] = [r['arxiv_id'] for r in results]\n",
    "    result_df['error'] = [r['error'] for r in results]\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9949bc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2b58ef1b1e4ea280f87e3aad937388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = download_papers_arxiv_package(df, title_col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b519d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f44c27f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['local_pdf_path'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f624bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e69833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7a654535f24c0e99948489e0275ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate = download_papers_arxiv_package(validate, title_col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88373ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.to_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5341f09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>conference</th>\n",
       "      <th>year</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topics</th>\n",
       "      <th>image_url</th>\n",
       "      <th>local_image_path</th>\n",
       "      <th>local_pdf_path</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>58149</td>\n",
       "      <td>Private and Robust Federated Learning using Pr...</td>\n",
       "      <td>Federated Learning (FL) is a distributed learn...</td>\n",
       "      <td>['Federated Learning' 'Privacy-Preserving Mach...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/58149.png</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>not found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2023</td>\n",
       "      <td>70289</td>\n",
       "      <td>Private Federated Frequency Estimation: Adapti...</td>\n",
       "      <td>In federated frequency estimation (FFE), multi...</td>\n",
       "      <td>['Federated Learning' 'Privacy-Preserving Mach...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/70289.png</td>\n",
       "      <td>papers/2306.09396v2_Private Federated Frequenc...</td>\n",
       "      <td>2306.09396v2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ICLR</td>\n",
       "      <td>2024</td>\n",
       "      <td>19285</td>\n",
       "      <td>Time Fairness in Online Knapsack Problems</td>\n",
       "      <td>The online knapsack problem is a classic probl...</td>\n",
       "      <td>['Online Algorithms' 'Knapsack Problems' 'Fair...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202024/1...</td>\n",
       "      <td>images/19285.png</td>\n",
       "      <td>papers/2305.13293v2_Time Fairness in Online Kn...</td>\n",
       "      <td>2305.13293v2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ICLR</td>\n",
       "      <td>2023</td>\n",
       "      <td>13994</td>\n",
       "      <td>Domain Generalization in Robust Invariant Repr...</td>\n",
       "      <td>Unsupervised approaches for learning represent...</td>\n",
       "      <td>['Domain Generalization' 'Representation Learn...</td>\n",
       "      <td>https://iclr.cc/media/PosterPDFs/ICLR%202023/1...</td>\n",
       "      <td>images/13994.png</td>\n",
       "      <td>papers/2304.03431v2_Domain Generalization in R...</td>\n",
       "      <td>2304.03431v2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>2022</td>\n",
       "      <td>62163</td>\n",
       "      <td>SEIFER: Scalable Edge Inference for Deep Neura...</td>\n",
       "      <td>Edge inference is becoming ever prevalent thro...</td>\n",
       "      <td>['Edge Computing' 'Deep Learning' 'Distributed...</td>\n",
       "      <td>https://neurips.cc/media/PosterPDFs/NeurIPS%20...</td>\n",
       "      <td>images/62163.png</td>\n",
       "      <td>papers/2210.12218v2_SEIFER Scalable Edge Infer...</td>\n",
       "      <td>2210.12218v2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 conference  year  paper_id  \\\n",
       "0           0    NeurIPS  2022     58149   \n",
       "1           1    NeurIPS  2023     70289   \n",
       "2           2       ICLR  2024     19285   \n",
       "3           3       ICLR  2023     13994   \n",
       "4           4    NeurIPS  2022     62163   \n",
       "\n",
       "                                               title  \\\n",
       "0  Private and Robust Federated Learning using Pr...   \n",
       "1  Private Federated Frequency Estimation: Adapti...   \n",
       "2          Time Fairness in Online Knapsack Problems   \n",
       "3  Domain Generalization in Robust Invariant Repr...   \n",
       "4  SEIFER: Scalable Edge Inference for Deep Neura...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Federated Learning (FL) is a distributed learn...   \n",
       "1  In federated frequency estimation (FFE), multi...   \n",
       "2  The online knapsack problem is a classic probl...   \n",
       "3  Unsupervised approaches for learning represent...   \n",
       "4  Edge inference is becoming ever prevalent thro...   \n",
       "\n",
       "                                              topics  \\\n",
       "0  ['Federated Learning' 'Privacy-Preserving Mach...   \n",
       "1  ['Federated Learning' 'Privacy-Preserving Mach...   \n",
       "2  ['Online Algorithms' 'Knapsack Problems' 'Fair...   \n",
       "3  ['Domain Generalization' 'Representation Learn...   \n",
       "4  ['Edge Computing' 'Deep Learning' 'Distributed...   \n",
       "\n",
       "                                           image_url  local_image_path  \\\n",
       "0  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  images/58149.png   \n",
       "1  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  images/70289.png   \n",
       "2  https://iclr.cc/media/PosterPDFs/ICLR%202024/1...  images/19285.png   \n",
       "3  https://iclr.cc/media/PosterPDFs/ICLR%202023/1...  images/13994.png   \n",
       "4  https://neurips.cc/media/PosterPDFs/NeurIPS%20...  images/62163.png   \n",
       "\n",
       "                                      local_pdf_path      arxiv_id      error  \n",
       "0                                               None          None  not found  \n",
       "1  papers/2306.09396v2_Private Federated Frequenc...  2306.09396v2       None  \n",
       "2  papers/2305.13293v2_Time Fairness in Online Kn...  2305.13293v2       None  \n",
       "3  papers/2304.03431v2_Domain Generalization in R...  2304.03431v2       None  \n",
       "4  papers/2210.12218v2_SEIFER Scalable Edge Infer...  2210.12218v2       None  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfce94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
