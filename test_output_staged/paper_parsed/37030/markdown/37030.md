# Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Denis Rakitin $^{*1}$ Ivan Shchekotov $^{*12}$ Dmitry Vetrov $^{3}$

## Abstract

Diffusion distillation methods aim to compress the diffusion models into efficient one-step generators while trying to preserve quality. Among them, Distribution Matching Distillation (DMD) offers a suitable framework for training general-form onestep generators, applicable beyond unconditional generation. In this work, we introduce its modification, called Regularized Distribution Matching Distillation, applicable to unpaired image-toimage problems. We demonstrate its empirical performance in application to several translation tasks, including 2D examples and I2I between different image datasets, where it performs on par or better than multi-step diffusion baselines.

## 1. Introduction

One of the global problems of contemporary generative modeling consists of solving the so-called generative learning trilemma ( Xiao et al. , 2021 ) . It states that a perfect generative model should possess three desirable properties: high generation quality, mode coverage/diversity of samples and efficient inference. Today, most model families tend to have only 2 of the 3. Generative Adversarial Networks (GANs) ( Goodfellow et al. , 2014 ) have fast inference and produce high-quality samples but tend to underrepresent some modes of the data set ( Metz et al. , 2016 ; Arjovsky et al. , 2017 ) . Variational Autoencoders (VAEs) ( Kingma & Welling , 2013 ; Rezende et al. , 2014 ) efficiently produce diverse samples while suffering from insufficient generation quality. Finally, diffusion-based generative models ( Ho et al. , 2020 ; Song et al. , 2020 ; Dhariwal & Nichol , 2021 ; Karras et al. , 2022 ) achieve SOTA generative metrics and

visual quality yet require running a high-cost multi-step inference procedure.

Satisfying these three properties is essential in numerous generative computer vision tasks beyond unconditional generation. One is image-to-image (I2I) translation ( Isola et al. , 2017 ; Zhu et al. , 2017 ) , which consists of learning a mapping between two distributions that preserves the crossdomain properties of input object while appropriately changing its source-domain features to match the target. Most examples, like transforming cats into dogs ( Choi et al. , 2020 ) or human faces into anime ( Kohrith et al. , 2022 ) belong to the unplained I2I because they do not assume ground truth pairs of objects in the data set. As in unconditional generation, shared fixed-parameter diffusion models use various GANs ( Huang et al. , 2018 ; Park et al. , 2020 ; Choi et al. , 2020 ; Zheng et al. , 2022 ) , but now tend to be compacted and surpassed by diffusion-based counterparts ( Choi et al. , 2021 ; Meng et al. , 2021 ; Zhao et al. , 2022 ; Wu & De Torre , 2023 ) . Most of these methods build on top of the original diffusion sampling procedure and tend to have high generation time as a consequence.

Since diffusion models succeed in both desirable qualitative properties of the trilemma, one could theoretically obtain samples of the desired quality level given sufficient computational resources. It makes the acceleration of diffusion models an appealing approach to satisfy all of the aforementioned requirements, including efficient inference.

Recently introduced diffusion distillation techniques (Soug et al., 2023; Kim et al., 2023b; Sauer et al., 2023) address this challenge by compressing diffusion models into onestep students with (hopefully) similar qualitative and quantitative properties. Among them, Distribution Matching Distillation (DMD) (Yin et al., 2023; Nguyen & Tran, 2023) offers an expressive and general framework for training freeform generators based on these methods. The structure form of DMDs (Yin et al., 2023; Nguyen et al., 2023) produce here means that the method does not make any assumptions about the generator's structure and distribution at the input. This crucial observation opens a large space for its applications beyond the noise $\rightarrow$ data problems.

In this work, we introduce the modification of DMD, called

Equal contribution 1HSE University, Moscow, Russia

$^{2}$Skolkovo Institute of Science and Technology, Moscow, Russia

*Correspondence: University, Bremen, Germany. Correspondence to: E.B. (E.B.0341@i.cuhk.edu). E-mail address: e.beguidas@cuhk. cuhk.edu.

Accepted by the Structured Probability Inference & Generative Modelling workshop © ICML 2024, Vienna, Austria. Copyright

1

arXiv:2406.14762v1 [cs.CV] 20 Jun 2024

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

![Figure](figures/37030_page_002_figure_001.png)

Figure 1. Illustration of performance of the proposed RDMD model on $cat \to wild$ translation problem/ from the AFHQv2 ( Choi et al. , 2020 ) data set.

Regularized Distribution Matching Distillation (RMD), that applies to the unpaired I2I problems. To achieve this, we replace the generator's input noise with the source data samples to further translate them into the target. We maintain the overall optimization parameters in state and output by regularizing the objective with the transport cost between them. As our main contributions, we

- 1. Propose a one-step diffusion-based method for un-
paired I2I;
2. Theoretically verify it by establishing its connection
with optimal transport ( Villani et al. , 2009 ; Peyré et al. ,
2019 ) ;
3. Ablate its qualitative properties and demonstrate its
generation quality on 2D and image-to-image exam-
ples, where it obtains comparable or better results than
the multi-step counterparts.
## 2. Background

### 2.1. Diffusion Models

Diffusion models (Song & Ermon, 2019; Ho et al., 2020) are a class of models that sequentially perturb data distribution $p^k$ with Gaussian noise, transforming it into some tractable parametric distribution, which contains no information about initial domain.

Using this distribution as a prior and reversing the process by progressively removing the noise yields a sampling procedure from $p(\mathbf{x})$ . A convenient way to formalize diffusion models is through stochastic differential equations (SDEs) defined in 2020 as a novel process that time stochastic dynamics of particles. The forward process is commonly defined as the Variance Exploding (VE) SDE:

$$d x_{t}=g(t) d w_{t},\quad (1)$$

where $t \in [0, T]$, $x_{0} \sim \mathcal{U}_{d \times d}$, $q(\cdot )$ is the scalar diffusion coefficient and $dw_t$ is the differential of a standard Wiener process. We denote by $p_t(x_t)$ marginal distribution of $x_t$, so that $p^{data}(x_0) = p_0(x_t)$, $p_T$ acts as an unstructured prior distribution that we can sample from.

Conveniently, SDE dynamics can be represented via a deterministic counterpart given by an ordinary differential equation (ODE), which yields the same marginal distributions $p_i(x_0) = e^{i\theta(x_0)}f_i(x_0)$ , given the same initial distribution

$$\begin{array} { r l } {  d  x _ { t } } & {  = - \frac { 1 } { 2 } \beta ^ { t } ( t ) \nabla _ { x } \log p _ { t } ( x _ { t } )  d  t . } \end{array} \quad ( 2 )$$

where $\nabla_{x_i} \log p_i(x_i)$ is called the score function of $p_i(x_i)$ . Equation 2 is also called Probability Flow ODE (PF-ODE). The ODE formulation allows us to obtain a backward process by simply reversing velocity of the particle. In particular we can obtain samples from $p^{ab}$ by taking $x_i' \approx p_{ij}$ and then using the PF-ODE backwards in time, given access to the score function.

However, in the case of generative modeling $\nabla_{\theta} \log p_i(\theta)$ , is intractable due to $p^{data}$ being intractable, and thus cannot be used directly in Equation 2 . Under mild regularity conditions, the unconditional score can be expressed by:

$$\nabla_{x_{i}} \log p_{t}\left(x_{i}\right)=E_{p_{0: t}\left(x_{0}\right) \mid x_{0}}\left[s_{t \mid 0}\left(x_{0} ; x_{0}\right)\right] .\quad (3)$$

where $s_{r_i}(x_i|x_0) = \sqrt{\pi}$, $\log p_{r_i}(x_i|x_0)$ is the conditional distribution (also called perturbation kernel) and $p_{r_i}(x_i|x_0)$ is the corresponding posterior distribution. The perturbation kernel in the case of VE-SDE corresponds to simply adding an independent Gaussian noise:

$$p _ { n _ { 0 } } ( x _ { t } | x _ { 0 } ) = \mathcal { N } ( x _ { t } | x _ { 0 } , \sigma _ { t } ^ { 2 } ) , \sigma _ { t } ^ { 2 } = \int _ { y } ^ { 1 } g ^ { 2 } ( y )  d  y . \quad ( 4 )$$

Denoising Score Matching (DSM) (Vincent, 2011) utilizes Equation and approximates $\nabla_{\theta} \log p_t(\sigma_t)$ with the score

$^{1}$The other popular forward processes (e.g. VP-SDE) can be obtained by scaling the VE-SDE.

2

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

model $s_{i}^{\theta}(\boldsymbol{x}_{t})$ via L2-regression minimization:

$$\int _ { 0 } ^ { T } \mathbb { E } _ { \rho ( t ) | ( x _ { 0 } | x _ { 1 } ) } \| s _ { t } ^ { \theta } ( x _ { t } ) - s _ { t } ( 0 ) ( x _ { t } | x _ { 0 } ) \| ^ { 2 }  d  t \to \operatorname* { m i n } _ { \theta } , ( 5 )$$

where $\beta_i$ is some positive weighting function. The minimum in the Equation 5 is obtained at $s_i(x)_i = -\nabla_{x_i}\log p_i(x_i)$ . Given a suitable parameterization of the score network, DSM objective is equivalent to

$$\int _ { 0 } ^ { T } \beta _ { t } \mathbb { E } _ { \rho _ { 0 , t } ( x _ { 0 } , x _ { t } ) } [ D _ { t } ^ { \rho } ( x _ { t } ) - x _ { 0 } ] ^ { 2 } d t \to \operatorname* { m i n } _ { \theta } . \quad ( 6 )$$

Where $D_i^0$ is called the denoising network (or simply denoise) and is related to the score network via $s_i'(x_i) = \langle  x_i - D_i^0(x_i) \rangle / \sigma_i^2$ . Therefore, Denoising Score Matching (DSM-M) consists of learning to denoise images at various noise levels:

Having obtained $s_T^0(x_t)$ , we solve Equation 2 backward in time, starting from $x_T \sim \mathcal{N}(0, \sigma_T^2 I)$ to obtain approximate samples from $p^{data}_{T}$ .

### 2.2. Distribution Matching Distillation

Distribution Matching Distillation (Yin et al., 2023) is the core technique of this paper. Essentially, it aims to train a generator $G_\theta(z)$ on matching the given distribution $p^{\text{nat}}$ . Its input $z$ is assumed to come from a tractable input distribution $p$ , such that the inputs of $G_\theta$ and $p$ can be be achieved by optimizing the KL divergence between the distribution: $p^*$ of $G_\theta(z)$ and the data distribution $p^{\text{nat}}$ .

$$\begin{array} { r } {  L L  ( \rho ^ { \prime } | \rho ^ {  p o l  } ) = \mathbb { E } _ { \rho ^ {  p o l  } : z \sim \log } \left[ \rho ^ {  p o l  } ( G ( z ) | z ) \right] - \operatorname* { m i n } _ { g } \quad ( 7 ) } \end{array}$$

Differentiating it by the parameters $\theta$ , using the chain rule, one encounters a summand, containing the difference $s^\alpha(\hat{G}_0(z)) - s^{\text{cal}}(\hat{G}_0(z))$ between the score functions of the corresponding distributions $^1$ . The pure data score function can be very non-smooth due to the Manifold Hypothesis (Tenenbaum et al., 2000) and is generally hard to train (Song & Ermon, 2019) , so the authors make the probability distribution of the original ones, but not necessarily end, they replace the original loss with an ensemble of KL divergences between distributions, perturbed by the forward diffusion process:

$$\int _ { T } ^ { T _ {  ~ \scriptsize ~ c ~  } } \omega \,  K L  ( \hat { p } _ { t } ^ { \pi } \| p _ { t } ^ {  ~ \scriptsize ~ c ~  } )  d  t . \quad ( 8 )$$

Here, $\omega_p$ is a weighting function, $p'_1$ and $p'_2$ are the perturbed versions of the generator distribution and $p_{rad}$ up to the time step $t$ . In theory, the minima of Equation 8 objective coincides (Wang et al., 2024, Thm. 1) with the original minima from Equation 7 . Meanwhile in practice taking the gradient of the new loss, which can be equivalently written as

$$\int _ { 0 } ^ { T } \int _ { \Omega } \mathbb { E } _ { \mathcal { N } ( 0 , t ) } p ^ {  s o r  } ( x ) \log \frac { p _ { t } ^ { \theta } ( G _ { 0 } ( x ) + \sigma _ { t } \varepsilon ) } { p _ { t } ^ {  r e l  } ( G _ { 0 } ( x ) + \sigma _ { t } \varepsilon ) }  d  t . \quad   ( 9 )$$

results in obtaining difference $s_i^d(G_\theta(z) + \sigma_i z) - s_i^real(G_\theta(z) + \sigma_i z)$, which can be approximated by the diffusion models.

Given this, authors approximate $s^{real}_{t}$ with the pre-trained diffusion model, which we will denote $\hat{s}^{real}_{t}$ and as well with a slight abuse of notation. The whole procedure now can be considered as distillation of $\hat{s}^{real}_{t}$ into $G_{t}$. At the same time, $s^{*}_{t}$ is the score of the noised distribution of the generator, which is intractable and therefore approximated by an additional "fake" diffusion model $s^{*}_{t}$ and the corresponding denoise $D_{t}$. It is natural on the other hand that the score that is computed by the generator is the generator's samples at the time $T$. The joint training procedure is essentially the coordinate descent

$$\left\{ \begin{array} { l c } { \displaystyle \int _ { 0 } ^ { T } \omega _ { e , x } \log \frac { p _ { t } ^ { 2 } ( G _ { 0 } ( t ) + \sigma _ { t } \varepsilon ) } { p _ { t } ^ { 2 } ( G _ { 0 } ( t ) + \sigma _ { t } \varepsilon ) }  d  t \to \operatorname* { m i n } ; } \\ { \displaystyle \int _ { 0 } ^ { T } \beta _ { t } \varepsilon _ { x , x } \| D _ { t } ^ { 2 } ( G _ { 0 } ( t ) + \sigma _ { t } \varepsilon ) - G _ { 0 } ( t ) \| ^ { 2 }  d  t \to \operatorname* { m i n } . } \end{array} \right.$$
(10)$$

where the stochastic gradient with respect to the fake network $z$ is given by (2) . The loss with the generator's stochastic gradient is calculated directly as

$$\begin{array} { r } { \dot { \omega } _ { i } \left( s _ { i } ^ { 0 } - s _ { i } ^ {  e x t  } \right) \nabla _ {  g  } G _ { 0 } (  z  ) , \quad ( 1 1 ) } \end{array}$$

where the scores are evaluated in the point $G_0(z) + \sigma\varepsilon z$ . Minimization of the fake network's objective ensures $s_i^b \leq  s_i^a = p_i^a$ . $s_i^b \leftrightarrow s_i^a = p_i^a$ . At this condition, the generator's objective is equal to the original ensemble of KL divergences from Equation 8 , minimizing which solves the initial problem and implies $p^i = p_i^{\text{real}}$

### 2.3. Unpaired I2I and optimal transport

The problem of unsuared I2I consists of learning a mapping G between the source distribution $p^S$ and the target distribution $p^T$ given the corresponding independent data sets of samples. When optimized, the mapping should appropriately adapt G(x) to the target distribution $p^T$ , while preserving the input's cross-domain features. However, from the first glance it is unclear what the preservation of crossdomain properties should be like.

$^{2}$The subscript $\theta$ in $\psi^{\theta}$ does not mean introduction the additional normal model of density but is rather used to emphasize its normality.

$^{1}$Note that there is one more summand, which contains the gradient $\nabla_q \log p^*$ with respect to the log-density parameters. We do not discuss how to approximate it, because it will be further omitted.

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

One way to look at that formally is by introducing the notion of "transportation cost $c(\cdot ,\cdot )$ between the generator's input and output and saying that it should not be too large on average. In a practical R2I setting, we can choose $c(\cdot ,\cdot )$ as any number above or equal to 100 such that we choose values that we aim to preserve, e.g. pixel-wise distances of differences between LPIPS ( Zhang et al. , 2018 ) embeddings.

Monge optimal transport (OT) problem ( Villani et al. , 2009 ; Santambroio , 2015 ) follows this reasoning and aims at finding the mapping with the least average transport cost among all the mappings that fit the target $p^{T}$ .

$$\begin{array} { r } { \operatorname* { I n f } _ { G } \left\{ \mathbb { E } _ { p ^ { - ( x ) } } c ( x , G ( x ) ) \left| G ( x ) \sim p ^ { T } \right. \right\} . \quad ( 1 2 ) } \end{array}$$

which can be seen as a mathematical formalization of the I2I task.

Under mild constraints, in the case when $p^S$ and $p^T$ have densities, the optimal transport map $G^*$ is bijective, differentiable, has differentiable inverse and thus satisfies the change of variables formula $p^S(x) = p^T(G^*(x)) \det(\nabla G^*(x))$ . Therefore, the change of variables condition gives us insight into why it is notoriously challenging to optimize Equation 12 directly.

## 3. Methodology

Our main goal is to adapt the DMD method for the unpaired I21 between an arbitrary source distribution $p^S$ and target distribution $p^T$ .

### 3.1. Regularized Distribution Matching Distillation

First, we note that the construction of DMD requires only having samples from the input distribution. Given this, we replace the Gaussian input $p^{\text{frame}}$ by $p^S$ , the data distribution $p^S_{\text{data}}$ by $p^I$ and aim at optimizing

$$\begin{array} { r l } & { \mathcal { L } ( \theta ) = \int _ { 0 } ^ { T } \omega _ { t } \,  K  \big ( p _ { t } ^ { \theta } \, \| \, p _ { t } ^ { T } \big ) \,  d  t = } \\ & { = \int _ { 0 } ^ { T } \omega _ { t } \, \mathbb { E } _ { p ^ { \theta } ( s ) \mathcal { N } ( 0 , I ) } \log \frac { p _ { t } ^ { \theta } ( G _ { 0 } ( x ) + \sigma _ { t } \varepsilon ) } { p _ { t } ^ { T } ( G _ { 0 } ( x ) + \sigma _ { t } \varepsilon ) } \,  d  t . \quad ( 1 3 ) } \end{array}$$

where $p_i^u$ and $p_i^T$ are now respectively the distribution of the generator output $G_\theta(x)$ and the target distribution $p^T$ perturbed by the forward process up to the timestep $t$ .

Optimizing the objective in Equation 13 , one obtains a generator, which takes $x \sim p^*$ and outputs $\hat{G}_T(x) \sim p^*$ , so it performs the desired transfer between the two distributions. However, there are no guarantees that the input and the output will be related. Similarly to the OT problem (Equation 12 ), we fix the issue by penalizing the transport

cost between them. We obtain the following objective

$$\begin{array} { r } { \mathcal { L } ( \theta ) + \lambda \, \mathbb { E } _ { p ^ { - c } ( x ) } c ^ { t } \left( x , G _ { \theta } ( x ) \right) \to \underset { \theta } {  m i n  } , \quad ( 1 4 ) } \end{array}$$

where $c(\cdot ,\cdot )$ is the cost function, which describes the objective properties that we aim to preserve after transfer, and $\lambda$ is the regularization parameter of the LMP. The appropriate $\lambda$ , which we will find a balance between between the target distribution and preserving properties of the input.

As in DMD, we assume that the perturbed target distributions are represented by a pre-trained diffusion model $s^*_t$ and approximate the generator distribution score $s^*_t$ by the additional fake diffusion model $s^*_{\theta}$ . Analogous to the DMD procedure (Equation 10 ), we perform the coordinate descent in which, however, the generator objective is now regularized. We call the procedure Regularized Distribution Matching Distillation (RMD) . Formally, we optimize

$$\left\{ \begin{array} { l c } { \displaystyle \int _ { 0 } ^ { T } \omega _ { i } \, \mathcal { E } _ { x , x } \log \frac { p _ { t } ^ { ( i ) } ( G _ { t } ( x ) + \sigma _ { t } ) } { p _ { t } ^ { ( i ) } ( G _ { t } ( x ) ) + \sigma _ { t } } \,  d  t } \\ { \textstyle \quad + \lambda \, \mathcal { E } _ { x , x } c ( x , G _ { t } ( x ) ) \to \operatorname* { m i n } ; } \\ { \displaystyle \int _ { 0 } ^ { T } \beta _ { i } \, \mathcal { E } _ { x , x } \| D _ { t } ^ { ( i ) } ( G _ { t } ( x ) + \sigma _ { t } ) - G _ { t } ( x ) \| ^ { 2 } \,  d  t \to \operatorname* { m i n } . } \end{array} \right.$$

Given the optimal fake score $s_i^0$ , the generator's objective becomes equal to the desired loss in Equation 14 , which validates the procedure.

### 3.2. Analysis of the method

The optimization problem in Equation 14 can be seen as the soft-constrained optimal transport, which balances between satisfying the output distribution constraint and preserving the original image properties. Moreover, if one takes $\lambda \to 0$ , the objective essentially becomes equivalent to the Monge problem (Equation 12 ). It can be seen by replacing the $\lambda$ coefficient before the transport cost with the $1/\lambda$ coefficient before the KL divergence. In this limit, it equals what we have just defined. However, if the two conditions are different, which makes the corresponding problem hardconstrained and, therefore, equivalent to the optimal optimal transport problem. Based on this observation, we prove the following

Theorem 3.1. Let $(\mathbf{x}, \mathbf{y})$ be the quadratic cost $\|\mathbf{x} - \mathbf{y}\|_2^2$ and $G^2$ be the theoretical optimum in the problem [14, 13] , under mild regularity conditions, it converges in probability (with respect to $P$ ) to the optimal transport map $G^*$ , i.e.,

$$G ^ { \lambda } \xrightarrow [ \lambda \to 0 ] { \rho ^ { 2 } } G ^ { * } . \quad ( 1 6 )$$

The detailed proof can be found in Appendix A . Informally, it means that the optimal transport map can be approximated

4

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

![Figure](figures/37030_page_005_figure_001.png)

Figure 2. Comparison of the DMD loss surfaces without (left) and with (right) transport cost regularization on a toy problem of translating $(0,0, f)$ to $(0,0, 1.5^2f)$ . We set the regularization coefficient $\lambda = 0.2$ . The generator is parameterized as: $C'(v)$ , where $C(v)$ is the rotation matrix, corresponding to the angle $\alpha$ . Minima at the left contains all orthogonal matrices, multiplied by $\sigma^{=1.5}$ , while the right and the right is retained in the only point, which is close, but not equal, to the OT map. The surfaces are moved for the sake of visualization.

by the RDMD generator, trained on Equation 15 , given a sufficiently small regularization coefficient, enough capacity of the architecture, and convergence of the optimization algorithm.

This result is important to examine from another angle. It is ideologically similar to the $L_2$ regularization for overparameterized least squares regression. The original least squares, in this case, have a manifold of solutions. At the same time, by adding $L_2$ weight penalty and taking the limit as the regularization coefficient goes to zero, one obtains a solution with the least norm based on the Moore-Penrose pseudo-inverse ( Moore , 1920 ; Penrose , 1955 ) . In our case, numerous maps may be optimal in the original DMD procedure, since it only requires matching the distribution at output. However, taking the limit when $\lambda \to 0$ , one obtains a feasible solution with the least transport cost.

We demonstrate this effect on a toy problem of translating $\mathcal{N}(0,1)$ to $\mathcal{N}(0,\sigma^2I)$ and consider linear generator $G(\boldsymbol{x}) = Ax$ . The solution to the optimal transport problem with the quadratic cost $c(\boldsymbol{x},\boldsymbol{y}) = \|\boldsymbol{x}-\boldsymbol{y}\|^2$ is $\bar{A} = \sigma I$ . For the DMD optimization problem without regularization, minima are obtained at the maxima of orthogonal matrices multipliers $\lambda_1, \lambda_2, \ldots, \lambda_n$ . However, if we only take into consideration, the minimum condition is one point at the cost of introducing the bias relatively to the true OT map. We illustrate this by comparing the loss surface with and without regularization in Figure 2 .

## 4. Related work

In this section, we give an overview of the existing methods for solving unpaired I2I including GANs, diffusion-based methods, and methods based on optimal transport.

GANs were the prevalent paradigm in the unpaired I2I for a long time. Among other methods, CycleGAN (Zhu et al., 2017) and the concurrent DualGAN (Yi et al., 2017) . DiscoGAN (Kim et al., 2017) utilized the cycle-consistency paradigm, consisting in training the transfer network along with its inverse and optimizing the consistency term along with an iterative optimization. On the other hand, there are two-sided methods, including UNIT (Liu et al., 2017) and MUNIT (Huang et al., 2018) that divide the encoding into style-space and content-space and SCAN (Li et al., 2018) that splits the procedure into coarse and fine stages.

The one-side GAN-based methods aim to train I2I without learning the inverse for better computational efficiency. DistanceGAN ( Benaïm & Wolf , 2017 ) achieves it by learning to preserve the distance between pairs of samples, GCGAN ( Fu et al. , 2019 ) imposes geometrical consistency constraints, and CUT ( Park et al. , 2020 ) uses the contrastive loss to maximize the patch-wise mutual information between input and output.

Diffusion-based V1 models mostly build on modifying the diffusion process using the source image. SDeDiff ( Meng et al. , 2021 ) initializes the reverse diffusion process for target distribution with the noisy source picture instead of the 原画, and generates a new image by diffusion ( Jing et al. , & Salimans , 2022 ; Epstein et al. , 2023 ) . The target diffusion process. ILVR ( Cho et al. , 2021 ) adds the correc-

5

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

![Figure](figures/37030_page_006_figure_001.png)

Figure 3. Visualization of RDMD mappings on Gaussian → 8Gaussians with different choices of the $r$

tion that enforces the current noisy sample to resemble the source. EGSDE ( Zhao et al. , 2022 ) trains a classifier between domains and encourages dissimilarity between the embeddings, corresponding to the source image and the current diffusion process state. At the same time, it enforces a small distance between their downsampled versions, which allows for a balance between faithfulness and realism. The source is re-sampled by sampling natural ones from the diffusion models based on the concatenation of two diffusion models with deterministic sampling ( Su et al. , 2022 ; Wu & De La Torre , 2023 ) .

Optimal transport ( Villani et al. , 2009 ; Peyré et al. , 2019 ) is another useful framework for the unpaired I2I. Methods based on it usually reformulate the OT problem (Equation 12 ) and its modifications as Entropic OT (EOT) ( Cuturi , 2013 ) or Schrödinger Bridge (SB) ( Föllmer , 1988 ) to be accessible in practice. In particular, NOT ( Korotin et al. , 2022 ) , ENOT ( Gushchin et al. , 2024 ) , and NSB ( Kim et al. , 2023a ) use the Lagrangian multipliers formulation of the distribution matching constraint, which results in simulation-based adversarial training. The other methods obtain (partially) simulation-free techniques by iteratively refining the stochastic process between two distributions. In the works ( De Bortoli et al. , 2021 ; Vargas et al. , 2021 ) refinement consists of learning the time-reversal with the corresponding initial distribution (source or target). The newer methods are based on Flow Matching ( Lipman et al. , 2022 ; Tong et al. , 2023 ; Albergo & Vanden-Eijnden , 2022 ) and the corresponding Rectification ( Liu et al. , 2022 ; Shi et al. , 2024 ; Liu et al. , 2023 ) procedure. While being theoretically sound, most of these methods work well for smaller dimensions ( Korotin et al. , 2023 ) but suffer from computationally hard training in large-scale scenarios.

## 5. Experiments

This section presents the experimental results on 2 upgraded methods, i.e., S-EM and E-EM with respect to increasing the perimeter. In Section 5.2 we compare our method with

the diffusion-based baselines on the translation problems of NELL and child animals from the AHEAD-2 data set (Choi et al., 2020) .

In all the experiments, we use the forward diffusion process with variance $\sigma_t = t$ and $T = 80.0$ as in the paper ( Karras et al. , 2022 ) . We parameterize all the diffusion models with the denoiser networks $D_\gamma(x)$ , conditioned on the noise level $\sigma$ , and optimize Equation 6 to train the target diffusion model. As for the RDMD procedure, we optimize Equation 15 , where the gradient with respect to the generator parameters is calculated analogously to Equation 11 . The transport cost $c(x,y)$ is chosen as the squared difference norm $\|x - y\|^2$ . The average transport cost, reported in the figures, is calculated as the square root of the MSE between all input and output images for the sake of interpretability.

we use the same architecture for all networks: target score, fake score, and generator. We utilize the pre-trained target score in two ways. First, we initialize the fake model with its copy. Second, we initialize the generator $G_{\theta}(x)$ with the same copy $D_{\theta}^{T}(x)$ , but with a fixed $\sigma \in [0, T]$ (since the generator is one-step). The denoiser parameterization is trained to predict the target domain's clean images, therefore, each initial trajectory has significantly different denoise values. These differences make the target domain the information about the target domain more efficiently (Nguyen & Tan, 2023 ; Yin et al., 2023 ) . We explore the initialization of $\sigma$ for I2I in Appendix B . The additional training details can be found in Appendix C .

### 5.1. Toy Experiment

We validate the qualitative properties of the RDMD method on 2-dimensional Gaussian $\to$ 8Gaussians . In this setting, we explore the effect of varying the regularization coefficient $\lambda$ on the trained transport map $G_{\theta}$ . In particular, we study its impact on the transport cost and fitness to the target distribution $p^T$ .

In the experiment, both sources $\mathcal{N}(0, f)$ and the target mixture of 8 Gaussians are represented with 5000 indepen-

6

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

![Figure](figures/37030_page_007_figure_001.png)

Figure 4. Comparison of RDMD with diffusion-based baselines. The figure demonstrates the tradeoff between generation quality (FID $\downarrow$ ) and the difference between the input and output (L2, PSNR, SSIM $\uparrow$ ). RDMD gives an overall better tradeoff given that it is slightly better than the baseline in both cases. As for the PSNR and SSIM, the $y$ -axis is swapped for the sales of identical readability with the first plot (left is better, low is better).

dent samples. We use the same small MLP-based architecture [ Shi et al. , 2024 ] for all the networks.

The main results are presented in Figure 3 . The standard $\Delta M$ ( $\lambda=0.0$ ) learns a transport map with several intersections when demonstrated as the set of lines between the inputs and the outputs. This observation means that the learned map is not OT, because it is not cycle-monotone ( McCann , 1995 ) . Increasing $\lambda$ yields fewer intersections, which can be used as a proxy evidence of optimality. At the same time, the generator output distribution becomes farther and farther from the desired target. The results show the importance of choosing the appropriate $\lambda$ to obtain a better trade-off between the two properties. Here, the regularization coefficient $\lambda=0.2$ offers a good trade-off by having small intersections and producing output distribution close to the target.

![Figure](figures/37030_page_007_figure_005.png)

Figure 5. Visual comparison of RDMD with diffusion-based baselines.

### 5.2. Cat to Wild

Finally, we compare the proposed RDMD method with the diffusion-based baselines ILVR ( Choi et al. , 2021 ) , SDEdit ( Meng et al. , 2021 ) , and EGSDE ( Zhao et al. , 2022 ) on the $4\ell \times 64$ Cat $\to$ Wild transition problem, based on the AFIHQ-2 ( Choi et al. , 2020 ) data set. Comparison with the diffusion-based models makes the setting fair since it allows to utilize the same pre-trained target diffusion model for training and evaluation. We evaluate our GANbased methods mostly demonstrate results inferior to EGSDE ( Zhao et al. , 2022 ) in terms of FID and PSNR at the same data set with resolution $256\times 256$ .

We pre-train the target diffusion model using the EDM ( Karras et al. , 2022 ) architecture with the hyperparameters used by the authors on the AFHQv2 data set. Our pre-trained model achieves FID equal to 2.0. We initialize the model with $\sigma = 1.0$ based on the observations from Section B and train 5 RDGM generators, corresponding to the regularization coefficients $(0.00, 0.02, 0.05, 0.1, 0.2)$ . We slightly reduce the noise of the DDPM diffusion models by training it with the EDM setting. The ESGDE classifier is trained analogous to the paper, it is initialized from the Dhariwal NLP ( Dhariwal & Nichol , 2021 ) , pre-trained on the Ima-

7

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

geNet ( Deng et al. , 2009 ) $64\times 64$ . For each of the baselines, we run a grid of hyperparameters. The detailed hyperparameter values can be found in Appendix C.3 .

We report our main quantitative results in Figure 5 and compare the achieved faithfulness-quality trade-off with the baselines. The quality metric is FID, the faithiness metrics are L2/PSNR/SSIM. Among these metrics, L2 is the least convenient for our method. Nevertheless, RDMD achieves a better trade-off given at least moderately strict requirements on the transport cost: all of our models beat all the baselines in the L2 range between $12.5$ and $20.0$ . In all cases, our model achieves strictly higher SSIM and almost strictly higher PSNR. We note, however, that if the lower FID is preferable over the transport cost (L2 values around $22.5-27.5$ ), then it might be better to use one of the baselines. An example of a map with a high OT cost ( $25.0$ ) and low FID ( $5.4$ ) is SDEdit on Figure 5 .

Finally, we present a visual comparison between the meth- ods. To this end, we randomly choose 6 pictures from the test data set and report the corresponding outputs in Figure 5 . Here, we take RDMD with $\lambda=0.05$ that achieves (FID, L2) equal to $(6.93, 17.86)$ . As for the baselines, we choose the hyperparameters (Appendix C.3 ) with the closest FID to the RDMD: $(8.87, 22.0)$ for ILVR, $(5.4, 25.0)$ for SDEdit, and $(7.02, 22.35)$ for EGSDE .

## 6. Discussion and limitations

In this paper, we propose RDMD, the novel one-step diffusion-based algorithm for the unpaired I2I task. This algorithm is a modification of the DMD method for diffusion distillation. The main novelty is the introduction of the diffusion distance between the input and the output of the model, which allows to control the trade-off between faithfulness and visual quality.

From the theoretical standpoint, we prove that at low regularization coefficients, the theoretical optimum of the introduced objective is close to the optimal transport map ( Thm. 3.1 ). Our experiments in Sec. 5.1 demonstrate how the choice of regularization coefficient affects the trained mapping and allows us to build the general intuition. In Sec. 5.2 we compare our method with the diffusion-based baselines (ILVR, SDEdit, EGSDE) and obtain better results given fair restrictions on the transport cost. The results are strictly better than all of the baselines in terms of SSIM and almost strictly superior to all of the baselines in terms of PSNR.

In terms of limitations, we admit that our theory works in the asymptotic regime, while one could derive more precise non-limit bounds. Our experimental results on diffusion under the Langevin equation show that the pre-trained diffusion model has 2.01. Improving the vi-

sual quality and testing our method on high dimensions is important for future work. Furthermore, the desired feature of the method would be switching among different reg coefficients without re-training.

## Acknowledgements

The work of Denis Rakitin was supported by the grant for research centers in the field of AI provided by the Analytical Center for the Government of the Russian Federation (ACRF) in accordance with the agreement on the provision of subsidies (identifier of the agreement: grant number: 18-02-0037-0031-0021) and research with HSE, University No. 70-2021-001-39. This agreement was supported in part through computational resources of HSE cities at HSE University ( Kostenetsky et al. , 2021 ) .

## References

Albergo, M. S. and Vanden-Eijnden, E. Building normalized and regularized hierarchical interpolants. arXiv preprint arXiv:2209.15371 , 2022.

Arjovsky, M., Chintala, S., and Bottou, L. . Wasserstein generation of normally distributed mixture models. In Proceeding of machine learning , pp. 214–223. PMLR, 2018.

Benaim, S. and Wolf, L. One-sided unsupervised domain adaptation for applications in neural information processing systems , 30, 2017.

Bogachev, V. I. and Ruas, M. A. S. Measure theory , volume 1, Springer, 2007.

Brenier, Y. Polar factorization and monotone rearrangement of a multivalued function. J. Theor. Comp. Math. 1991, 164: 270-285. doi:10.1016/0022-3714(91)90147-1

Choi, J., Kim, J., Seong, Y., Gwon, Y., and Yoon, S.: IOT devices: An overview, IEEE Internet of Things Journal, vol. 13, pp. models, arXiv preprint arXiv:2108.02498, 2021.

Choi, Y., Uh, Y., Yoo, J., and Ha, J.-W. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8188–8197, 2020.

Csizsár, I. On information-type measurement of difference of temperature between two refrigerant mixtures. Fundam. Phys. Sci. Math. Hung., 2:299-318, 1967.

Cuturi, M.: Sinkhorn distances: Lightspeed comparison and its applications in neural information processing systems , 26, 2013.

De Bortoli, V., Thornton, J. Heng, J., and Doucet A.: Diffusion schrödinger bridge with applications to score-based

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

generative modeling. Advances in Neural Information Processing Systems , 34:17695–17709, 2021.

De Philippis, G. and Figalli, A. The monge-ampere equation and its link to optimal transportation. Bulletin of the American Mathematical Society , 51(4):527–580, 2014.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248–255. Ieee, 2009.

Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems , 34:8780–8794, 2021.

Donsker, M. D. and Varadhan, S. S. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on pure and applied mathematics , 36 (2):183–212, 1983.

Dudley, R. M. Real analysis and probability. Chapman and Hall/CRC, 2018.

Epstein, D., Jabri, A., Poole, B., Efros, A., and Holynski, A. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems , 36: 16222–16239, 2023.

Föllmer, H. Random fields and diffusion processes. Lect. Notes Math. 1362:101–204, 1988.

Fu, H., Gong, M., Wang, C., Batmanghelich, K., Zhang, K., and Tao, D. Geometry-consistent generative adversarial neural network for object detection based on data mapping. In In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 2427–2436, 2019.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems , 27, 2014.

Gushchin, N., Kolesov, A., Korotin, A., Vetrov, D. P., and Burnaev, E. Entropic neural optimal transport via diffusion processes. Advances in Neural Information Processing Systems , 36, 2024.

Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.

Ho, J., Jain, A., and Abbeel, P.: Denoising diffusion probabilities for natural images, and information processing systems, 33, 6840–6851, 2002.

Huang, X., Liu, M.-Y., Belongie, S., and Kautz, J. Multimodal unsupervised image-to-image translation. In Proceedings of the European conference on computer vision (ECCV) , pp. 172–189, 2018.

Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-toimage translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 1125–1134, 2017.

Kantorovich, L. On the translocation of masses. Management science, 5(1):1-4, 1958.

Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems , 35: 26565–26577, 2022.

Kim, B., Kwon, G., Kim, C., Ye, J.: Unpaired imagedrift models for large-scale image segmentation. arXiv preprint arXiv:2305.15086 , 2023a.

Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takeda, Y., Uesaka, T. He., Y. Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279 , 2023b.

Kim, T., Cha, M., Kim, H., Lee, J. K., and Kim, J. Learning to discover cross-domain relations with generative adversarial networks. In International conference on machine learning , pp. 1857–1865. PMLR, 2017.

Kingma, D. P. and Ba, J.: Adam: A method for stochastic optimization, CoRR , abs/1412.6980, 2014. URL: https://api.semanticscholar. org/CorpusID:6628106 .

Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013.

Korotin, A., Selikhovanych, D., and Burnaev, E.: Optical optimal transport, arXiv preprint arXiv:2201.12220, 2022.

Korotin, A., Gushchin, N., and Burnaue, E.: Light schiddering bridge, arXiv preprint arXiv:2310.01774 , 2023.

Kostenetskiy, P., Kuchylevich, R., and Kozyrev, V. Hpc resources of the higher school of economics. In Journal of Physics: Conference Series , volume 1740, pp. 012050. IOP Publishing, 2021.

LeCun, Y. The mnist database of handwritten digits. http://yann. lecun. com/edrvlmnist/, 1998.

Li, M., Huang, H., Ma, L., Liu, W., Zhang, T., and Jiang, Y. Unsupervised image-to-image translation with stacked cycle-consistent adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision (ICCV) , pp. 184–199, 2018.

Liero, M., Mielke, A., and Savar é , G. Optimal entropytransport problems and a new hellinger-kantorovich distance between positive measures. Inventiones mathematicae , 211(3):969–1117, 2018.

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022.

Liu, M.-Y., Breuel, T., and Kautz, J. Unsupervised visual characterization of transient attacks in neural information processing systems, 30, 2017.

Liu, X., Gong, C., and Lin, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03005 , 2022.

Liu, X., Zhang, X., Ma, J., Peng, J., et al. Infowall: One step is enough for high-quality diffusion-based text-toimage generation. In The Twelfth International Conference on Learning Representations , 2023.

McCann, R. J. Existence and uniqueness of monotone measure-preserving maps. 1995.

Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 , 2021.

Metz, L., Poole, B., Plau, D., and Sohl-Dickstein, J. (2015). A convolutional neural network. arXiv preprint arXiv:1511.02163 . 2015.

Moore, E. H. On the reciprocal of the general algebraic coe. Transactions of the american mathematical society, 26:294-295, 1920.

Nguyen, T.H. and Tran, A., Swishbsh: One-step test-toquantum transition to a low-cost score distiller, ArXiv preprint arXiv:2312.05239 , 2023.

Park, T., Efros, A. A., Zhang, R., and Zhu, J. Y.: Contrastive learning for unpaired image-to-image translation, In: Computer Vision–ECCV 2020: 16th European Conference on Machine Learning (ECML’20), Proceedings, Part IX 16, pp. 319–345, Springer, 2020.

Penrose, R. A generalized inverse for matrices. In Mathematical proceedings of the Cambridge philosophical society, volume 51. pp. 406–413. Cambridge University Press, 1955.

Peyré, G., Cuturi, M., et al. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning , 11(5-6):355–607, 2019.

Poole, B., Jain, A., Barron, T. J., and Mildenhall, B.: Dracunnus australis from birth to early death 3d diffusion, ArXiv preprint arXiv:2209.14988, 2022.

Posner, E. Random coding strategies for minimum entropy information. Transaction on Information Theory, 21(4):387– 391, 1972.

Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning , pp. 1278–1286. PMLR, 2014.

Santambongio, F. Optimal transport for applied mathematicians. Birkäuser, NY , 55(58-63):94, 2015.

Sauer, A., Lorenz, D., Blattmann, A., and Romisch, T. (1997). Ethanol distillation. arXiv preprint arXiv:2311.17042 , 2023.

Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. Diffusion-tolerant Population Dynamics Model for Dynamics in Neural Information Processing Systems, 36, 2024.

Song, Y. and Ermon, S. Generation modeling by estimating latent structures from structural data. In Advances in neural information processing systems , 32, 2019.

Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 , 2020.

Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2305.01469 , 2023.

Su, X., Song, J., Meng, C., and Ermon, S.: Dual diffusion time series detection via language translation, arXiv preprint arXiv:2205.08182 , 2022.

Tenenbaum, J. B., Silva, V. D., and Langford, J. C. A. Altered resting-state magnetic activity during visual reduction. science, 290(5550):2319-2323, 2000.

Tong, A., Malkin, N., Huguet, G., Zhang, Y., Restor-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.04482 , 2023.

Vargas, F., Theodoroff, P., Lamacra, A., and Lawrence, J. (1997). Bayesian knowledge via maximum likelihood. Entropy , 23(9):1134–1152.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. V., and Polosukhin, I. Attention is all you need. In Guyon, I., Luburgu, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf .

Villani, C. et al. Optimal transport: old and new , volume 338. Springer, 2009.

10

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Vincent, P. A connection between score matching and denoising autoencoders. Neural Computation , 23:1661–1674, 2011. URL https://api. semanticscholar.org/CorpusID:5560641 .

Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems , 36, 2024.

Wu, C. H., and De la Torre, F. A latent space of stochastic diffusion models for zero-shot image editing and guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 7378–7387, 2023.

Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative optimization problem with improved diffusion gans. arXiv preprint arXiv:2112.07804 , 2021.

Yi, Z., Zhang, H., Tan, P., and Gong, M.: Duallan: Unsupervised dual learning for image-to-image translation, In Proceedings of the IEEE international conference on computer vision , pp. 2849–2857, 2017.

Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828 , 2023.

Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, N. (2018). Deep residual learning for motion of deep features as a perceptual metric. In CVPR , 2018.

Zhao, M., Bao, F., Li, C., and Zhu, J., Eds.: Unpaired image-to-image translation via energy-guided stochastic differential equations. Advances in Neural Information Processing Systems , 35:3609–3623, 2022.

Zheng, W., Li, Q., Zhang, G., Wan, P., and Wang, Z. 1tr: Unpaired image-to-image translation with transformers. arXiv preprint arXiv:2203.16015 , 2022.

Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision , pp. 2223–2232, 2017.

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

## A. Theory

In this section, we aim at proving the main theoretical result of the work: solution of the soft-constrained RDMD objective converts to the solution of the hard-constrained Monge problem. Our proof is largely based on the work ( Lierò et al. , 2018 ) . It introduces the family of entropy-transport problems, consisting in optimizing the transport cost with soft constraints based on the divergence between the map's output distribution and the target. There are, however, differences between the problems that prevent us from reducing the functional in Equation 14 for the entropy-transport problems. First, authors of the paper in [ 10 ] showed that by introducing an empirical variational inequality, they could represent the Csiszár $f$ -divergences ( Csiszár , 1967 ) , used in ( Lierò et al. , 2018 ) , seemingly does not contain the integral ensemble of KL divergences, used in Equation 14 . Finally, we illustrate the proof in a simpler particular setting for the narrative purposes. Nevertheless, the used ideas are very similar.

### A.1. Proof outline

We start by giving a simple outline of the proof. Given a pair of source and target distributions $p^S$ and $p^T$ , RDMD optimizes the following functional with respect to the generator $G$:

$$\int _ { 0 } ^ { T } \omega _ { t } \,  K L  \left( p _ { t } ^ { i } \, \| \, p _ { t } ^ { f } \right) \,  d  t + \lambda \, \mathbb { E } _ { p ^ { x ( t ) } } c \left(  x  , G (  x  ) \right) , \quad ( 1 7 )$$

where $p_t^G$ and $p_t^T$ are the generator distribution $p^G$ and the target distribution $p^T$ , perturbed by the forward diffusion process up to the time step $t$ . Our goal is to prove that the optimal generator of the regularized objective converges to the optimal transport map when $\lambda \to 0$ . With a slight abuse of notation, in this section we will use a different objective

$$\begin{array} { r } { \mathcal { L } ^ { \alpha } ( G ) = \alpha \int _ { 0 } ^ { T } \omega _ { t } \mathbb { K L } \left( p _ { t } ^ { G } \, \| \, p _ { t } ^ { T } \right) \,  d  t + \mathbb { E } _ { p ^ { G } ( x ) } c \left( x , G ( x ) \right) } \end{array} \quad ( 1 8 )$$

and consider the equivalent limit $\alpha \to +\infty$. We

$$\begin{array} { r } { \mathcal { L } ^ { \infty } ( G ) = \left\{ \begin{array} { l l } { \mathbb { E } _ { p ^ { \delta } ( x ) } c \left( x , G ( x ) \right) ,  ~ i f ~  ~ p ^ { G } = p ^ { T } ; } \\ { + \infty ,  ~ e l s e ~  } \end{array} \right. } \end{array} \quad ( 1 9 )$$

to be the objective, corresponding to the unconditional formulation of the Monge problem (Equation 12 ). In this section, we will denote minimum of this objective (which is, therefore, the optimal transport map) as $G^{\infty, a}$ .

We first assume that the infimum of the objective $\mathcal{L}^0$ is reached and define $G^0$ be the optimal generator. We denote by $\{\alpha_n\}_{n=1}^{+\infty}$ an arbitrary sequence with $\alpha_n \to +\infty$ . We first make two informal assumptions that need to be proved (and will be in some sence further in the section):

- 1. The sequence $G^{n}$ converges (in some sence) to some function $\hat{G}$;
2. $\mathcal{L}^{n}$ is continuous with respect to this convergence, i.e. for every convergent sequence $G_{n} \to G$ holds $\mathcal{L}^{n}(G_{n}) \to \mathcal{L}^{n}(\hat{G})$.
Given this, we first observe that for each map ( ) the sequence of objectives $\mathcal{L}^{(m)}$ ( ) monotonically converges to the objective $\mathcal{L}^*(\zeta)$ . It follows from the fact that the first summand of $\mathcal{L}^{(m)}$ converges to $+\infty$ if and only if the KL divergence is non-zero, which is equivalent to saying that $p^{(1)}$ and $p^*$ differ ( Wang et al. , 2024 ) . If instead $p^{(1)} = p^*$ , the summand zeroes out. This also means that the minimal values of the corresponding objectives form a monotonic sequence:

$$\mathcal{L}^{\alpha_{\kappa}}(G^{\alpha_{0}})\leq \mathcal{L}^{\alpha_{\kappa+1}}(G^{\alpha_{0}+1})\leq \mathcal{L}^{\infty}(G^{\infty}).\quad (20)$$

Finally, the monotonicity implies that for a fixed m

$$\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } \mathcal { L } ^ { \alpha _ { n } } ( G ^ { \alpha _ { n } } ) \geq \operatorname* { l i m } _ { n \to \infty } \mathcal { L } ^ { \alpha _ { n } } ( G ^ { \alpha _ { n } } ) , \quad ( 2 1 ) } \end{array}$$

$^{4}$ Solution to the Monge problem is not always unique, but we will further impose assumptions that will guarantee the uniqueness.

12

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

since the input $G^{\alpha_n}$ is fixed and $\mathcal{L}^{\alpha_n}$ monotonically increases. Using the assumed continuity of the objective, we obtain

$$\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } \mathcal { L } ^ { \alpha _ { n } } ( G ^ { \alpha _ { n } } ) \geq \mathcal { L } ^ { \alpha _ { n } } ( \overline { G } ) \quad ( 2 2 ) } \end{array}$$

for each $m$. Taking the limit $m \to \infty$, we obtain

$$\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } G ^ { \Omega _ { n } } ( G ^ { \Omega _ { n } } ) \geq L ^ { \infty } ( \tilde { G } ) . \quad ( 2 3 ) } \end{array}$$

Combining this set of equations, we obtain:

$$\mathcal { L } ^ { \infty } ( G ^ { n } ) \geq \operatorname* { l i m } _ { n \to \infty } \mathcal { L } ^ { \infty } ( G ^ { n } ) \geq \mathcal { L } ^ { \infty } ( \hat { G } ) \geq \mathcal { L } ^ { \infty } ( G ^ { \infty } ) , \quad   ( 2 4 )$$

where the first inequality comes from the monotonicity of the minimal values and the last inequality uses that $\mathcal{G}^\infty$ is the minimum of the objective $\mathcal{L}^\infty$ . Hence, that limiting map $\tilde{G}$ achieves minimal value of the objective $\mathcal{L}^\infty$ and is, therefore, the optimal transport map.

At this point, we only need to define and prove some versions of the aforementioned facts:

- 1. Infinimum of $L^0$ is reached;
2. The sequence of minima $G^{s_0}$ converges;
3. $L^0$ is continuous with respect to this converg
From now on, we formulate the result in details and stick to the formal proof.

### A.2. Assumptions and theorem statement

First, we list the assumptions.

Assumption A1. The distributions $p^S$ and $p^T$ have densities with respect to the Lebesgue measure. The distributions are defined on open bounded subsets $\mathcal{X} \subset \mathbb{R}^d$ and $\mathcal{Y} \subset \mathbb{R}^d$ , where $\mathcal{Y}$ is convex. The densities are bounded away from zero and infinity on $\mathcal{X}$ and $\mathcal{Y}$ , respectively.

We admit that boundedness of the support is a very restrictive assumption from the theoretical standpoint, however in our applications (21) both source and target distributions are supported on the bounded space of images. We thus can set $\mathcal{X} = \mathcal{Y} = (0,1)^d$ .

Assumption A.2. The cost $c(\mathbf {x}, \mathbf {y})$ is quadratic $\|\mathbf {x} - \mathbf {y}\|^2$ .

Here, we stick to proving the theory only for $L_2$ cost due to difficulties in investigation of Monge map existence and regularity for general transport costs ( De Philippis & Figalli , 2014 ) .

Assumption A.3. The weighting function $\omega_t$ is positive and bounded.

Assumption A.4. Standard deviation $\sigma_t$ of the noise, defined by the forward process, is continuous in $t$ .

Theorem A10. Let $\eta^S, \gamma^S$ , $c, \omega_1$ , and $\sigma_1$ satisfy the assumptions 1 - 3 . Then, there exists a minimum $G^\alpha$ of the objective $\mathcal{L}^\alpha$ from the Equation 18 . If $\alpha_n \to \infty$ , the sequence $G^{n\alpha}$ converges in probability (with respect to the source distribution) to the optimal transport map $G^\infty$ .

$$G ^ { \alpha } \to \frac { g ^ { \beta } } { n \to \infty } G ^ { \beta } . \quad ( 2 5 )$$

### A.3. Theoretical background

We start by listing all the results necessary for the proof. They are mostly related to the topics of measure theory (weak measures, Borel measures, and so on). However, some of the important facts can be found in the books (Bogacz & Riesz, 2007; Dudley, 2018) . Otherwise, we make the corresponding citations.

Definition A.2. A sequence of probability distributions $p^n(x)$ converges weakly to the distribution $p(x)$ if for all continuous bounded test functions $\varphi \in C_b(\mathbb{R}^d)$ holds

$$ E  _ { p ( x ) } \varphi ( x ) \xrightarrow { n \to \infty }  E  _ { p ( x ) } \varphi ( x ) . \quad   ( 2 6 )$$

Notation: $p^n \overset{w}{\to} p$ .

13

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Definition A.3. A function $f: \mathbb{R}^d \to \mathbb{R}$ is called lower semi-continuous (lsc), if for all $x_n \to x$ holds

$$\begin{array} { r } { \operatorname* { l i m i n } _ { n \to \infty } f ( x _ { n } ) \geq f ( x ) . \tag { 2 7 } } \end{array}$$

Theorem A.4 (Portmanteau/Alexandrov). $p^{n} \stackrel{w}{\to} p$ is equivalent to the following statement: for every lsc function f, bounded from below; holds

$$\begin{array} { r } { \operatorname* { l i m i n } _ { n \to \infty } \mathbb { E } _ { p ^ { n } ( x ) } f ( x ) \geq \mathbb { E } _ { p ( x ) } f ( x ) . \quad ( 2 8 ) } \end{array}$$

Definition A.5. A sequence of probability measures $p^n$ is called relatively compact, if for every subsequence $p^{n_k}$ there exists a weakly convergent subsequence $p^{n_{k+1}}$ .

Definition A.6. A sequence of probability measures $\rho^n$ is called tight, if for every $\varepsilon > 0$ there exists a compact set $K_c$ such that $\rho^n(K_c) \geq 1 - \varepsilon$ for all $n$.

Theorem A.7 (Brokhunov). A sequence of probability measures $\mu ^{n}$ is relatively compact if and only if it is tight. In particular, every weakly convergent sequence is tight.

Corollary A.8. If there exists a function $\varphi(x)$ such that its sublevels $\{x: \varphi(x) \leq  r\}$ are compact and for all $n$

$$ E  _ { p ^ { n } ( x ) } \varphi ( x ) \leq  C$$

holds with some constant $C$, then $p^{n}$ is tight (i.e. at least it has a weakly convergent subsequence).

Corollary A.9. If a sequence $p^{n}$ is tight and all of its weakly convergent subsequences converge to the same measure $p$, then $p^{n}\to p$.

Definition A.10. The functional $\mathcal{L}(p)$ is called lower semi-continuous (lsc) with respect to the weak convergence if for all weakly convergent sequences $p^{n} \stackrel{\text{a.p.}}{\to} p$ holds

$$\begin{array} { r } { \operatorname* { l i m i n } _ { n \to \infty } \mathcal { L } ( p ^ { n } ) \geq \mathcal { L } ( p ) . \tag { 2 9 } } \end{array}$$

Theorem A.1 (Posner 1975). The KL divergence $KL(p\parallel q)$ is lsc (in sense of weak convergence) with respect to each argument, i.e. if $p\stackrel{1}{\to}q$ and $q_{n}\stackrel{1}{\to}q$, then

$$\operatorname* { l i m i n } _ { n \to \infty }  K L  ( p ^ { n } \| q ) \geq  K L  ( p \| q ) \quad ( 3 0 )$$

$$\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty }  K L  ( p | q _ { n } ) \geq  K L  ( p | q ) . \tag { 3 1 } } \end{array}$$

Theorem A.12 (Donsker & Varadhan 1983) . The KL divergence can be expressed as

$$ K L ( p | q ) = \operatorname* { s u p } _ { g } \left( E _ { p ( a ) } g ( x ) - \log _ { E _ { q } ( x ) } e ^ { v ( g ) x } \right) . \quad   ( 3 2 )$$

Definition A.13. The expression

$$\mathbb { E } _ { p ( x ) \in ^ { t } ( s , x ) } \quad ( 3 3 )$$

is called the characteristic function (Fourier transform) of the distribution $p(x)$.

Theorem A.14 (Lývy) . Weak convergence of probability measures $p^{w}$ . w.p. p is equivalent to the point-wise convergence of characteristic functions, i.e. $\overline{\mathbb{E}}_{p^{w}(x)^{t\in \{0,n\}}}\to \overline{\mathbb{E}}_{p^{w}(x)}$ for all $s$.

Definition A.15. A sequence of measurable functions $\varepsilon^n(x)$ is said to converge in measure (in probability) to the function $\varphi$ with respect to the measure $p(x)$, if for all $\varepsilon > 0$ holds

$$p\left\{x:\left|\varphi^{n}(x)-\varphi(x)\right|>\varepsilon\right\}\to0.$$

Theorem A.16 (Lebesgue). Let $\varphi^*$, $\varphi$ be measurable functions such that $\|\varphi^*(x)\|, \|\varphi(x)\| \leq  C$ and $\varphi^*(x) \to \varphi(x)$ as pointwise. Then $\overline{\int_{\mathbb{R}^d}} \varphi^*(x) \varphi(x) \to \mathbb{E}_{p(x)} \varphi(x)$ .

Lemma A.17 (Fatou) . For any sequence of measurable functions $\varphi^n$ the function $\liminf_n \varphi^n$ is measurable and

$$(34)\quad \int\limits_{0}^{b}\liminf _{n\to \infty}\varphi ^{n}(x)dx\leq  \liminf _{n\to \infty}\int\limits_{0}^{b}\varphi ^{n}(x)dx.$$

14

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Theorem A.18 (Brenier 1991) . Given the Assumption A.1 , there exists a unique optimal transport map that solves the Monge problem 12 for the quadratic cost.

Proof. This result can be found e.g. in ( De Philippis & Figalli, 2014, Theorem 3.1 ) .

Theorem A.19. Given the Assumption A.1 , the unique OT Monge map is continuous.

Proof. This is a simplified version of (De Philippis & Figalli, 2014, Theorem 3.3).

### A.4. Lower semi-continuity of the loss

Having defined all the needed terms and results, we start the proof by re-defining the objective in Equation 18 with respect to the joint distribution $\pi$ input and output of the generator instead of the generator $G$ itself. Analogous to the Kantorovich formulation of the optimal transport problem ( Kantorovich , 1958 ) , for each measure $\pi$ on $\mathbb{R}^d \times \mathbb{R}^d$ (which is also called a transport plan or just plan) we define the corresponding functional as

$$\begin{array} { r } { \mathcal { L } ^ { \alpha } (  x  ) = \alpha \int _ { 0 } ^ { T } \omega _ { t } ~  K L  \left(  \pi  _ { t } |  y  , t \, \| \, p _ { t } ^ { T } \right) \, d t + \mathbb { E } _ {  \pi  (  x  ,  y  ) } r \left(  x  ,  y  \right) , } \end{array} \quad ( 3 5 )$$

where $\pi_x$ and $\pi_y$ are the corresponding projections (marginal distributions) of $\pi$ and $\pi_{xy}$ is the perturbed $y$ -marginal distribution of $\pi$. Note that for $\pi$, corresponding to the joint distribution of $(x, G(\boldsymbol{x}))$, $\mathcal{L}^{\pi}(\pi)$ coincides with $\mathcal{L}^{y}(\mathcal{G})$, defined in Equation 18. Thus, we aim to optimize $\mathcal{L}^{y}(\pi)$ with respect to such plans $\pi$, that their $\boldsymbol{x}$ marginal is equal to $p^y$ and $\pi^y(\boldsymbol{y}=G(\boldsymbol{x}))=1$ for some $G$.

Definition A.20. We will call a measure $\pi$ generator-based if its $x$ -marginal is equal to $p^x$ and $\pi(y = G(x))$ for some function $G$ .

For the sake of clarity, we now that the distributions $x^\eta$ and $p'_\eta$ can be represented as $x^\eta * q_i$ and $p^\eta * q_i$, where $*$ is the convolution operation and $q_i = \mathcal{N}(0, \sigma_i^2 I)$ . We thus rewrite the functional as

$$\mathcal { L } ^ { n } ( \pi ) = \alpha \int _ { 0 } ^ { T } \omega _ { \zeta } \,  K L  \left( \pi _ { y } * q _ { t } \parallel p ^ { T } * q _ { t } \right) \,  d  t + \mathbb { E } _ { x ( : y ) } e \left( x , y \right) , \quad   ( 3 6 )$$

Previously, we wanted to establish continuity of the objective. This may not be the case in general. Instead, we prove the following

Lemma A.21. $\mathcal{L}^{\omega}(\pi)$ is lsc with respect to the weak convergence, i.e. for all weakly convergent sequences $\pi^n\xrightarrow{w} \pi$ holds

$$\operatorname* { l i m } _ { n \to \infty } \mathcal { L } ^ { \alpha } ( \pi ^ { n } ) \geq \mathcal { L } ^ { \alpha } ( \pi ) . \quad   ( 3 7 )$$

This result is a direct consequence of the Theorem A.11 about lower semi-continuity of the KL divergence.

Proof. We start by proving that the projection and the convolution operation preserve weak convergence. For the first, we need to prove that for any test function $g \in \mathcal{C}_0(\mathbb{R}^d)$ holds

$$\mathbb { E } _ { x _ { y } ( y ) } g ( y ) \to \mathbb { E } _ { x _ { y } ( y ) } g ( y ) \quad ( 3 8 )$$

given $\pi^0 \xrightarrow{w} \pi$. For this, we note that the function $\varphi(\boldsymbol{x}, \boldsymbol{y}) = g(\boldsymbol{y})$ is also bounded and continuous and, thus

$$\begin{array} { r } { \mathbb { E } _ { \pi _ { y } ( y ) } g ( y ) = \mathbb { E } _ { \pi ^ { n } ( x , y ) } \varphi ( x , y ) \to \mathbb { E } _ { \pi ( x , y ) } \varphi ( x , y ) = \mathbb { E } _ { \pi _ { y } ( y ) } g ( y ) . \quad ( 3 9 ) } \end{array}$$

Regarding the convolution, recall that $\sigma_{ij}^2 \cdot  e_j$ is the distribution of the sum of independent variables with corresponding distributions. Its characteristic function is equal to

$$\begin{array} { r } { \mathbb { E } _ { \pi _ { y } ^ { t } \sim q _ { t } ( y _ { t } ) } e ^ { t ( s _ { t } ) } ( y _ { t } ) = \mathbb { E } _ { \pi _ { y } ^ { t } ( y ) } q _ { t } ( x _ { t } ) e ^ { t ( s _ { t } ) } ( y _ { t + \varepsilon } ) = \mathbb { E } _ { \pi _ { y } ^ { t } ( y ) } e ^ { t ( s ) } ( y ) \, q _ { t ( x _ { t } ) } e ^ { t ( s _ { t } \varepsilon ) } . } \end{array} \quad ( 4 0 )$$

15

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Applying the Lévy's continuity theorem to $\pi_{y}^{n} \stackrel{w}{\to} \pi_{y}$, we take the limit and obtain

$$\mathbb { E } _ { \pi _ { y } ( y ) } e ^ { \ell ( s , y ) } \mathbb { E } _ { q _ { t } ( z _ { t } ) } e ^ { \ell ( s , z _ { t } ) } = \mathbb { E } _ { \pi _ { y } ( y ) } q _ { t } ( z _ { t } ) e ^ { \ell ( s , y + z _ { t } ) } = \mathbb { E } _ { \pi _ { y } \star q _ { t } ( y _ { t } ) } e ^ { \ell ( s , y _ { t } ) } , \quad ( 4 1 )$$

which implies

$$E_{\pi_{Y}^{*}, q_{1}(y_{i})} e^{i\left\langle  s, y_{i}\right\rangle} \to E_{\pi_{Y}^{*}, q_{1}(y_{i})} e^{i\left\langle  s, y_{i}\right\rangle} .\quad (42)$$

We apply the continuity theorem for the convolutions and obtain $\pi_{q}^{n} * q_{t} \xrightarrow{w} \pi_{y} * q_{t}$ .

With this observation, we prove that the first term of $\mathcal{L}^{\pi}(\pi)$ is lsc. First, we apply Lemma A.17 (Fatou) and move the limit inside the integral

$$\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } \int _ { 0 } ^ { T } \omega _ { t }  K L  \left( \pi _ { y } ^ { n } * q _ { t } \mid p ^ { T } * q _ { t } \right) d t \geq \int _ { 0 } ^ { T } \operatorname* { l i m } _ { n \to \infty }  K L  \left( \pi _ { y } ^ { n } * q _ { t } \mid p ^ { T } * q _ { t } \right) d t . } \end{array} \quad ( 4 3 )$$

Using the lower semi-continuity of the KL divergence (Theorem A.11), we obtain

$$\int _ { 0 } ^ { T } \operatorname* { l i m } _ { n \to \infty }  K L  \left( \pi _ { y } ^ { n } * q _ { t } \, \| \, p ^ { T } * q _ { t } \right) \mathrm d t \geq \int _ { 0 } ^ { T }  K L  \left( \pi _ { y } * q _ { t } \, \| \, p ^ { T } * q _ { t } \right) \mathrm d t . \quad   ( 4 4 )$$

Finally, the Assumption A.2 on the continuity of $c(\cdot ,\cdot )$ implies its lower semi-continuity. Theorem A.4 (Portmanteau) states that

$$\begin{array} { r } { \operatorname* { l i m i n } _ { n \to \infty } \mathbb { E } _ { \pi ^ { n } ( x , y ) } c ( x , y ) \geq \mathbb { E } _ { \pi ( x , y ) } c ( x , y ) . \quad ( 4 5 ) } \end{array}$$

Combining inequalities from Equation 43 , Equation 44 and Equation 45 , we obtain

$$\begin{array} { r } { \operatorname* { l i m } _ { n \to \infty } \mathcal { L } ^ { \alpha } ( \pi ^ { n } ) \geq \mathcal { L } ^ { \alpha } ( \pi ) . } \end{array} \quad ( 4 6 )$$

### A.5. Existence of the minimizer

Now we aim to prove that the objective $\mathcal{L}^o(\pi)$ has a minimum over generator-based plans. First, we need the following technical lemma about sublevels of the KL part of the functional.

Lemma A.22. Let $\{\pi^n\}_{n=1}^\infty$ be a sequence of generator-based plans that satisfy

$$\int _ { 0 } ^ { T } \omega _ { t }  K L  \left( \pi _ {  y  } ^ { n } \| p _ { t } ^ { T } \right) d t \leq  C \quad  ( 4 7 )$$

for some constant $C$ . Then, the sequence $\{\frac{1}{n}\}^{\infty}_{n=1}$ is tight.

Proof. We take arbitrary $\pi$ from the sequence and apply the Donsker-Varadhan representation (Theorem divergence). We take the test function $g(\pi) = \|g(\pi)\|_2^2/(2\pi)^2$ and obtain

$$\begin{array} { r } { \int _ { 0 } ^ { T } \omega _ { k }  K L  \left( \pi _ { y , t } \| p _ { t } ^ { T } \right)  d  t = \int _ { 0 } ^ { T } \omega _ { k } \left( \mathbb { E } _ { \pi _ { y , t } ( y _ { t } ) } \frac { 1 } { 2 \sigma _ { y } ^ { 2 } } \| y _ { t } \| ^ { 2 } - \log \mathbb { E } _ { p _ { t } ^ { T } ( y _ { t } ) } e ^ { \| y _ { t } \| / ( 2 \sigma _ { y } ^ { 2 } ) } \right)  d  t . } \end{array} \quad ( 4 8 )$$

The choice of $g(a)$ is not very specific, i.e. every function that will produce finite expectations and integrals is suitable. In the right-hand side, we rewrite the expectations with respect to the original variable and noise:

$$\begin{array} { r } { \int _ { 0 } ^ { T } \int _ { 0 } ^ { T } \mathbb { E } _ { x _ { y } ( \mathbf { y } ) \mathcal { N } ( 0 , I ) } \frac { 1 } { 2 \sigma _ { T } ^ { 2 } } \| \mathbf { y } + \sigma _ { t } \varepsilon \| ^ { 2 } - \log \mathbb { E } _ { \rho ^ { \tau } ( \mathbf { y } ) \mathcal { N } ( 0 , I ) } e ^ { \| \mathbf { y } + \sigma _ { t } \varepsilon \| ^ { 2 } / ( 2 \sigma _ { t } ^ { 2 } ) } \right)  d  \mathbf { t } . } \end{array} \quad ( 4 9 )$$

16

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

We rewrite $\|\gamma + \sigma \epsilon\|^2$ as $\|y\|^2 + 2\sigma \langle  y, (\sigma_t \epsilon) + \sigma_t^2\epsilon \|^2$ and note that expectation of the second term is zero. The first term is then equal to

$$\begin{array} { r } { \frac { 1 } { 2 \pi \sigma _ { x } ^ { 2 } } \int _ { \omega _ { x } } \omega _ { x } \,  d  \mathbf r \cdot  \mathbb { E } _ { x _ { 0 } } ( \omega ) \| \mathbf B \| ^ { 2 } + \frac { 1 } { 2 \pi \sigma _ { x } ^ { 2 } } \int _ { \Omega _ { x } } \omega _ { x } ^ { 2 }  d  \mathbf r \cdot  \mathbb { E } _ { x } \chi _ { x } ( 0 , t _ { 0 } ) \| \mathbf v \| ^ { 2 } . } \end{array} \quad ( 5 0 )$$

Boundedness of $\omega_t$ (Assumption A.3 ) implies that the first integral is finite and, say, equal to $C_1$ . The second integral contains a product of bounded $\omega_t$ and continuous $\sigma_t^2$ (Assumption A.4 ), which is also integrable. We then denote the second summand by $C_2$ and rewrite the first summand as

$$\begin{array} { r } { C _ { 1 } \mathbb { E } _ { \pi _ { y } ( y ) } \| y \| ^ { 2 } + C _ { 2 } . \quad ( 5 1 ) } \end{array}$$

As for the second summand, we see that the expectation

$$\begin{array} { r l } & { \mathbb { E } _ { p ^ { \prime } \tau (  y  ) \mathcal { N } ( \varepsilon | 0 , 1 ) } e ^ { |  y  +  \sigma  \varepsilon | ^ { 2 } / ( 2 \sigma _ { \tau } ^ { 2 } ) } \quad ( 5 2 ) } \end{array}$$

with respect to $\varepsilon$ will be finite, because $\sigma_f^2/(2\pi^2)$ is always less than $1/2$, which will make the exponent have negative degree. Moreover, simple calculations show that this function will be continuous with respect to $\sigma_f$ and have only quadratic terms with respect to $y$ inside the exponent, i.e. have the form

$$e ^ { a ( \sigma _ { t } ) } \| y - b ( \sigma _ { t } ) \| ^ { 2 } + c ( \sigma _ { t } ) \quad ( 5 3 )$$

with continuous $a, b, c$. We now want to prove that the expectation

$$\begin{array} { r } { \mathbb { E } _ { p ^ { T } ( y ) } e ^ { \alpha ( \sigma _ { t } ) \| y - \beta ( \sigma _ { t } ) \| ^ { 2 } + \gamma ( \sigma _ { t } ) } ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~$$

will also be continuous in $t$. First, due to the boundedness of $\boldsymbol{y}$, this expectation is finite. Second, for $t_n \to t$:

$$\begin{array} { r l } & { \underset { n \to \infty } { \operatorname* { l i m } } \, _ { t \to \infty } \mathbb { E } _ { p ^ { \prime } ( t ) } e ^ { i \left( \langle  \sigma _ { 1 } \rangle _ { n } \right) t } \| y - \langle  \sigma _ { 1 } \rangle _ { n } \| ^ { 2 } + \langle  \sigma _ { 1 } \rangle _ { n } \| ^ { 4 } = } \\ & { \quad = \underset { n \to \infty } { \operatorname* { \mathop { \mathbb { E } } } _ { p ^ { \prime } ( t ) } } \underset { t \to \infty } { \mathbb { E } } e ^ { i \langle  \sigma _ { 1 } \rangle _ { n } } \| y - \langle  \sigma _ { 1 } \rangle _ { n } \| ^ { 2 } + \langle  \sigma _ { 1 } \rangle _ { n } \| ^ { 4 } = } \\ & { \quad = \underset { n \to \infty } { \operatorname* { \mathbb { E } } } e ^ { i \langle  \sigma _ { 1 } \rangle _ { n } } \| y - \langle  \sigma _ { 1 } \rangle \| ^ { 2 } + \langle  \sigma _ { 1 } \rangle ^ { 4 } . } \end{array}$$

due to the Theorem A.16 (Lebesgue's dominated convergence). It is applicable, since $y$ is bounded and all the functions are continuous, thus bounded in $[0, T]$.

We thus obtain that the second integral contains bounded $\omega_2$ multiplied by the logarithm of continuous function, which is always $\geq 1$ (positive exponent). This means that the whole integral is finite. Denoting it by $C_3$ , we obtain

$$C _ { 1 } \mathbb { E } _ { \pi _ { u } ( y ) } \| y \| ^ { 2 } + C _ { 2 } - C _ { 3 } \leq  \int _ { 0 } ^ { T } \omega _ { t }  K L  \left( \pi _ { y , t } \| \rho _ { t } ^ { \prime } \right) d t .\quad (58)$$

Combined with the condition of the lemma, we obtain

$$C _ { 1 } \mathbb { E } _ { \pi _ { y | t } | | y | ^ { 2 } } + C _ { 2 } - C _ { 3 } \leq  \int _ { 0 } ^ { T } \omega _ { t }  K L  \left( \pi _ { y , t } | | p _ { t } ^ { T } \right)  d  t \leq  C , \quad   ( 5 9 )$$

which implies

$$\begin{array} { r } { \mathbb { E } _ { x _ { y } | y } \| y \| ^ { 2 } \leq  \frac { C + C _ { 3 } - C _ { 2 } } { C _ { 1 } } : = C _ { 4 } . \quad ( 6 0 ) } \end{array}$$

We thus obtained a uniform bound on some statistic with respect to all measures from $\{\pi^n\}$. The function $|y|^2$ has compact sublevel sets $\{|y|^2 \le r\}$. Lemma A.8 then states that the sequence $\pi_y^n$ is tight, i.e. for all $\varepsilon > 0$ there is a compact set $K_\varepsilon$ with $\pi_y^n(y_K) \ge 1 - \varepsilon$.

Finally, marginal $\sigma$ distribution of each of the $\pi^i$ is $p^{\tilde{\sigma}^i}$, which is bounded (Assumption A.1), i.e. there is a compact $K$ that $\sigma(\pi \in K) = 1$. Combined with the previous observation, we obtain

$$\pi^{n}\left(x \in K, y \in K_{\varepsilon}\right) \geq 1-\varepsilon\quad (61)$$

for all n. The cartesian product $K\times K_r$ is also compact. Theorem A.7 (Prokhorov) then implies that the sequence $\pi^n$ is all.

17

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Now we are ready to prove the following

Lemma A.23. Ifinimum of the loss $\mathcal{L}^{\pi}(\pi)$ over all generator-based transport plans $\pi$ (with $\pi_{\pi} = p^{\delta}$ and $\pi(y = G(x))$ for some $G$ ) is attained on some plan $\hat{\pi}$ .

Proof. We start by observing that there is at least one feasible $w$ with the aforementioned properties. For this purpose one can take the optimal transport $G^*$ between $p^*$ and $p^T$ , which is unique by Theorem A.18 under Assumptions A.1 , A.2 , and A.3 .

Let $\pi^v$ be a sequence of feasible generator-based measures that $L^v(\pi^v)$ converges to the corresponding infimum $L_{inf}^v$ (it exists by the definition of the infimum). Without loss of generality, we can assume that $L^v(\pi^v) \leq  L_{inf}^v + 1$ for all $v$ (if not, one can drop large enough sequence prefix). This implies that for all $v$ holds

$$\begin{array} { r } { \alpha \int \limits _ { 0 } ^ { T } \omega _ { t }  K L  \left( \pi _ { y , t } \| p _ { t } ^ { T } \right)  d  t \leq  \mathcal { L } _ {  i n f  } ^ { \alpha } + 1 . } \end{array} \quad ( 6 2 )$$

Lemma A.22 implies that the sequence $\pi^n$ is tight. Prochonov theorem then states that $\pi^n$ has a weakly convergent subsequence $\pi^{n+1} \to \pi$. Lower semi-continuity of the loss $\mathcal{L}(\pi^n)$ implies that

$$\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \mathcal { L } ^ { \alpha } ( \pi ^ { n _ { k } } ) \geq \mathcal { L } ^ { \alpha } ( \hat { \pi } ) \geq \mathcal { L } _ {  i n i .  } ^ { \alpha } . \quad ( 6 3 ) } \end{array}$$

At the same time, $\mathcal{L}^{\alpha}(\pi^{n_{\lambda}})$ is assumed to converge to $\mathcal{L}_{inf}^{\alpha}$, which means that $\hat{\pi}$ is indeed the minimum.

### A.6. Finish of the proof

Theorem A.1. proof. Finally, we combine the previous technical observations with the proof sketch from the Section A.1. Let $\alpha_n \to \infty$ be a sequence of coefficients, $G^{\alpha_n}$ be the optimal generators with respect to $\mathcal{L}^{\alpha_n}$ and $\pi^{\alpha_n}$ the joint distributions of $(x, G^{\alpha_n}(x))$. Additionally, we define $\pi^\infty$ to be the optimal transport plan, corresponding to $(x, G^{\infty}(x))$, where $G^{\infty}(x)$ is the optimal transport map. First, due to the monotonicity of $\mathcal{L}^{\infty}$ with respect to $\alpha$, we have

$$\mathcal { L } ^ { \alpha _ { a } } ( \pi ^ { \alpha _ { a } } ) \leq  \mathcal { L } ^ { \alpha _ { a + 1 } } ( \pi ^ { \alpha _ { a + 1 } } ) \leq  \mathcal { L } ^ { \infty } ( \pi ^ { \infty } ) . \quad ( 6 4 )$$

This implies that for all n holds

$$\begin{array} { r } { \alpha _ { n } \int _ { 0 } ^ { T } \omega _ { 1 }  K  ( \tilde { \tau } _ { y , t } ^ { ( n ) } \| p _ { t } ^ { n } )  d  t \leq  \mathcal { L } ^ { \infty } ( \pi ^ { \infty } ) \Rightarrow } \end{array} \quad ( 6 5 )$$

$$\begin{array} { r } { \Rightarrow \int \limits _ { 0 } ^ { T } \omega _ { t }  K L  \left( \pi _ { y , t } ^ { \alpha _ { n } } \Vert \rho _ { t } ^ { T } \right)  d  t \leq  \frac { \mathcal { L } ^ { \infty } ( \pi ^ { \infty } ) } { \alpha _ { n } } \leq  \frac { \mathcal { L } ^ { \infty } ( \pi ^ { \infty } ) } { \operatorname* { m i n } \sigma _ { n } } , } \end{array} \quad ( 6 6 )$$

which is finite, since $\alpha_n \to +\infty$. One more time, we apply Lemma A.22 and conclude that the sequence $\pi^{n+1}$ is tight. Let $\pi^{n+1}$ be its weakly convergent subsequence: $\pi^{n+1} \xrightarrow{n} \pi$. Analogously to the Section A.1, we observe that

$$\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \mathcal { L } ^ { \alpha _ { n + 1 } } ( \pi ^ { \alpha _ { n + 1 } } ) \geq \operatorname* { l i m } _ { k \to \infty } \mathcal { L } ^ { \alpha _ { n + 1 } } ( \pi ^ { \alpha _ { n + 1 } } ) \geq \mathcal { L } ^ { \alpha _ { n + 1 } } ( \overline { \pi } ) \quad ( 6 7 ) } \end{array}$$

for any fixed m. The first inequality is due to the monotonicity of $\mathcal{L}^0$ with respect to $\alpha$ and second is the implication of lower semi-continuity of the loss $\mathcal{L}^0$ with respect to weak convergence. Taking the limit $m \to \infty$, we obtain

$$\begin{array} { r } { \operatorname* { l i m } _ { k \to \infty } \mathcal { L } ^ { \alpha _ { n _ { k } } } ( \pi ^ { \alpha _ { n _ { k } } } ) \geq \mathcal { L } ^ { \infty } ( \hat { \pi } ) . } \end{array} \quad ( 6 8 )$$

Combining all these observations, we obtain the following sequence of inequalities

$$\mathcal { L } ^ { \infty } ( \pi ^ { \infty } ) \geq \operatorname* { l i m i n f } _ { k \to \infty } \mathcal { L } ^ { \alpha _ { n _ { k } } } ( \pi ^ { \alpha _ { n _ { k } } } ) \geq \mathcal { L } ^ { \infty } ( \hat { \pi } ) \geq \mathcal { L } ^ { \infty } ( \pi ^ { \infty } ) , \quad   { ( 6 9 ) }$$

18

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

which implies that the limiting measure $\hat{\pi}$ reaches the minimum of the objective over generator-based plans. By the uniqueness of the optimal transport map $G^*$ under the Assumptions A.1 , A.2 , A.3 , we conclude that all the convergent subsequences $\pi^{n_k}$ converge to the optimal measure $\pi^\infty$ . Using Corollary A.9 of the Prokhorov theorem, we deduce that $\pi^{n_n} \to \pi^\infty$.

Finally, we want to replace the weak convergence of $\pi^{o_n}$ to $\pi^\infty$ with the convergence in probability of the generators, i.e. show

$$G ^ { \alpha _ { n } } \xrightarrow { p ^ { \xi } } G ^ { \infty } . \quad  ( 7 0 )$$

To this end, we represent the corresponding probability as the expectation of the indicator and upper bound it with a continuous function:

$$\begin{array} { r l } { p ^ { \mathcal { Z } } \left( \left\| G ^ { n _ { 0 } } ( x ) - G ^ { \infty } ( x ) \right\| > \varepsilon \right) } & { = \mathbb { E } _ { p ^ { \mathcal { Z } } ( x ) } d \left( \left\| G ^ { n _ { 0 } } ( x ) - G ^ { \infty } ( x ) \right\| > \varepsilon \right) } \\ & { \leq  \mathbb { E } _ { p ^ { \mathcal { Z } } ( x ) } d \left( \left\| G ^ { n _ { 0 } } ( x ) - G ^ { \infty } ( x ) \right\| > \varepsilon \right) . } \end{array} \quad \begin{array} { r l } & { ( 7 1 ) } \\ & { ( 7 2 ) } \end{array}$$

where $d$ is a continuous indicator approximation, defined as

$$d ( u , v ) = \begin{cases} \frac { \| u - v \| } { \| v \| } , &  ~ i ~ f ~  \ 0 \leq  \| u - v \| < c ; \\ 1 , &  ~ i ~ f ~  \ \| u - v \| \geq c . \end{cases} \quad ( 7 )$$

We define the test function

$$\varphi ( x , y ) = d \left( y , G ^ { \infty } ( x ) \right) \quad ( 7 4 )$$

and rewrite the upper bound as

$$\begin{array} { r } { \mathbb { E } _ { p ^ { < s } ( x ) } d \left( G ^ { \alpha _ { n } } ( x ) , G ^ { \infty } ( x ) \right) = \mathbb { E } _ { p ^ { < s } ( x ) } \varphi ( x , G ^ { \alpha _ { n } } ( x ) ) = \mathbb { E } _ { p ^ { \alpha _ { n } } ( x , y ) } \varphi ( x , y ) . \quad ( 7 5 ) } \end{array}$$

Due to Assumptions A.1, A.2 and Theorem A.4 the optimal transport map $G^{**}$ is continuous, which implies that this test function is bounded and continuous. Given the weak convergence of $\sigma^{n_{k}}$ , we have

$$\begin{array} { r l } { E _ { \gamma = \gamma } ( x , y ) \varphi ( x , y ) \to E _ { \gamma = \gamma } ( x , y ) \varphi ( x , y ) E _ { \gamma = \gamma } ( x , G ^ { - } ( x ) ) = } & {  \quad ( 7 6 ) } \\ { = E _ { \gamma = \gamma } ( d ( G ^ { - } ( x ) , G ^ { - } ( x ) ) ) = , } & {  \quad ( 7 7 ) } \end{array}$$

which implies the desired

$$p ^ { S } \left( \| G ^ { \alpha _ { s } } ( x ) - G ^ { \infty } ( x ) \| > \varepsilon \right) \to 0 . \quad ( 7 8$$

## B. Ablation of the initialization parameter

In this section, we further explore the design space of our method by investigating the effect of the fixed generator input noise parameter $\sigma$ on the resulting quality. To this end, we take the colored version of the MNIST (LeCun, 1998) data set and perform translation between the digits "2" and "3" initializing from various $\sigma$ . We use a small UNet architecture from (Gushchin et al., 2024) .

The parameter $\alpha$ is residual from the pre-trained diffusion architecture and therefore fixed throughout training and evaluation. However, the target denoiser network tries to convert the expected noisy input into the corresponding sample from the output distribution. Consequently, one may expect that at a suitable noise level, the generator may change the input's details to make them look appropriate for the target while preserving the original structural properties.

We demonstrate this effect on various noise levels in Figure 6 . Here we observe that the small signals lead to the mapping close to the identity, whereas the large signals lead to almost constant blurry images, corresponding to the average "3" of the data set. However, there is a segment $[1.0, 10.0]$ of levels that gives a moderate-quality mapping in terms of both faithfulness and realism, which makes it a suitable initial point. Note that the FID-L2 plot is not monotone at high L2 values in comparison with the FID-Noise plot in Figure 5 (right). This is because the variance of the noise features can easily be investigate optimal $\sigma$ choice by going through a 2D grid of the hyperparameters $(\sigma, \lambda)$ and aim to see if it is possible to choose the uniform best noise level. In Figure 6 we report the faithfulness-quality trade-off concerning various $\sigma, \lambda$ . We

19

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

![Figure](figures/37030_page_020_figure_001.png)

Figure 6. Left: Visualization of the generator initialization at various $\sigma \in [0.1, 80, 0]$, where $\sigma$ is the noise level parameter residual from the pre-trained diffusion architecture. Right: comparison of different $\sigma$ terms of the quality-faithfulness trade-off. The metrics are obtained by initializing the generator at the corresponding $\sigma$ level and training it with the RDMD procedure. Here, $\lambda \in \{0, 1.0, 2.0, 4.0\}$. Higher $\lambda$ corresponds to the lower transport cost values.

observe that there is almost monotone dependence on $\sigma$ on the segment $[1.0, 40.0]$; here the $\sigma = 1.0$ gives almost uniformly best results in terms of both metrics. Similar results are obtained by the values $5.0, 10.0$ which have fair quality visual results at initialization. Therefore, we conclude that it is best to choose the least parameter $\sigma$ among the parameters with appropriate visuals at the initial point.

## C. Experimental Details

### C.1. 2D experiments

Architecture. The architecture used for training diffusion model and generator ( De Bortoli et al. , 2021 ) consists of input-encoding MLP block, time-encoding MLP block, and decoding MLP block. Input encoding MLP block consists of 4 hidden layers with dimensions $[16, 32, 32, 32]$ interspersed by LeakyReLU activations. Time encoding MLP consists of a positional encoding layer ( Vaswani et al. , 2017 ) and then follows the same MLP block structure as the input encoder. The decoding MLP block consists of 5 hidden layers with dimensions $[128, 256, 128, 64, 2]$ and operates on concatenated time embedding and input embedding each obtained from their respective encoder. The model contains 88k parameters.

Training Diffusion Model. Diffusion model was trained for 100k iterations with batch size 1024 with Adam optimizer (Kingma & Ba, 2014) with learning rate $10^{-4}$.

Training RDMD. Fake denoising network was trained with Adam optimizer with learning rate $10^{-4}$ . The generator model was trained with a different Adam optimizer with a learning rate equal to $2\cdot  10^{-5}$ . We trained RDMD for 100k iterations with batch size 1024.

Computational resources. All the experiments were run on CPU. Running 100k iterations with the batch size 1024 took approximately 1 hour.

### C.2. Colored MNIST

Architecture. We used the architecture from (Gushchin et al., 2024) , which utilizes convolutional UNet with conditional instance normalization on time embeddings used after each upscaling block of the decoder. $^5$ Model produces time embeddings via positional encoding. The model size was approximately $9.9M$ parameters.

'https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/06feb6ba8b43865a30bd0b626384fa64da39b

src/cunet.py

Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation

Training Diffusion Model. The diffusion model was trained for 24500 iterations with batch size $192$. We used the Adam optimizer with a learning rate equal to $4 \cdot  10^{-3}$. The model was trained in PP2P, it obtained FID equal to 2.09.

Training RDMD. Fake denoising network was trained with Adam optimizer with a learning rate equal to $2\cdot 10^{-3}$. The generator model was trained with Adam optimizer with learning rate $5\cdot 10^{-5}$. RDMD was trained for 7300 iterations with batch size 4096.

Computational resources. All the experiments were run on 2x NVIDIA GeForce RTX 4090 GPUs. Training Diffusion model for 24500 iterations with batch size 8192 took approximately 6 hours. Training RDMD for 7300 iterations with batch size 4096 took approximately 3 hours.

### C.3. Cat2Wild

Architecture. We used the SongUet architecture from EDM repository 6 , which corresponds to DDPM++ and NCSNN++ networks from the work ( Song et al. , 2020 ) . The model contains approximately 55M parameters.

Training Diffusion Model. The diffusion model was trained for 80k iterations. We set the batch size to 512 and chose the best checkpoint according to FID. We used the Adam optimizer with a learning rate equal to $2 \cdot  10^{-4}$. We used a dropout rate equal to 0.25 during training and the augmentation pipeline from (Karras et al., 2022) with probability 0.15. The model was trained in FP32. It obtained FID equal to 2.01.

Training RDMD. In all runs we initialized the generator from the target diffusion model with the fixed $\sigma=1.0$. We've run 4 models, corresponding to the regularization coefficients $[0.02, 0.05, 0.1, 0.2]$. All models were trained with Adam optimizer with generator learning rate $5 \cdot  10^{-5}$ and fake diffusion learning rate $3 \cdot  10^{-4}$. We trained all models for 25000 iterations with batch size 512. Training took approximately 35 hours on 4 x NVidia Tesla A100 80GB.

ILVR hyperparameters. The only hyperparameter of ILVR is the downsampling factor N for the low-pass filter, which determines whether guidance would be conducted on coarser or finer information. $n_\text{steps}$ denotes number of sampling steps. All metrics in Figure 4 for ILVR were obtained on the following hyperparameter grid: $N = [2, 4, 8, 16, 32]$, $n_\text{steps} = [18, 32, 50]$ . We excluded runs that have the same statistical significance and achieve FID higher than 20.0. The images in Figure 5 were obtained with hyperparameters $N = 16$ and $n_\text{steps} = 18$ .

SDEdit hyperparameters. The only hyperparameter of SDEdit is the noise level $\sigma$ , which acts as a starting point for sampling. The higher the noise level, the closer is the sampling procedure to the unconditional generation. The smaller the noise values, the more features are carried over to the target domain at the expense of generation quality. $n_{samp}$ denotes number of sampling steps. All metrics in Figure 4 for SDEdit were obtained on the following hyperparameter grid: $\sigma=\{4.5, 10, 15, 20, 30, 40\}$ . $n_{samp} = \{18, 32, 50\}$ . We exclude runs that have the same statistical significance and achieve FID higher than 20.0. The images in Figure 5 were obtained with hyperparameters $\sigma=10$ and $n_{samp}=50$ .

EGSDE hyperparameters. EGSDE sampling hyperparameters include the initial noise level $\alpha$ at which the source image is perturbed, and the downsampling factor $N$ for the low-pass filter. $n_\text{sample}$ denotes number of sampling steps. All metrics in Figure 4 for EGSDE were obtained on the following hyperparameter grid: $\sigma = [5, 10, 15, 20, 40]$ , $N = [8, 16, 32]$ , $n_\text{upsp} = [18, 32]$ . We exclude runs that have the same statistical significance and achieve FID higher than 20.0. The images in Figure 5 were obtained with hyperparameters $\sigma = 10$ , $N = 32$ , $n_\text{upsp} = 30$ .

7 https://github.com/NVlabs/edm/blob/008a4e5316c8e3bfe61a62f874bddba254295afb/training/ networks.py

21

