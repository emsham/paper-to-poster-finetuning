{
  "paper_id": "37030",
  "paper_title": "Regularized Distribution Matching Distillation",
  "paper_abstract": "Test abstract",
  "paper_markdown": "# Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nDenis Rakitin $^{*1}$ Ivan Shchekotov $^{*12}$ Dmitry Vetrov $^{3}$\n\n## Abstract\n\nDiffusion distillation methods aim to compress the diffusion models into efficient one-step generators while trying to preserve quality. Among them, Distribution Matching Distillation (DMD) offers a suitable framework for training general-form onestep generators, applicable beyond unconditional generation. In this work, we introduce its modification, called Regularized Distribution Matching Distillation, applicable to unpaired image-toimage problems. We demonstrate its empirical performance in application to several translation tasks, including 2D examples and I2I between different image datasets, where it performs on par or better than multi-step diffusion baselines.\n\n## 1. Introduction\n\nOne of the global problems of contemporary generative modeling consists of solving the so-called generative learning trilemma ( Xiao et al. , 2021 ) . It states that a perfect generative model should possess three desirable properties: high generation quality, mode coverage/diversity of samples and efficient inference. Today, most model families tend to have only 2 of the 3. Generative Adversarial Networks (GANs) ( Goodfellow et al. , 2014 ) have fast inference and produce high-quality samples but tend to underrepresent some modes of the data set ( Metz et al. , 2016 ; Arjovsky et al. , 2017 ) . Variational Autoencoders (VAEs) ( Kingma & Welling , 2013 ; Rezende et al. , 2014 ) efficiently produce diverse samples while suffering from insufficient generation quality. Finally, diffusion-based generative models ( Ho et al. , 2020 ; Song et al. , 2020 ; Dhariwal & Nichol , 2021 ; Karras et al. , 2022 ) achieve SOTA generative metrics and\n\nvisual quality yet require running a high-cost multi-step inference procedure.\n\nSatisfying these three properties is essential in numerous generative computer vision tasks beyond unconditional generation. One is image-to-image (I2I) translation ( Isola et al. , 2017 ; Zhu et al. , 2017 ) , which consists of learning a mapping between two distributions that preserves the crossdomain properties of input object while appropriately changing its source-domain features to match the target. Most examples, like transforming cats into dogs ( Choi et al. , 2020 ) or human faces into anime ( Kohrith et al. , 2022 ) belong to the unplained I2I because they do not assume ground truth pairs of objects in the data set. As in unconditional generation, shared fixed-parameter diffusion models use various GANs ( Huang et al. , 2018 ; Park et al. , 2020 ; Choi et al. , 2020 ; Zheng et al. , 2022 ) , but now tend to be compacted and surpassed by diffusion-based counterparts ( Choi et al. , 2021 ; Meng et al. , 2021 ; Zhao et al. , 2022 ; Wu & De Torre , 2023 ) . Most of these methods build on top of the original diffusion sampling procedure and tend to have high generation time as a consequence.\n\nSince diffusion models succeed in both desirable qualitative properties of the trilemma, one could theoretically obtain samples of the desired quality level given sufficient computational resources. It makes the acceleration of diffusion models an appealing approach to satisfy all of the aforementioned requirements, including efficient inference.\n\nRecently introduced diffusion distillation techniques (Soug et al., 2023; Kim et al., 2023b; Sauer et al., 2023) address this challenge by compressing diffusion models into onestep students with (hopefully) similar qualitative and quantitative properties. Among them, Distribution Matching Distillation (DMD) (Yin et al., 2023; Nguyen & Tran, 2023) offers an expressive and general framework for training freeform generators based on these methods. The structure form of DMDs (Yin et al., 2023; Nguyen et al., 2023) produce here means that the method does not make any assumptions about the generator's structure and distribution at the input. This crucial observation opens a large space for its applications beyond the noise $\\rightarrow$ data problems.\n\nIn this work, we introduce the modification of DMD, called\n\nEqual contribution 1HSE University, Moscow, Russia\n\n$^{2}$Skolkovo Institute of Science and Technology, Moscow, Russia\n\n*Correspondence: University, Bremen, Germany. Correspondence to: E.B. (E.B.0341@i.cuhk.edu). E-mail address: e.beguidas@cuhk. cuhk.edu.\n\nAccepted by the Structured Probability Inference & Generative Modelling workshop \u00a9 ICML 2024, Vienna, Austria. Copyright\n\n1\n\narXiv:2406.14762v1 [cs.CV] 20 Jun 2024\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\n![Figure](figures/37030_page_002_figure_001.png)\n\nFigure 1. Illustration of performance of the proposed RDMD model on $cat \\to wild$ translation problem/ from the AFHQv2 ( Choi et al. , 2020 ) data set.\n\nRegularized Distribution Matching Distillation (RMD), that applies to the unpaired I2I problems. To achieve this, we replace the generator's input noise with the source data samples to further translate them into the target. We maintain the overall optimization parameters in state and output by regularizing the objective with the transport cost between them. As our main contributions, we\n\n- 1. Propose a one-step diffusion-based method for un-\npaired I2I;\n2. Theoretically verify it by establishing its connection\nwith optimal transport ( Villani et al. , 2009 ; Peyr\u00e9 et al. ,\n2019 ) ;\n3. Ablate its qualitative properties and demonstrate its\ngeneration quality on 2D and image-to-image exam-\nples, where it obtains comparable or better results than\nthe multi-step counterparts.\n## 2. Background\n\n### 2.1. Diffusion Models\n\nDiffusion models (Song & Ermon, 2019; Ho et al., 2020) are a class of models that sequentially perturb data distribution $p^k$ with Gaussian noise, transforming it into some tractable parametric distribution, which contains no information about initial domain.\n\nUsing this distribution as a prior and reversing the process by progressively removing the noise yields a sampling procedure from $p(\\mathbf{x})$ . A convenient way to formalize diffusion models is through stochastic differential equations (SDEs) defined in 2020 as a novel process that time stochastic dynamics of particles. The forward process is commonly defined as the Variance Exploding (VE) SDE:\n\n$$d x_{t}=g(t) d w_{t},\\quad (1)$$\n\nwhere $t \\in [0, T]$, $x_{0} \\sim \\mathcal{U}_{d \\times d}$, $q(\\cdot )$ is the scalar diffusion coefficient and $dw_t$ is the differential of a standard Wiener process. We denote by $p_t(x_t)$ marginal distribution of $x_t$, so that $p^{data}(x_0) = p_0(x_t)$, $p_T$ acts as an unstructured prior distribution that we can sample from.\n\nConveniently, SDE dynamics can be represented via a deterministic counterpart given by an ordinary differential equation (ODE), which yields the same marginal distributions $p_i(x_0) = e^{i\\theta(x_0)}f_i(x_0)$ , given the same initial distribution\n\n$$\\begin{array} { r l } {  d  x _ { t } } & {  = - \\frac { 1 } { 2 } \\beta ^ { t } ( t ) \\nabla _ { x } \\log p _ { t } ( x _ { t } )  d  t . } \\end{array} \\quad ( 2 )$$\n\nwhere $\\nabla_{x_i} \\log p_i(x_i)$ is called the score function of $p_i(x_i)$ . Equation 2 is also called Probability Flow ODE (PF-ODE). The ODE formulation allows us to obtain a backward process by simply reversing velocity of the particle. In particular we can obtain samples from $p^{ab}$ by taking $x_i' \\approx p_{ij}$ and then using the PF-ODE backwards in time, given access to the score function.\n\nHowever, in the case of generative modeling $\\nabla_{\\theta} \\log p_i(\\theta)$ , is intractable due to $p^{data}$ being intractable, and thus cannot be used directly in Equation 2 . Under mild regularity conditions, the unconditional score can be expressed by:\n\n$$\\nabla_{x_{i}} \\log p_{t}\\left(x_{i}\\right)=E_{p_{0: t}\\left(x_{0}\\right) \\mid x_{0}}\\left[s_{t \\mid 0}\\left(x_{0} ; x_{0}\\right)\\right] .\\quad (3)$$\n\nwhere $s_{r_i}(x_i|x_0) = \\sqrt{\\pi}$, $\\log p_{r_i}(x_i|x_0)$ is the conditional distribution (also called perturbation kernel) and $p_{r_i}(x_i|x_0)$ is the corresponding posterior distribution. The perturbation kernel in the case of VE-SDE corresponds to simply adding an independent Gaussian noise:\n\n$$p _ { n _ { 0 } } ( x _ { t } | x _ { 0 } ) = \\mathcal { N } ( x _ { t } | x _ { 0 } , \\sigma _ { t } ^ { 2 } ) , \\sigma _ { t } ^ { 2 } = \\int _ { y } ^ { 1 } g ^ { 2 } ( y )  d  y . \\quad ( 4 )$$\n\nDenoising Score Matching (DSM) (Vincent, 2011) utilizes Equation and approximates $\\nabla_{\\theta} \\log p_t(\\sigma_t)$ with the score\n\n$^{1}$The other popular forward processes (e.g. VP-SDE) can be obtained by scaling the VE-SDE.\n\n2\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nmodel $s_{i}^{\\theta}(\\boldsymbol{x}_{t})$ via L2-regression minimization:\n\n$$\\int _ { 0 } ^ { T } \\mathbb { E } _ { \\rho ( t ) | ( x _ { 0 } | x _ { 1 } ) } \\| s _ { t } ^ { \\theta } ( x _ { t } ) - s _ { t } ( 0 ) ( x _ { t } | x _ { 0 } ) \\| ^ { 2 }  d  t \\to \\operatorname* { m i n } _ { \\theta } , ( 5 )$$\n\nwhere $\\beta_i$ is some positive weighting function. The minimum in the Equation 5 is obtained at $s_i(x)_i = -\\nabla_{x_i}\\log p_i(x_i)$ . Given a suitable parameterization of the score network, DSM objective is equivalent to\n\n$$\\int _ { 0 } ^ { T } \\beta _ { t } \\mathbb { E } _ { \\rho _ { 0 , t } ( x _ { 0 } , x _ { t } ) } [ D _ { t } ^ { \\rho } ( x _ { t } ) - x _ { 0 } ] ^ { 2 } d t \\to \\operatorname* { m i n } _ { \\theta } . \\quad ( 6 )$$\n\nWhere $D_i^0$ is called the denoising network (or simply denoise) and is related to the score network via $s_i'(x_i) = \\langle  x_i - D_i^0(x_i) \\rangle / \\sigma_i^2$ . Therefore, Denoising Score Matching (DSM-M) consists of learning to denoise images at various noise levels:\n\nHaving obtained $s_T^0(x_t)$ , we solve Equation 2 backward in time, starting from $x_T \\sim \\mathcal{N}(0, \\sigma_T^2 I)$ to obtain approximate samples from $p^{data}_{T}$ .\n\n### 2.2. Distribution Matching Distillation\n\nDistribution Matching Distillation (Yin et al., 2023) is the core technique of this paper. Essentially, it aims to train a generator $G_\\theta(z)$ on matching the given distribution $p^{\\text{nat}}$ . Its input $z$ is assumed to come from a tractable input distribution $p$ , such that the inputs of $G_\\theta$ and $p$ can be be achieved by optimizing the KL divergence between the distribution: $p^*$ of $G_\\theta(z)$ and the data distribution $p^{\\text{nat}}$ .\n\n$$\\begin{array} { r } {  L L  ( \\rho ^ { \\prime } | \\rho ^ {  p o l  } ) = \\mathbb { E } _ { \\rho ^ {  p o l  } : z \\sim \\log } \\left[ \\rho ^ {  p o l  } ( G ( z ) | z ) \\right] - \\operatorname* { m i n } _ { g } \\quad ( 7 ) } \\end{array}$$\n\nDifferentiating it by the parameters $\\theta$ , using the chain rule, one encounters a summand, containing the difference $s^\\alpha(\\hat{G}_0(z)) - s^{\\text{cal}}(\\hat{G}_0(z))$ between the score functions of the corresponding distributions $^1$ . The pure data score function can be very non-smooth due to the Manifold Hypothesis (Tenenbaum et al., 2000) and is generally hard to train (Song & Ermon, 2019) , so the authors make the probability distribution of the original ones, but not necessarily end, they replace the original loss with an ensemble of KL divergences between distributions, perturbed by the forward diffusion process:\n\n$$\\int _ { T } ^ { T _ {  ~ \\scriptsize ~ c ~  } } \\omega \\,  K L  ( \\hat { p } _ { t } ^ { \\pi } \\| p _ { t } ^ {  ~ \\scriptsize ~ c ~  } )  d  t . \\quad ( 8 )$$\n\nHere, $\\omega_p$ is a weighting function, $p'_1$ and $p'_2$ are the perturbed versions of the generator distribution and $p_{rad}$ up to the time step $t$ . In theory, the minima of Equation 8 objective coincides (Wang et al., 2024, Thm. 1) with the original minima from Equation 7 . Meanwhile in practice taking the gradient of the new loss, which can be equivalently written as\n\n$$\\int _ { 0 } ^ { T } \\int _ { \\Omega } \\mathbb { E } _ { \\mathcal { N } ( 0 , t ) } p ^ {  s o r  } ( x ) \\log \\frac { p _ { t } ^ { \\theta } ( G _ { 0 } ( x ) + \\sigma _ { t } \\varepsilon ) } { p _ { t } ^ {  r e l  } ( G _ { 0 } ( x ) + \\sigma _ { t } \\varepsilon ) }  d  t . \\quad   ( 9 )$$\n\nresults in obtaining difference $s_i^d(G_\\theta(z) + \\sigma_i z) - s_i^real(G_\\theta(z) + \\sigma_i z)$, which can be approximated by the diffusion models.\n\nGiven this, authors approximate $s^{real}_{t}$ with the pre-trained diffusion model, which we will denote $\\hat{s}^{real}_{t}$ and as well with a slight abuse of notation. The whole procedure now can be considered as distillation of $\\hat{s}^{real}_{t}$ into $G_{t}$. At the same time, $s^{*}_{t}$ is the score of the noised distribution of the generator, which is intractable and therefore approximated by an additional \"fake\" diffusion model $s^{*}_{t}$ and the corresponding denoise $D_{t}$. It is natural on the other hand that the score that is computed by the generator is the generator's samples at the time $T$. The joint training procedure is essentially the coordinate descent\n\n$$\\left\\{ \\begin{array} { l c } { \\displaystyle \\int _ { 0 } ^ { T } \\omega _ { e , x } \\log \\frac { p _ { t } ^ { 2 } ( G _ { 0 } ( t ) + \\sigma _ { t } \\varepsilon ) } { p _ { t } ^ { 2 } ( G _ { 0 } ( t ) + \\sigma _ { t } \\varepsilon ) }  d  t \\to \\operatorname* { m i n } ; } \\\\ { \\displaystyle \\int _ { 0 } ^ { T } \\beta _ { t } \\varepsilon _ { x , x } \\| D _ { t } ^ { 2 } ( G _ { 0 } ( t ) + \\sigma _ { t } \\varepsilon ) - G _ { 0 } ( t ) \\| ^ { 2 }  d  t \\to \\operatorname* { m i n } . } \\end{array} \\right.$$\n(10)$$\n\nwhere the stochastic gradient with respect to the fake network $z$ is given by (2) . The loss with the generator's stochastic gradient is calculated directly as\n\n$$\\begin{array} { r } { \\dot { \\omega } _ { i } \\left( s _ { i } ^ { 0 } - s _ { i } ^ {  e x t  } \\right) \\nabla _ {  g  } G _ { 0 } (  z  ) , \\quad ( 1 1 ) } \\end{array}$$\n\nwhere the scores are evaluated in the point $G_0(z) + \\sigma\\varepsilon z$ . Minimization of the fake network's objective ensures $s_i^b \\leq  s_i^a = p_i^a$ . $s_i^b \\leftrightarrow s_i^a = p_i^a$ . At this condition, the generator's objective is equal to the original ensemble of KL divergences from Equation 8 , minimizing which solves the initial problem and implies $p^i = p_i^{\\text{real}}$\n\n### 2.3. Unpaired I2I and optimal transport\n\nThe problem of unsuared I2I consists of learning a mapping G between the source distribution $p^S$ and the target distribution $p^T$ given the corresponding independent data sets of samples. When optimized, the mapping should appropriately adapt G(x) to the target distribution $p^T$ , while preserving the input's cross-domain features. However, from the first glance it is unclear what the preservation of crossdomain properties should be like.\n\n$^{2}$The subscript $\\theta$ in $\\psi^{\\theta}$ does not mean introduction the additional normal model of density but is rather used to emphasize its normality.\n\n$^{1}$Note that there is one more summand, which contains the gradient $\\nabla_q \\log p^*$ with respect to the log-density parameters. We do not discuss how to approximate it, because it will be further omitted.\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nOne way to look at that formally is by introducing the notion of \"transportation cost $c(\\cdot ,\\cdot )$ between the generator's input and output and saying that it should not be too large on average. In a practical R2I setting, we can choose $c(\\cdot ,\\cdot )$ as any number above or equal to 100 such that we choose values that we aim to preserve, e.g. pixel-wise distances of differences between LPIPS ( Zhang et al. , 2018 ) embeddings.\n\nMonge optimal transport (OT) problem ( Villani et al. , 2009 ; Santambroio , 2015 ) follows this reasoning and aims at finding the mapping with the least average transport cost among all the mappings that fit the target $p^{T}$ .\n\n$$\\begin{array} { r } { \\operatorname* { I n f } _ { G } \\left\\{ \\mathbb { E } _ { p ^ { - ( x ) } } c ( x , G ( x ) ) \\left| G ( x ) \\sim p ^ { T } \\right. \\right\\} . \\quad ( 1 2 ) } \\end{array}$$\n\nwhich can be seen as a mathematical formalization of the I2I task.\n\nUnder mild constraints, in the case when $p^S$ and $p^T$ have densities, the optimal transport map $G^*$ is bijective, differentiable, has differentiable inverse and thus satisfies the change of variables formula $p^S(x) = p^T(G^*(x)) \\det(\\nabla G^*(x))$ . Therefore, the change of variables condition gives us insight into why it is notoriously challenging to optimize Equation 12 directly.\n\n## 3. Methodology\n\nOur main goal is to adapt the DMD method for the unpaired I21 between an arbitrary source distribution $p^S$ and target distribution $p^T$ .\n\n### 3.1. Regularized Distribution Matching Distillation\n\nFirst, we note that the construction of DMD requires only having samples from the input distribution. Given this, we replace the Gaussian input $p^{\\text{frame}}$ by $p^S$ , the data distribution $p^S_{\\text{data}}$ by $p^I$ and aim at optimizing\n\n$$\\begin{array} { r l } & { \\mathcal { L } ( \\theta ) = \\int _ { 0 } ^ { T } \\omega _ { t } \\,  K  \\big ( p _ { t } ^ { \\theta } \\, \\| \\, p _ { t } ^ { T } \\big ) \\,  d  t = } \\\\ & { = \\int _ { 0 } ^ { T } \\omega _ { t } \\, \\mathbb { E } _ { p ^ { \\theta } ( s ) \\mathcal { N } ( 0 , I ) } \\log \\frac { p _ { t } ^ { \\theta } ( G _ { 0 } ( x ) + \\sigma _ { t } \\varepsilon ) } { p _ { t } ^ { T } ( G _ { 0 } ( x ) + \\sigma _ { t } \\varepsilon ) } \\,  d  t . \\quad ( 1 3 ) } \\end{array}$$\n\nwhere $p_i^u$ and $p_i^T$ are now respectively the distribution of the generator output $G_\\theta(x)$ and the target distribution $p^T$ perturbed by the forward process up to the timestep $t$ .\n\nOptimizing the objective in Equation 13 , one obtains a generator, which takes $x \\sim p^*$ and outputs $\\hat{G}_T(x) \\sim p^*$ , so it performs the desired transfer between the two distributions. However, there are no guarantees that the input and the output will be related. Similarly to the OT problem (Equation 12 ), we fix the issue by penalizing the transport\n\ncost between them. We obtain the following objective\n\n$$\\begin{array} { r } { \\mathcal { L } ( \\theta ) + \\lambda \\, \\mathbb { E } _ { p ^ { - c } ( x ) } c ^ { t } \\left( x , G _ { \\theta } ( x ) \\right) \\to \\underset { \\theta } {  m i n  } , \\quad ( 1 4 ) } \\end{array}$$\n\nwhere $c(\\cdot ,\\cdot )$ is the cost function, which describes the objective properties that we aim to preserve after transfer, and $\\lambda$ is the regularization parameter of the LMP. The appropriate $\\lambda$ , which we will find a balance between between the target distribution and preserving properties of the input.\n\nAs in DMD, we assume that the perturbed target distributions are represented by a pre-trained diffusion model $s^*_t$ and approximate the generator distribution score $s^*_t$ by the additional fake diffusion model $s^*_{\\theta}$ . Analogous to the DMD procedure (Equation 10 ), we perform the coordinate descent in which, however, the generator objective is now regularized. We call the procedure Regularized Distribution Matching Distillation (RMD) . Formally, we optimize\n\n$$\\left\\{ \\begin{array} { l c } { \\displaystyle \\int _ { 0 } ^ { T } \\omega _ { i } \\, \\mathcal { E } _ { x , x } \\log \\frac { p _ { t } ^ { ( i ) } ( G _ { t } ( x ) + \\sigma _ { t } ) } { p _ { t } ^ { ( i ) } ( G _ { t } ( x ) ) + \\sigma _ { t } } \\,  d  t } \\\\ { \\textstyle \\quad + \\lambda \\, \\mathcal { E } _ { x , x } c ( x , G _ { t } ( x ) ) \\to \\operatorname* { m i n } ; } \\\\ { \\displaystyle \\int _ { 0 } ^ { T } \\beta _ { i } \\, \\mathcal { E } _ { x , x } \\| D _ { t } ^ { ( i ) } ( G _ { t } ( x ) + \\sigma _ { t } ) - G _ { t } ( x ) \\| ^ { 2 } \\,  d  t \\to \\operatorname* { m i n } . } \\end{array} \\right.$$\n\nGiven the optimal fake score $s_i^0$ , the generator's objective becomes equal to the desired loss in Equation 14 , which validates the procedure.\n\n### 3.2. Analysis of the method\n\nThe optimization problem in Equation 14 can be seen as the soft-constrained optimal transport, which balances between satisfying the output distribution constraint and preserving the original image properties. Moreover, if one takes $\\lambda \\to 0$ , the objective essentially becomes equivalent to the Monge problem (Equation 12 ). It can be seen by replacing the $\\lambda$ coefficient before the transport cost with the $1/\\lambda$ coefficient before the KL divergence. In this limit, it equals what we have just defined. However, if the two conditions are different, which makes the corresponding problem hardconstrained and, therefore, equivalent to the optimal optimal transport problem. Based on this observation, we prove the following\n\nTheorem 3.1. Let $(\\mathbf{x}, \\mathbf{y})$ be the quadratic cost $\\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ and $G^2$ be the theoretical optimum in the problem [14, 13] , under mild regularity conditions, it converges in probability (with respect to $P$ ) to the optimal transport map $G^*$ , i.e.,\n\n$$G ^ { \\lambda } \\xrightarrow [ \\lambda \\to 0 ] { \\rho ^ { 2 } } G ^ { * } . \\quad ( 1 6 )$$\n\nThe detailed proof can be found in Appendix A . Informally, it means that the optimal transport map can be approximated\n\n4\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\n![Figure](figures/37030_page_005_figure_001.png)\n\nFigure 2. Comparison of the DMD loss surfaces without (left) and with (right) transport cost regularization on a toy problem of translating $(0,0, f)$ to $(0,0, 1.5^2f)$ . We set the regularization coefficient $\\lambda = 0.2$ . The generator is parameterized as: $C'(v)$ , where $C(v)$ is the rotation matrix, corresponding to the angle $\\alpha$ . Minima at the left contains all orthogonal matrices, multiplied by $\\sigma^{=1.5}$ , while the right and the right is retained in the only point, which is close, but not equal, to the OT map. The surfaces are moved for the sake of visualization.\n\nby the RDMD generator, trained on Equation 15 , given a sufficiently small regularization coefficient, enough capacity of the architecture, and convergence of the optimization algorithm.\n\nThis result is important to examine from another angle. It is ideologically similar to the $L_2$ regularization for overparameterized least squares regression. The original least squares, in this case, have a manifold of solutions. At the same time, by adding $L_2$ weight penalty and taking the limit as the regularization coefficient goes to zero, one obtains a solution with the least norm based on the Moore-Penrose pseudo-inverse ( Moore , 1920 ; Penrose , 1955 ) . In our case, numerous maps may be optimal in the original DMD procedure, since it only requires matching the distribution at output. However, taking the limit when $\\lambda \\to 0$ , one obtains a feasible solution with the least transport cost.\n\nWe demonstrate this effect on a toy problem of translating $\\mathcal{N}(0,1)$ to $\\mathcal{N}(0,\\sigma^2I)$ and consider linear generator $G(\\boldsymbol{x}) = Ax$ . The solution to the optimal transport problem with the quadratic cost $c(\\boldsymbol{x},\\boldsymbol{y}) = \\|\\boldsymbol{x}-\\boldsymbol{y}\\|^2$ is $\\bar{A} = \\sigma I$ . For the DMD optimization problem without regularization, minima are obtained at the maxima of orthogonal matrices multipliers $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ . However, if we only take into consideration, the minimum condition is one point at the cost of introducing the bias relatively to the true OT map. We illustrate this by comparing the loss surface with and without regularization in Figure 2 .\n\n## 4. Related work\n\nIn this section, we give an overview of the existing methods for solving unpaired I2I including GANs, diffusion-based methods, and methods based on optimal transport.\n\nGANs were the prevalent paradigm in the unpaired I2I for a long time. Among other methods, CycleGAN (Zhu et al., 2017) and the concurrent DualGAN (Yi et al., 2017) . DiscoGAN (Kim et al., 2017) utilized the cycle-consistency paradigm, consisting in training the transfer network along with its inverse and optimizing the consistency term along with an iterative optimization. On the other hand, there are two-sided methods, including UNIT (Liu et al., 2017) and MUNIT (Huang et al., 2018) that divide the encoding into style-space and content-space and SCAN (Li et al., 2018) that splits the procedure into coarse and fine stages.\n\nThe one-side GAN-based methods aim to train I2I without learning the inverse for better computational efficiency. DistanceGAN ( Bena\u00efm & Wolf , 2017 ) achieves it by learning to preserve the distance between pairs of samples, GCGAN ( Fu et al. , 2019 ) imposes geometrical consistency constraints, and CUT ( Park et al. , 2020 ) uses the contrastive loss to maximize the patch-wise mutual information between input and output.\n\nDiffusion-based V1 models mostly build on modifying the diffusion process using the source image. SDeDiff ( Meng et al. , 2021 ) initializes the reverse diffusion process for target distribution with the noisy source picture instead of the \u539f\u753b, and generates a new image by diffusion ( Jing et al. , & Salimans , 2022 ; Epstein et al. , 2023 ) . The target diffusion process. ILVR ( Cho et al. , 2021 ) adds the correc-\n\n5\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\n![Figure](figures/37030_page_006_figure_001.png)\n\nFigure 3. Visualization of RDMD mappings on Gaussian \u2192 8Gaussians with different choices of the $r$\n\ntion that enforces the current noisy sample to resemble the source. EGSDE ( Zhao et al. , 2022 ) trains a classifier between domains and encourages dissimilarity between the embeddings, corresponding to the source image and the current diffusion process state. At the same time, it enforces a small distance between their downsampled versions, which allows for a balance between faithfulness and realism. The source is re-sampled by sampling natural ones from the diffusion models based on the concatenation of two diffusion models with deterministic sampling ( Su et al. , 2022 ; Wu & De La Torre , 2023 ) .\n\nOptimal transport ( Villani et al. , 2009 ; Peyr\u00e9 et al. , 2019 ) is another useful framework for the unpaired I2I. Methods based on it usually reformulate the OT problem (Equation 12 ) and its modifications as Entropic OT (EOT) ( Cuturi , 2013 ) or Schr\u00f6dinger Bridge (SB) ( F\u00f6llmer , 1988 ) to be accessible in practice. In particular, NOT ( Korotin et al. , 2022 ) , ENOT ( Gushchin et al. , 2024 ) , and NSB ( Kim et al. , 2023a ) use the Lagrangian multipliers formulation of the distribution matching constraint, which results in simulation-based adversarial training. The other methods obtain (partially) simulation-free techniques by iteratively refining the stochastic process between two distributions. In the works ( De Bortoli et al. , 2021 ; Vargas et al. , 2021 ) refinement consists of learning the time-reversal with the corresponding initial distribution (source or target). The newer methods are based on Flow Matching ( Lipman et al. , 2022 ; Tong et al. , 2023 ; Albergo & Vanden-Eijnden , 2022 ) and the corresponding Rectification ( Liu et al. , 2022 ; Shi et al. , 2024 ; Liu et al. , 2023 ) procedure. While being theoretically sound, most of these methods work well for smaller dimensions ( Korotin et al. , 2023 ) but suffer from computationally hard training in large-scale scenarios.\n\n## 5. Experiments\n\nThis section presents the experimental results on 2 upgraded methods, i.e., S-EM and E-EM with respect to increasing the perimeter. In Section 5.2 we compare our method with\n\nthe diffusion-based baselines on the translation problems of NELL and child animals from the AHEAD-2 data set (Choi et al., 2020) .\n\nIn all the experiments, we use the forward diffusion process with variance $\\sigma_t = t$ and $T = 80.0$ as in the paper ( Karras et al. , 2022 ) . We parameterize all the diffusion models with the denoiser networks $D_\\gamma(x)$ , conditioned on the noise level $\\sigma$ , and optimize Equation 6 to train the target diffusion model. As for the RDMD procedure, we optimize Equation 15 , where the gradient with respect to the generator parameters is calculated analogously to Equation 11 . The transport cost $c(x,y)$ is chosen as the squared difference norm $\\|x - y\\|^2$ . The average transport cost, reported in the figures, is calculated as the square root of the MSE between all input and output images for the sake of interpretability.\n\nwe use the same architecture for all networks: target score, fake score, and generator. We utilize the pre-trained target score in two ways. First, we initialize the fake model with its copy. Second, we initialize the generator $G_{\\theta}(x)$ with the same copy $D_{\\theta}^{T}(x)$ , but with a fixed $\\sigma \\in [0, T]$ (since the generator is one-step). The denoiser parameterization is trained to predict the target domain's clean images, therefore, each initial trajectory has significantly different denoise values. These differences make the target domain the information about the target domain more efficiently (Nguyen & Tan, 2023 ; Yin et al., 2023 ) . We explore the initialization of $\\sigma$ for I2I in Appendix B . The additional training details can be found in Appendix C .\n\n### 5.1. Toy Experiment\n\nWe validate the qualitative properties of the RDMD method on 2-dimensional Gaussian $\\to$ 8Gaussians . In this setting, we explore the effect of varying the regularization coefficient $\\lambda$ on the trained transport map $G_{\\theta}$ . In particular, we study its impact on the transport cost and fitness to the target distribution $p^T$ .\n\nIn the experiment, both sources $\\mathcal{N}(0, f)$ and the target mixture of 8 Gaussians are represented with 5000 indepen-\n\n6\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\n![Figure](figures/37030_page_007_figure_001.png)\n\nFigure 4. Comparison of RDMD with diffusion-based baselines. The figure demonstrates the tradeoff between generation quality (FID $\\downarrow$ ) and the difference between the input and output (L2, PSNR, SSIM $\\uparrow$ ). RDMD gives an overall better tradeoff given that it is slightly better than the baseline in both cases. As for the PSNR and SSIM, the $y$ -axis is swapped for the sales of identical readability with the first plot (left is better, low is better).\n\ndent samples. We use the same small MLP-based architecture [ Shi et al. , 2024 ] for all the networks.\n\nThe main results are presented in Figure 3 . The standard $\\Delta M$ ( $\\lambda=0.0$ ) learns a transport map with several intersections when demonstrated as the set of lines between the inputs and the outputs. This observation means that the learned map is not OT, because it is not cycle-monotone ( McCann , 1995 ) . Increasing $\\lambda$ yields fewer intersections, which can be used as a proxy evidence of optimality. At the same time, the generator output distribution becomes farther and farther from the desired target. The results show the importance of choosing the appropriate $\\lambda$ to obtain a better trade-off between the two properties. Here, the regularization coefficient $\\lambda=0.2$ offers a good trade-off by having small intersections and producing output distribution close to the target.\n\n![Figure](figures/37030_page_007_figure_005.png)\n\nFigure 5. Visual comparison of RDMD with diffusion-based baselines.\n\n### 5.2. Cat to Wild\n\nFinally, we compare the proposed RDMD method with the diffusion-based baselines ILVR ( Choi et al. , 2021 ) , SDEdit ( Meng et al. , 2021 ) , and EGSDE ( Zhao et al. , 2022 ) on the $4\\ell \\times 64$ Cat $\\to$ Wild transition problem, based on the AFIHQ-2 ( Choi et al. , 2020 ) data set. Comparison with the diffusion-based models makes the setting fair since it allows to utilize the same pre-trained target diffusion model for training and evaluation. We evaluate our GANbased methods mostly demonstrate results inferior to EGSDE ( Zhao et al. , 2022 ) in terms of FID and PSNR at the same data set with resolution $256\\times 256$ .\n\nWe pre-train the target diffusion model using the EDM ( Karras et al. , 2022 ) architecture with the hyperparameters used by the authors on the AFHQv2 data set. Our pre-trained model achieves FID equal to 2.0. We initialize the model with $\\sigma = 1.0$ based on the observations from Section B and train 5 RDGM generators, corresponding to the regularization coefficients $(0.00, 0.02, 0.05, 0.1, 0.2)$ . We slightly reduce the noise of the DDPM diffusion models by training it with the EDM setting. The ESGDE classifier is trained analogous to the paper, it is initialized from the Dhariwal NLP ( Dhariwal & Nichol , 2021 ) , pre-trained on the Ima-\n\n7\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\ngeNet ( Deng et al. , 2009 ) $64\\times 64$ . For each of the baselines, we run a grid of hyperparameters. The detailed hyperparameter values can be found in Appendix C.3 .\n\nWe report our main quantitative results in Figure 5 and compare the achieved faithfulness-quality trade-off with the baselines. The quality metric is FID, the faithiness metrics are L2/PSNR/SSIM. Among these metrics, L2 is the least convenient for our method. Nevertheless, RDMD achieves a better trade-off given at least moderately strict requirements on the transport cost: all of our models beat all the baselines in the L2 range between $12.5$ and $20.0$ . In all cases, our model achieves strictly higher SSIM and almost strictly higher PSNR. We note, however, that if the lower FID is preferable over the transport cost (L2 values around $22.5-27.5$ ), then it might be better to use one of the baselines. An example of a map with a high OT cost ( $25.0$ ) and low FID ( $5.4$ ) is SDEdit on Figure 5 .\n\nFinally, we present a visual comparison between the meth- ods. To this end, we randomly choose 6 pictures from the test data set and report the corresponding outputs in Figure 5 . Here, we take RDMD with $\\lambda=0.05$ that achieves (FID, L2) equal to $(6.93, 17.86)$ . As for the baselines, we choose the hyperparameters (Appendix C.3 ) with the closest FID to the RDMD: $(8.87, 22.0)$ for ILVR, $(5.4, 25.0)$ for SDEdit, and $(7.02, 22.35)$ for EGSDE .\n\n## 6. Discussion and limitations\n\nIn this paper, we propose RDMD, the novel one-step diffusion-based algorithm for the unpaired I2I task. This algorithm is a modification of the DMD method for diffusion distillation. The main novelty is the introduction of the diffusion distance between the input and the output of the model, which allows to control the trade-off between faithfulness and visual quality.\n\nFrom the theoretical standpoint, we prove that at low regularization coefficients, the theoretical optimum of the introduced objective is close to the optimal transport map ( Thm. 3.1 ). Our experiments in Sec. 5.1 demonstrate how the choice of regularization coefficient affects the trained mapping and allows us to build the general intuition. In Sec. 5.2 we compare our method with the diffusion-based baselines (ILVR, SDEdit, EGSDE) and obtain better results given fair restrictions on the transport cost. The results are strictly better than all of the baselines in terms of SSIM and almost strictly superior to all of the baselines in terms of PSNR.\n\nIn terms of limitations, we admit that our theory works in the asymptotic regime, while one could derive more precise non-limit bounds. Our experimental results on diffusion under the Langevin equation show that the pre-trained diffusion model has 2.01. Improving the vi-\n\nsual quality and testing our method on high dimensions is important for future work. Furthermore, the desired feature of the method would be switching among different reg coefficients without re-training.\n\n## Acknowledgements\n\nThe work of Denis Rakitin was supported by the grant for research centers in the field of AI provided by the Analytical Center for the Government of the Russian Federation (ACRF) in accordance with the agreement on the provision of subsidies (identifier of the agreement: grant number: 18-02-0037-0031-0021) and research with HSE, University No. 70-2021-001-39. This agreement was supported in part through computational resources of HSE cities at HSE University ( Kostenetsky et al. , 2021 ) .\n\n## References\n\nAlbergo, M. S. and Vanden-Eijnden, E. Building normalized and regularized hierarchical interpolants. arXiv preprint arXiv:2209.15371 , 2022.\n\nArjovsky, M., Chintala, S., and Bottou, L. . Wasserstein generation of normally distributed mixture models. In Proceeding of machine learning , pp. 214\u2013223. PMLR, 2018.\n\nBenaim, S. and Wolf, L. One-sided unsupervised domain adaptation for applications in neural information processing systems , 30, 2017.\n\nBogachev, V. I. and Ruas, M. A. S. Measure theory , volume 1, Springer, 2007.\n\nBrenier, Y. Polar factorization and monotone rearrangement of a multivalued function. J. Theor. Comp. Math. 1991, 164: 270-285. doi:10.1016/0022-3714(91)90147-1\n\nChoi, J., Kim, J., Seong, Y., Gwon, Y., and Yoon, S.: IOT devices: An overview, IEEE Internet of Things Journal, vol. 13, pp. models, arXiv preprint arXiv:2108.02498, 2021.\n\nChoi, Y., Uh, Y., Yoo, J., and Ha, J.-W. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8188\u20138197, 2020.\n\nCsizs\u00e1r, I. On information-type measurement of difference of temperature between two refrigerant mixtures. Fundam. Phys. Sci. Math. Hung., 2:299-318, 1967.\n\nCuturi, M.: Sinkhorn distances: Lightspeed comparison and its applications in neural information processing systems , 26, 2013.\n\nDe Bortoli, V., Thornton, J. Heng, J., and Doucet A.: Diffusion schr\u00f6dinger bridge with applications to score-based\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\ngenerative modeling. Advances in Neural Information Processing Systems , 34:17695\u201317709, 2021.\n\nDe Philippis, G. and Figalli, A. The monge-ampere equation and its link to optimal transportation. Bulletin of the American Mathematical Society , 51(4):527\u2013580, 2014.\n\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248\u2013255. Ieee, 2009.\n\nDhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems , 34:8780\u20138794, 2021.\n\nDonsker, M. D. and Varadhan, S. S. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on pure and applied mathematics , 36 (2):183\u2013212, 1983.\n\nDudley, R. M. Real analysis and probability. Chapman and Hall/CRC, 2018.\n\nEpstein, D., Jabri, A., Poole, B., Efros, A., and Holynski, A. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems , 36: 16222\u201316239, 2023.\n\nF\u00f6llmer, H. Random fields and diffusion processes. Lect. Notes Math. 1362:101\u2013204, 1988.\n\nFu, H., Gong, M., Wang, C., Batmanghelich, K., Zhang, K., and Tao, D. Geometry-consistent generative adversarial neural network for object detection based on data mapping. In In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 2427\u20132436, 2019.\n\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems , 27, 2014.\n\nGushchin, N., Kolesov, A., Korotin, A., Vetrov, D. P., and Burnaev, E. Entropic neural optimal transport via diffusion processes. Advances in Neural Information Processing Systems , 36, 2024.\n\nHo, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 , 2022.\n\nHo, J., Jain, A., and Abbeel, P.: Denoising diffusion probabilities for natural images, and information processing systems, 33, 6840\u20136851, 2002.\n\nHuang, X., Liu, M.-Y., Belongie, S., and Kautz, J. Multimodal unsupervised image-to-image translation. In Proceedings of the European conference on computer vision (ECCV) , pp. 172\u2013189, 2018.\n\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-toimage translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 1125\u20131134, 2017.\n\nKantorovich, L. On the translocation of masses. Management science, 5(1):1-4, 1958.\n\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems , 35: 26565\u201326577, 2022.\n\nKim, B., Kwon, G., Kim, C., Ye, J.: Unpaired imagedrift models for large-scale image segmentation. arXiv preprint arXiv:2305.15086 , 2023a.\n\nKim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takeda, Y., Uesaka, T. He., Y. Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279 , 2023b.\n\nKim, T., Cha, M., Kim, H., Lee, J. K., and Kim, J. Learning to discover cross-domain relations with generative adversarial networks. In International conference on machine learning , pp. 1857\u20131865. PMLR, 2017.\n\nKingma, D. P. and Ba, J.: Adam: A method for stochastic optimization, CoRR , abs/1412.6980, 2014. URL: https://api.semanticscholar. org/CorpusID:6628106 .\n\nKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013.\n\nKorotin, A., Selikhovanych, D., and Burnaev, E.: Optical optimal transport, arXiv preprint arXiv:2201.12220, 2022.\n\nKorotin, A., Gushchin, N., and Burnaue, E.: Light schiddering bridge, arXiv preprint arXiv:2310.01774 , 2023.\n\nKostenetskiy, P., Kuchylevich, R., and Kozyrev, V. Hpc resources of the higher school of economics. In Journal of Physics: Conference Series , volume 1740, pp. 012050. IOP Publishing, 2021.\n\nLeCun, Y. The mnist database of handwritten digits. http://yann. lecun. com/edrvlmnist/, 1998.\n\nLi, M., Huang, H., Ma, L., Liu, W., Zhang, T., and Jiang, Y. Unsupervised image-to-image translation with stacked cycle-consistent adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision (ICCV) , pp. 184\u2013199, 2018.\n\nLiero, M., Mielke, A., and Savar \u00e9 , G. Optimal entropytransport problems and a new hellinger-kantorovich distance between positive measures. Inventiones mathematicae , 211(3):969\u20131117, 2018.\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nLipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 , 2022.\n\nLiu, M.-Y., Breuel, T., and Kautz, J. Unsupervised visual characterization of transient attacks in neural information processing systems, 30, 2017.\n\nLiu, X., Gong, C., and Lin, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03005 , 2022.\n\nLiu, X., Zhang, X., Ma, J., Peng, J., et al. Infowall: One step is enough for high-quality diffusion-based text-toimage generation. In The Twelfth International Conference on Learning Representations , 2023.\n\nMcCann, R. J. Existence and uniqueness of monotone measure-preserving maps. 1995.\n\nMeng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073 , 2021.\n\nMetz, L., Poole, B., Plau, D., and Sohl-Dickstein, J. (2015). A convolutional neural network. arXiv preprint arXiv:1511.02163 . 2015.\n\nMoore, E. H. On the reciprocal of the general algebraic coe. Transactions of the american mathematical society, 26:294-295, 1920.\n\nNguyen, T.H. and Tran, A., Swishbsh: One-step test-toquantum transition to a low-cost score distiller, ArXiv preprint arXiv:2312.05239 , 2023.\n\nPark, T., Efros, A. A., Zhang, R., and Zhu, J. Y.: Contrastive learning for unpaired image-to-image translation, In: Computer Vision\u2013ECCV 2020: 16th European Conference on Machine Learning (ECML\u201920), Proceedings, Part IX 16, pp. 319\u2013345, Springer, 2020.\n\nPenrose, R. A generalized inverse for matrices. In Mathematical proceedings of the Cambridge philosophical society, volume 51. pp. 406\u2013413. Cambridge University Press, 1955.\n\nPeyr\u00e9, G., Cuturi, M., et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning , 11(5-6):355\u2013607, 2019.\n\nPoole, B., Jain, A., Barron, T. J., and Mildenhall, B.: Dracunnus australis from birth to early death 3d diffusion, ArXiv preprint arXiv:2209.14988, 2022.\n\nPosner, E. Random coding strategies for minimum entropy information. Transaction on Information Theory, 21(4):387\u2013 391, 1972.\n\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning , pp. 1278\u20131286. PMLR, 2014.\n\nSantambongio, F. Optimal transport for applied mathematicians. Birk\u00e4user, NY , 55(58-63):94, 2015.\n\nSauer, A., Lorenz, D., Blattmann, A., and Romisch, T. (1997). Ethanol distillation. arXiv preprint arXiv:2311.17042 , 2023.\n\nShi, Y., De Bortoli, V., Campbell, A., and Doucet, A. Diffusion-tolerant Population Dynamics Model for Dynamics in Neural Information Processing Systems, 36, 2024.\n\nSong, Y. and Ermon, S. Generation modeling by estimating latent structures from structural data. In Advances in neural information processing systems , 32, 2019.\n\nSong, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 , 2020.\n\nSong, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2305.01469 , 2023.\n\nSu, X., Song, J., Meng, C., and Ermon, S.: Dual diffusion time series detection via language translation, arXiv preprint arXiv:2205.08182 , 2022.\n\nTenenbaum, J. B., Silva, V. D., and Langford, J. C. A. Altered resting-state magnetic activity during visual reduction. science, 290(5550):2319-2323, 2000.\n\nTong, A., Malkin, N., Huguet, G., Zhang, Y., Restor-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.04482 , 2023.\n\nVargas, F., Theodoroff, P., Lamacra, A., and Lawrence, J. (1997). Bayesian knowledge via maximum likelihood. Entropy , 23(9):1134\u20131152.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. V., and Polosukhin, I. Attention is all you need. In Guyon, I., Luburgu, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf .\n\nVillani, C. et al. Optimal transport: old and new , volume 338. Springer, 2009.\n\n10\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nVincent, P. A connection between score matching and denoising autoencoders. Neural Computation , 23:1661\u20131674, 2011. URL https://api. semanticscholar.org/CorpusID:5560641 .\n\nWang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems , 36, 2024.\n\nWu, C. H., and De la Torre, F. A latent space of stochastic diffusion models for zero-shot image editing and guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 7378\u20137387, 2023.\n\nXiao, Z., Kreis, K., and Vahdat, A. Tackling the generative optimization problem with improved diffusion gans. arXiv preprint arXiv:2112.07804 , 2021.\n\nYi, Z., Zhang, H., Tan, P., and Gong, M.: Duallan: Unsupervised dual learning for image-to-image translation, In Proceedings of the IEEE international conference on computer vision , pp. 2849\u20132857, 2017.\n\nYin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828 , 2023.\n\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, N. (2018). Deep residual learning for motion of deep features as a perceptual metric. In CVPR , 2018.\n\nZhao, M., Bao, F., Li, C., and Zhu, J., Eds.: Unpaired image-to-image translation via energy-guided stochastic differential equations. Advances in Neural Information Processing Systems , 35:3609\u20133623, 2022.\n\nZheng, W., Li, Q., Zhang, G., Wan, P., and Wang, Z. 1tr: Unpaired image-to-image translation with transformers. arXiv preprint arXiv:2203.16015 , 2022.\n\nZhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision , pp. 2223\u20132232, 2017.\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\n## A. Theory\n\nIn this section, we aim at proving the main theoretical result of the work: solution of the soft-constrained RDMD objective converts to the solution of the hard-constrained Monge problem. Our proof is largely based on the work ( Lier\u00f2 et al. , 2018 ) . It introduces the family of entropy-transport problems, consisting in optimizing the transport cost with soft constraints based on the divergence between the map's output distribution and the target. There are, however, differences between the problems that prevent us from reducing the functional in Equation 14 for the entropy-transport problems. First, authors of the paper in [ 10 ] showed that by introducing an empirical variational inequality, they could represent the Csisz\u00e1r $f$ -divergences ( Csisz\u00e1r , 1967 ) , used in ( Lier\u00f2 et al. , 2018 ) , seemingly does not contain the integral ensemble of KL divergences, used in Equation 14 . Finally, we illustrate the proof in a simpler particular setting for the narrative purposes. Nevertheless, the used ideas are very similar.\n\n### A.1. Proof outline\n\nWe start by giving a simple outline of the proof. Given a pair of source and target distributions $p^S$ and $p^T$ , RDMD optimizes the following functional with respect to the generator $G$:\n\n$$\\int _ { 0 } ^ { T } \\omega _ { t } \\,  K L  \\left( p _ { t } ^ { i } \\, \\| \\, p _ { t } ^ { f } \\right) \\,  d  t + \\lambda \\, \\mathbb { E } _ { p ^ { x ( t ) } } c \\left(  x  , G (  x  ) \\right) , \\quad ( 1 7 )$$\n\nwhere $p_t^G$ and $p_t^T$ are the generator distribution $p^G$ and the target distribution $p^T$ , perturbed by the forward diffusion process up to the time step $t$ . Our goal is to prove that the optimal generator of the regularized objective converges to the optimal transport map when $\\lambda \\to 0$ . With a slight abuse of notation, in this section we will use a different objective\n\n$$\\begin{array} { r } { \\mathcal { L } ^ { \\alpha } ( G ) = \\alpha \\int _ { 0 } ^ { T } \\omega _ { t } \\mathbb { K L } \\left( p _ { t } ^ { G } \\, \\| \\, p _ { t } ^ { T } \\right) \\,  d  t + \\mathbb { E } _ { p ^ { G } ( x ) } c \\left( x , G ( x ) \\right) } \\end{array} \\quad ( 1 8 )$$\n\nand consider the equivalent limit $\\alpha \\to +\\infty$. We\n\n$$\\begin{array} { r } { \\mathcal { L } ^ { \\infty } ( G ) = \\left\\{ \\begin{array} { l l } { \\mathbb { E } _ { p ^ { \\delta } ( x ) } c \\left( x , G ( x ) \\right) ,  ~ i f ~  ~ p ^ { G } = p ^ { T } ; } \\\\ { + \\infty ,  ~ e l s e ~  } \\end{array} \\right. } \\end{array} \\quad ( 1 9 )$$\n\nto be the objective, corresponding to the unconditional formulation of the Monge problem (Equation 12 ). In this section, we will denote minimum of this objective (which is, therefore, the optimal transport map) as $G^{\\infty, a}$ .\n\nWe first assume that the infimum of the objective $\\mathcal{L}^0$ is reached and define $G^0$ be the optimal generator. We denote by $\\{\\alpha_n\\}_{n=1}^{+\\infty}$ an arbitrary sequence with $\\alpha_n \\to +\\infty$ . We first make two informal assumptions that need to be proved (and will be in some sence further in the section):\n\n- 1. The sequence $G^{n}$ converges (in some sence) to some function $\\hat{G}$;\n2. $\\mathcal{L}^{n}$ is continuous with respect to this convergence, i.e. for every convergent sequence $G_{n} \\to G$ holds $\\mathcal{L}^{n}(G_{n}) \\to \\mathcal{L}^{n}(\\hat{G})$.\nGiven this, we first observe that for each map ( ) the sequence of objectives $\\mathcal{L}^{(m)}$ ( ) monotonically converges to the objective $\\mathcal{L}^*(\\zeta)$ . It follows from the fact that the first summand of $\\mathcal{L}^{(m)}$ converges to $+\\infty$ if and only if the KL divergence is non-zero, which is equivalent to saying that $p^{(1)}$ and $p^*$ differ ( Wang et al. , 2024 ) . If instead $p^{(1)} = p^*$ , the summand zeroes out. This also means that the minimal values of the corresponding objectives form a monotonic sequence:\n\n$$\\mathcal{L}^{\\alpha_{\\kappa}}(G^{\\alpha_{0}})\\leq \\mathcal{L}^{\\alpha_{\\kappa+1}}(G^{\\alpha_{0}+1})\\leq \\mathcal{L}^{\\infty}(G^{\\infty}).\\quad (20)$$\n\nFinally, the monotonicity implies that for a fixed m\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n } } ( G ^ { \\alpha _ { n } } ) \\geq \\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n } } ( G ^ { \\alpha _ { n } } ) , \\quad ( 2 1 ) } \\end{array}$$\n\n$^{4}$ Solution to the Monge problem is not always unique, but we will further impose assumptions that will guarantee the uniqueness.\n\n12\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nsince the input $G^{\\alpha_n}$ is fixed and $\\mathcal{L}^{\\alpha_n}$ monotonically increases. Using the assumed continuity of the objective, we obtain\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n } } ( G ^ { \\alpha _ { n } } ) \\geq \\mathcal { L } ^ { \\alpha _ { n } } ( \\overline { G } ) \\quad ( 2 2 ) } \\end{array}$$\n\nfor each $m$. Taking the limit $m \\to \\infty$, we obtain\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty } G ^ { \\Omega _ { n } } ( G ^ { \\Omega _ { n } } ) \\geq L ^ { \\infty } ( \\tilde { G } ) . \\quad ( 2 3 ) } \\end{array}$$\n\nCombining this set of equations, we obtain:\n\n$$\\mathcal { L } ^ { \\infty } ( G ^ { n } ) \\geq \\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { L } ^ { \\infty } ( G ^ { n } ) \\geq \\mathcal { L } ^ { \\infty } ( \\hat { G } ) \\geq \\mathcal { L } ^ { \\infty } ( G ^ { \\infty } ) , \\quad   ( 2 4 )$$\n\nwhere the first inequality comes from the monotonicity of the minimal values and the last inequality uses that $\\mathcal{G}^\\infty$ is the minimum of the objective $\\mathcal{L}^\\infty$ . Hence, that limiting map $\\tilde{G}$ achieves minimal value of the objective $\\mathcal{L}^\\infty$ and is, therefore, the optimal transport map.\n\nAt this point, we only need to define and prove some versions of the aforementioned facts:\n\n- 1. Infinimum of $L^0$ is reached;\n2. The sequence of minima $G^{s_0}$ converges;\n3. $L^0$ is continuous with respect to this converg\nFrom now on, we formulate the result in details and stick to the formal proof.\n\n### A.2. Assumptions and theorem statement\n\nFirst, we list the assumptions.\n\nAssumption A1. The distributions $p^S$ and $p^T$ have densities with respect to the Lebesgue measure. The distributions are defined on open bounded subsets $\\mathcal{X} \\subset \\mathbb{R}^d$ and $\\mathcal{Y} \\subset \\mathbb{R}^d$ , where $\\mathcal{Y}$ is convex. The densities are bounded away from zero and infinity on $\\mathcal{X}$ and $\\mathcal{Y}$ , respectively.\n\nWe admit that boundedness of the support is a very restrictive assumption from the theoretical standpoint, however in our applications (21) both source and target distributions are supported on the bounded space of images. We thus can set $\\mathcal{X} = \\mathcal{Y} = (0,1)^d$ .\n\nAssumption A.2. The cost $c(\\mathbf {x}, \\mathbf {y})$ is quadratic $\\|\\mathbf {x} - \\mathbf {y}\\|^2$ .\n\nHere, we stick to proving the theory only for $L_2$ cost due to difficulties in investigation of Monge map existence and regularity for general transport costs ( De Philippis & Figalli , 2014 ) .\n\nAssumption A.3. The weighting function $\\omega_t$ is positive and bounded.\n\nAssumption A.4. Standard deviation $\\sigma_t$ of the noise, defined by the forward process, is continuous in $t$ .\n\nTheorem A10. Let $\\eta^S, \\gamma^S$ , $c, \\omega_1$ , and $\\sigma_1$ satisfy the assumptions 1 - 3 . Then, there exists a minimum $G^\\alpha$ of the objective $\\mathcal{L}^\\alpha$ from the Equation 18 . If $\\alpha_n \\to \\infty$ , the sequence $G^{n\\alpha}$ converges in probability (with respect to the source distribution) to the optimal transport map $G^\\infty$ .\n\n$$G ^ { \\alpha } \\to \\frac { g ^ { \\beta } } { n \\to \\infty } G ^ { \\beta } . \\quad ( 2 5 )$$\n\n### A.3. Theoretical background\n\nWe start by listing all the results necessary for the proof. They are mostly related to the topics of measure theory (weak measures, Borel measures, and so on). However, some of the important facts can be found in the books (Bogacz & Riesz, 2007; Dudley, 2018) . Otherwise, we make the corresponding citations.\n\nDefinition A.2. A sequence of probability distributions $p^n(x)$ converges weakly to the distribution $p(x)$ if for all continuous bounded test functions $\\varphi \\in C_b(\\mathbb{R}^d)$ holds\n\n$$ E  _ { p ( x ) } \\varphi ( x ) \\xrightarrow { n \\to \\infty }  E  _ { p ( x ) } \\varphi ( x ) . \\quad   ( 2 6 )$$\n\nNotation: $p^n \\overset{w}{\\to} p$ .\n\n13\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nDefinition A.3. A function $f: \\mathbb{R}^d \\to \\mathbb{R}$ is called lower semi-continuous (lsc), if for all $x_n \\to x$ holds\n\n$$\\begin{array} { r } { \\operatorname* { l i m i n } _ { n \\to \\infty } f ( x _ { n } ) \\geq f ( x ) . \\tag { 2 7 } } \\end{array}$$\n\nTheorem A.4 (Portmanteau/Alexandrov). $p^{n} \\stackrel{w}{\\to} p$ is equivalent to the following statement: for every lsc function f, bounded from below; holds\n\n$$\\begin{array} { r } { \\operatorname* { l i m i n } _ { n \\to \\infty } \\mathbb { E } _ { p ^ { n } ( x ) } f ( x ) \\geq \\mathbb { E } _ { p ( x ) } f ( x ) . \\quad ( 2 8 ) } \\end{array}$$\n\nDefinition A.5. A sequence of probability measures $p^n$ is called relatively compact, if for every subsequence $p^{n_k}$ there exists a weakly convergent subsequence $p^{n_{k+1}}$ .\n\nDefinition A.6. A sequence of probability measures $\\rho^n$ is called tight, if for every $\\varepsilon > 0$ there exists a compact set $K_c$ such that $\\rho^n(K_c) \\geq 1 - \\varepsilon$ for all $n$.\n\nTheorem A.7 (Brokhunov). A sequence of probability measures $\\mu ^{n}$ is relatively compact if and only if it is tight. In particular, every weakly convergent sequence is tight.\n\nCorollary A.8. If there exists a function $\\varphi(x)$ such that its sublevels $\\{x: \\varphi(x) \\leq  r\\}$ are compact and for all $n$\n\n$$ E  _ { p ^ { n } ( x ) } \\varphi ( x ) \\leq  C$$\n\nholds with some constant $C$, then $p^{n}$ is tight (i.e. at least it has a weakly convergent subsequence).\n\nCorollary A.9. If a sequence $p^{n}$ is tight and all of its weakly convergent subsequences converge to the same measure $p$, then $p^{n}\\to p$.\n\nDefinition A.10. The functional $\\mathcal{L}(p)$ is called lower semi-continuous (lsc) with respect to the weak convergence if for all weakly convergent sequences $p^{n} \\stackrel{\\text{a.p.}}{\\to} p$ holds\n\n$$\\begin{array} { r } { \\operatorname* { l i m i n } _ { n \\to \\infty } \\mathcal { L } ( p ^ { n } ) \\geq \\mathcal { L } ( p ) . \\tag { 2 9 } } \\end{array}$$\n\nTheorem A.1 (Posner 1975). The KL divergence $KL(p\\parallel q)$ is lsc (in sense of weak convergence) with respect to each argument, i.e. if $p\\stackrel{1}{\\to}q$ and $q_{n}\\stackrel{1}{\\to}q$, then\n\n$$\\operatorname* { l i m i n } _ { n \\to \\infty }  K L  ( p ^ { n } \\| q ) \\geq  K L  ( p \\| q ) \\quad ( 3 0 )$$\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty }  K L  ( p | q _ { n } ) \\geq  K L  ( p | q ) . \\tag { 3 1 } } \\end{array}$$\n\nTheorem A.12 (Donsker & Varadhan 1983) . The KL divergence can be expressed as\n\n$$ K L ( p | q ) = \\operatorname* { s u p } _ { g } \\left( E _ { p ( a ) } g ( x ) - \\log _ { E _ { q } ( x ) } e ^ { v ( g ) x } \\right) . \\quad   ( 3 2 )$$\n\nDefinition A.13. The expression\n\n$$\\mathbb { E } _ { p ( x ) \\in ^ { t } ( s , x ) } \\quad ( 3 3 )$$\n\nis called the characteristic function (Fourier transform) of the distribution $p(x)$.\n\nTheorem A.14 (L\u00fdvy) . Weak convergence of probability measures $p^{w}$ . w.p. p is equivalent to the point-wise convergence of characteristic functions, i.e. $\\overline{\\mathbb{E}}_{p^{w}(x)^{t\\in \\{0,n\\}}}\\to \\overline{\\mathbb{E}}_{p^{w}(x)}$ for all $s$.\n\nDefinition A.15. A sequence of measurable functions $\\varepsilon^n(x)$ is said to converge in measure (in probability) to the function $\\varphi$ with respect to the measure $p(x)$, if for all $\\varepsilon > 0$ holds\n\n$$p\\left\\{x:\\left|\\varphi^{n}(x)-\\varphi(x)\\right|>\\varepsilon\\right\\}\\to0.$$\n\nTheorem A.16 (Lebesgue). Let $\\varphi^*$, $\\varphi$ be measurable functions such that $\\|\\varphi^*(x)\\|, \\|\\varphi(x)\\| \\leq  C$ and $\\varphi^*(x) \\to \\varphi(x)$ as pointwise. Then $\\overline{\\int_{\\mathbb{R}^d}} \\varphi^*(x) \\varphi(x) \\to \\mathbb{E}_{p(x)} \\varphi(x)$ .\n\nLemma A.17 (Fatou) . For any sequence of measurable functions $\\varphi^n$ the function $\\liminf_n \\varphi^n$ is measurable and\n\n$$(34)\\quad \\int\\limits_{0}^{b}\\liminf _{n\\to \\infty}\\varphi ^{n}(x)dx\\leq  \\liminf _{n\\to \\infty}\\int\\limits_{0}^{b}\\varphi ^{n}(x)dx.$$\n\n14\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nTheorem A.18 (Brenier 1991) . Given the Assumption A.1 , there exists a unique optimal transport map that solves the Monge problem 12 for the quadratic cost.\n\nProof. This result can be found e.g. in ( De Philippis & Figalli, 2014, Theorem 3.1 ) .\n\nTheorem A.19. Given the Assumption A.1 , the unique OT Monge map is continuous.\n\nProof. This is a simplified version of (De Philippis & Figalli, 2014, Theorem 3.3).\n\n### A.4. Lower semi-continuity of the loss\n\nHaving defined all the needed terms and results, we start the proof by re-defining the objective in Equation 18 with respect to the joint distribution $\\pi$ input and output of the generator instead of the generator $G$ itself. Analogous to the Kantorovich formulation of the optimal transport problem ( Kantorovich , 1958 ) , for each measure $\\pi$ on $\\mathbb{R}^d \\times \\mathbb{R}^d$ (which is also called a transport plan or just plan) we define the corresponding functional as\n\n$$\\begin{array} { r } { \\mathcal { L } ^ { \\alpha } (  x  ) = \\alpha \\int _ { 0 } ^ { T } \\omega _ { t } ~  K L  \\left(  \\pi  _ { t } |  y  , t \\, \\| \\, p _ { t } ^ { T } \\right) \\, d t + \\mathbb { E } _ {  \\pi  (  x  ,  y  ) } r \\left(  x  ,  y  \\right) , } \\end{array} \\quad ( 3 5 )$$\n\nwhere $\\pi_x$ and $\\pi_y$ are the corresponding projections (marginal distributions) of $\\pi$ and $\\pi_{xy}$ is the perturbed $y$ -marginal distribution of $\\pi$. Note that for $\\pi$, corresponding to the joint distribution of $(x, G(\\boldsymbol{x}))$, $\\mathcal{L}^{\\pi}(\\pi)$ coincides with $\\mathcal{L}^{y}(\\mathcal{G})$, defined in Equation 18. Thus, we aim to optimize $\\mathcal{L}^{y}(\\pi)$ with respect to such plans $\\pi$, that their $\\boldsymbol{x}$ marginal is equal to $p^y$ and $\\pi^y(\\boldsymbol{y}=G(\\boldsymbol{x}))=1$ for some $G$.\n\nDefinition A.20. We will call a measure $\\pi$ generator-based if its $x$ -marginal is equal to $p^x$ and $\\pi(y = G(x))$ for some function $G$ .\n\nFor the sake of clarity, we now that the distributions $x^\\eta$ and $p'_\\eta$ can be represented as $x^\\eta * q_i$ and $p^\\eta * q_i$, where $*$ is the convolution operation and $q_i = \\mathcal{N}(0, \\sigma_i^2 I)$ . We thus rewrite the functional as\n\n$$\\mathcal { L } ^ { n } ( \\pi ) = \\alpha \\int _ { 0 } ^ { T } \\omega _ { \\zeta } \\,  K L  \\left( \\pi _ { y } * q _ { t } \\parallel p ^ { T } * q _ { t } \\right) \\,  d  t + \\mathbb { E } _ { x ( : y ) } e \\left( x , y \\right) , \\quad   ( 3 6 )$$\n\nPreviously, we wanted to establish continuity of the objective. This may not be the case in general. Instead, we prove the following\n\nLemma A.21. $\\mathcal{L}^{\\omega}(\\pi)$ is lsc with respect to the weak convergence, i.e. for all weakly convergent sequences $\\pi^n\\xrightarrow{w} \\pi$ holds\n\n$$\\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { L } ^ { \\alpha } ( \\pi ^ { n } ) \\geq \\mathcal { L } ^ { \\alpha } ( \\pi ) . \\quad   ( 3 7 )$$\n\nThis result is a direct consequence of the Theorem A.11 about lower semi-continuity of the KL divergence.\n\nProof. We start by proving that the projection and the convolution operation preserve weak convergence. For the first, we need to prove that for any test function $g \\in \\mathcal{C}_0(\\mathbb{R}^d)$ holds\n\n$$\\mathbb { E } _ { x _ { y } ( y ) } g ( y ) \\to \\mathbb { E } _ { x _ { y } ( y ) } g ( y ) \\quad ( 3 8 )$$\n\ngiven $\\pi^0 \\xrightarrow{w} \\pi$. For this, we note that the function $\\varphi(\\boldsymbol{x}, \\boldsymbol{y}) = g(\\boldsymbol{y})$ is also bounded and continuous and, thus\n\n$$\\begin{array} { r } { \\mathbb { E } _ { \\pi _ { y } ( y ) } g ( y ) = \\mathbb { E } _ { \\pi ^ { n } ( x , y ) } \\varphi ( x , y ) \\to \\mathbb { E } _ { \\pi ( x , y ) } \\varphi ( x , y ) = \\mathbb { E } _ { \\pi _ { y } ( y ) } g ( y ) . \\quad ( 3 9 ) } \\end{array}$$\n\nRegarding the convolution, recall that $\\sigma_{ij}^2 \\cdot  e_j$ is the distribution of the sum of independent variables with corresponding distributions. Its characteristic function is equal to\n\n$$\\begin{array} { r } { \\mathbb { E } _ { \\pi _ { y } ^ { t } \\sim q _ { t } ( y _ { t } ) } e ^ { t ( s _ { t } ) } ( y _ { t } ) = \\mathbb { E } _ { \\pi _ { y } ^ { t } ( y ) } q _ { t } ( x _ { t } ) e ^ { t ( s _ { t } ) } ( y _ { t + \\varepsilon } ) = \\mathbb { E } _ { \\pi _ { y } ^ { t } ( y ) } e ^ { t ( s ) } ( y ) \\, q _ { t ( x _ { t } ) } e ^ { t ( s _ { t } \\varepsilon ) } . } \\end{array} \\quad ( 4 0 )$$\n\n15\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nApplying the L\u00e9vy's continuity theorem to $\\pi_{y}^{n} \\stackrel{w}{\\to} \\pi_{y}$, we take the limit and obtain\n\n$$\\mathbb { E } _ { \\pi _ { y } ( y ) } e ^ { \\ell ( s , y ) } \\mathbb { E } _ { q _ { t } ( z _ { t } ) } e ^ { \\ell ( s , z _ { t } ) } = \\mathbb { E } _ { \\pi _ { y } ( y ) } q _ { t } ( z _ { t } ) e ^ { \\ell ( s , y + z _ { t } ) } = \\mathbb { E } _ { \\pi _ { y } \\star q _ { t } ( y _ { t } ) } e ^ { \\ell ( s , y _ { t } ) } , \\quad ( 4 1 )$$\n\nwhich implies\n\n$$E_{\\pi_{Y}^{*}, q_{1}(y_{i})} e^{i\\left\\langle  s, y_{i}\\right\\rangle} \\to E_{\\pi_{Y}^{*}, q_{1}(y_{i})} e^{i\\left\\langle  s, y_{i}\\right\\rangle} .\\quad (42)$$\n\nWe apply the continuity theorem for the convolutions and obtain $\\pi_{q}^{n} * q_{t} \\xrightarrow{w} \\pi_{y} * q_{t}$ .\n\nWith this observation, we prove that the first term of $\\mathcal{L}^{\\pi}(\\pi)$ is lsc. First, we apply Lemma A.17 (Fatou) and move the limit inside the integral\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty } \\int _ { 0 } ^ { T } \\omega _ { t }  K L  \\left( \\pi _ { y } ^ { n } * q _ { t } \\mid p ^ { T } * q _ { t } \\right) d t \\geq \\int _ { 0 } ^ { T } \\operatorname* { l i m } _ { n \\to \\infty }  K L  \\left( \\pi _ { y } ^ { n } * q _ { t } \\mid p ^ { T } * q _ { t } \\right) d t . } \\end{array} \\quad ( 4 3 )$$\n\nUsing the lower semi-continuity of the KL divergence (Theorem A.11), we obtain\n\n$$\\int _ { 0 } ^ { T } \\operatorname* { l i m } _ { n \\to \\infty }  K L  \\left( \\pi _ { y } ^ { n } * q _ { t } \\, \\| \\, p ^ { T } * q _ { t } \\right) \\mathrm d t \\geq \\int _ { 0 } ^ { T }  K L  \\left( \\pi _ { y } * q _ { t } \\, \\| \\, p ^ { T } * q _ { t } \\right) \\mathrm d t . \\quad   ( 4 4 )$$\n\nFinally, the Assumption A.2 on the continuity of $c(\\cdot ,\\cdot )$ implies its lower semi-continuity. Theorem A.4 (Portmanteau) states that\n\n$$\\begin{array} { r } { \\operatorname* { l i m i n } _ { n \\to \\infty } \\mathbb { E } _ { \\pi ^ { n } ( x , y ) } c ( x , y ) \\geq \\mathbb { E } _ { \\pi ( x , y ) } c ( x , y ) . \\quad ( 4 5 ) } \\end{array}$$\n\nCombining inequalities from Equation 43 , Equation 44 and Equation 45 , we obtain\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { n \\to \\infty } \\mathcal { L } ^ { \\alpha } ( \\pi ^ { n } ) \\geq \\mathcal { L } ^ { \\alpha } ( \\pi ) . } \\end{array} \\quad ( 4 6 )$$\n\n### A.5. Existence of the minimizer\n\nNow we aim to prove that the objective $\\mathcal{L}^o(\\pi)$ has a minimum over generator-based plans. First, we need the following technical lemma about sublevels of the KL part of the functional.\n\nLemma A.22. Let $\\{\\pi^n\\}_{n=1}^\\infty$ be a sequence of generator-based plans that satisfy\n\n$$\\int _ { 0 } ^ { T } \\omega _ { t }  K L  \\left( \\pi _ {  y  } ^ { n } \\| p _ { t } ^ { T } \\right) d t \\leq  C \\quad  ( 4 7 )$$\n\nfor some constant $C$ . Then, the sequence $\\{\\frac{1}{n}\\}^{\\infty}_{n=1}$ is tight.\n\nProof. We take arbitrary $\\pi$ from the sequence and apply the Donsker-Varadhan representation (Theorem divergence). We take the test function $g(\\pi) = \\|g(\\pi)\\|_2^2/(2\\pi)^2$ and obtain\n\n$$\\begin{array} { r } { \\int _ { 0 } ^ { T } \\omega _ { k }  K L  \\left( \\pi _ { y , t } \\| p _ { t } ^ { T } \\right)  d  t = \\int _ { 0 } ^ { T } \\omega _ { k } \\left( \\mathbb { E } _ { \\pi _ { y , t } ( y _ { t } ) } \\frac { 1 } { 2 \\sigma _ { y } ^ { 2 } } \\| y _ { t } \\| ^ { 2 } - \\log \\mathbb { E } _ { p _ { t } ^ { T } ( y _ { t } ) } e ^ { \\| y _ { t } \\| / ( 2 \\sigma _ { y } ^ { 2 } ) } \\right)  d  t . } \\end{array} \\quad ( 4 8 )$$\n\nThe choice of $g(a)$ is not very specific, i.e. every function that will produce finite expectations and integrals is suitable. In the right-hand side, we rewrite the expectations with respect to the original variable and noise:\n\n$$\\begin{array} { r } { \\int _ { 0 } ^ { T } \\int _ { 0 } ^ { T } \\mathbb { E } _ { x _ { y } ( \\mathbf { y } ) \\mathcal { N } ( 0 , I ) } \\frac { 1 } { 2 \\sigma _ { T } ^ { 2 } } \\| \\mathbf { y } + \\sigma _ { t } \\varepsilon \\| ^ { 2 } - \\log \\mathbb { E } _ { \\rho ^ { \\tau } ( \\mathbf { y } ) \\mathcal { N } ( 0 , I ) } e ^ { \\| \\mathbf { y } + \\sigma _ { t } \\varepsilon \\| ^ { 2 } / ( 2 \\sigma _ { t } ^ { 2 } ) } \\right)  d  \\mathbf { t } . } \\end{array} \\quad ( 4 9 )$$\n\n16\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nWe rewrite $\\|\\gamma + \\sigma \\epsilon\\|^2$ as $\\|y\\|^2 + 2\\sigma \\langle  y, (\\sigma_t \\epsilon) + \\sigma_t^2\\epsilon \\|^2$ and note that expectation of the second term is zero. The first term is then equal to\n\n$$\\begin{array} { r } { \\frac { 1 } { 2 \\pi \\sigma _ { x } ^ { 2 } } \\int _ { \\omega _ { x } } \\omega _ { x } \\,  d  \\mathbf r \\cdot  \\mathbb { E } _ { x _ { 0 } } ( \\omega ) \\| \\mathbf B \\| ^ { 2 } + \\frac { 1 } { 2 \\pi \\sigma _ { x } ^ { 2 } } \\int _ { \\Omega _ { x } } \\omega _ { x } ^ { 2 }  d  \\mathbf r \\cdot  \\mathbb { E } _ { x } \\chi _ { x } ( 0 , t _ { 0 } ) \\| \\mathbf v \\| ^ { 2 } . } \\end{array} \\quad ( 5 0 )$$\n\nBoundedness of $\\omega_t$ (Assumption A.3 ) implies that the first integral is finite and, say, equal to $C_1$ . The second integral contains a product of bounded $\\omega_t$ and continuous $\\sigma_t^2$ (Assumption A.4 ), which is also integrable. We then denote the second summand by $C_2$ and rewrite the first summand as\n\n$$\\begin{array} { r } { C _ { 1 } \\mathbb { E } _ { \\pi _ { y } ( y ) } \\| y \\| ^ { 2 } + C _ { 2 } . \\quad ( 5 1 ) } \\end{array}$$\n\nAs for the second summand, we see that the expectation\n\n$$\\begin{array} { r l } & { \\mathbb { E } _ { p ^ { \\prime } \\tau (  y  ) \\mathcal { N } ( \\varepsilon | 0 , 1 ) } e ^ { |  y  +  \\sigma  \\varepsilon | ^ { 2 } / ( 2 \\sigma _ { \\tau } ^ { 2 } ) } \\quad ( 5 2 ) } \\end{array}$$\n\nwith respect to $\\varepsilon$ will be finite, because $\\sigma_f^2/(2\\pi^2)$ is always less than $1/2$, which will make the exponent have negative degree. Moreover, simple calculations show that this function will be continuous with respect to $\\sigma_f$ and have only quadratic terms with respect to $y$ inside the exponent, i.e. have the form\n\n$$e ^ { a ( \\sigma _ { t } ) } \\| y - b ( \\sigma _ { t } ) \\| ^ { 2 } + c ( \\sigma _ { t } ) \\quad ( 5 3 )$$\n\nwith continuous $a, b, c$. We now want to prove that the expectation\n\n$$\\begin{array} { r } { \\mathbb { E } _ { p ^ { T } ( y ) } e ^ { \\alpha ( \\sigma _ { t } ) \\| y - \\beta ( \\sigma _ { t } ) \\| ^ { 2 } + \\gamma ( \\sigma _ { t } ) } ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~$$\n\nwill also be continuous in $t$. First, due to the boundedness of $\\boldsymbol{y}$, this expectation is finite. Second, for $t_n \\to t$:\n\n$$\\begin{array} { r l } & { \\underset { n \\to \\infty } { \\operatorname* { l i m } } \\, _ { t \\to \\infty } \\mathbb { E } _ { p ^ { \\prime } ( t ) } e ^ { i \\left( \\langle  \\sigma _ { 1 } \\rangle _ { n } \\right) t } \\| y - \\langle  \\sigma _ { 1 } \\rangle _ { n } \\| ^ { 2 } + \\langle  \\sigma _ { 1 } \\rangle _ { n } \\| ^ { 4 } = } \\\\ & { \\quad = \\underset { n \\to \\infty } { \\operatorname* { \\mathop { \\mathbb { E } } } _ { p ^ { \\prime } ( t ) } } \\underset { t \\to \\infty } { \\mathbb { E } } e ^ { i \\langle  \\sigma _ { 1 } \\rangle _ { n } } \\| y - \\langle  \\sigma _ { 1 } \\rangle _ { n } \\| ^ { 2 } + \\langle  \\sigma _ { 1 } \\rangle _ { n } \\| ^ { 4 } = } \\\\ & { \\quad = \\underset { n \\to \\infty } { \\operatorname* { \\mathbb { E } } } e ^ { i \\langle  \\sigma _ { 1 } \\rangle _ { n } } \\| y - \\langle  \\sigma _ { 1 } \\rangle \\| ^ { 2 } + \\langle  \\sigma _ { 1 } \\rangle ^ { 4 } . } \\end{array}$$\n\ndue to the Theorem A.16 (Lebesgue's dominated convergence). It is applicable, since $y$ is bounded and all the functions are continuous, thus bounded in $[0, T]$.\n\nWe thus obtain that the second integral contains bounded $\\omega_2$ multiplied by the logarithm of continuous function, which is always $\\geq 1$ (positive exponent). This means that the whole integral is finite. Denoting it by $C_3$ , we obtain\n\n$$C _ { 1 } \\mathbb { E } _ { \\pi _ { u } ( y ) } \\| y \\| ^ { 2 } + C _ { 2 } - C _ { 3 } \\leq  \\int _ { 0 } ^ { T } \\omega _ { t }  K L  \\left( \\pi _ { y , t } \\| \\rho _ { t } ^ { \\prime } \\right) d t .\\quad (58)$$\n\nCombined with the condition of the lemma, we obtain\n\n$$C _ { 1 } \\mathbb { E } _ { \\pi _ { y | t } | | y | ^ { 2 } } + C _ { 2 } - C _ { 3 } \\leq  \\int _ { 0 } ^ { T } \\omega _ { t }  K L  \\left( \\pi _ { y , t } | | p _ { t } ^ { T } \\right)  d  t \\leq  C , \\quad   ( 5 9 )$$\n\nwhich implies\n\n$$\\begin{array} { r } { \\mathbb { E } _ { x _ { y } | y } \\| y \\| ^ { 2 } \\leq  \\frac { C + C _ { 3 } - C _ { 2 } } { C _ { 1 } } : = C _ { 4 } . \\quad ( 6 0 ) } \\end{array}$$\n\nWe thus obtained a uniform bound on some statistic with respect to all measures from $\\{\\pi^n\\}$. The function $|y|^2$ has compact sublevel sets $\\{|y|^2 \\le r\\}$. Lemma A.8 then states that the sequence $\\pi_y^n$ is tight, i.e. for all $\\varepsilon > 0$ there is a compact set $K_\\varepsilon$ with $\\pi_y^n(y_K) \\ge 1 - \\varepsilon$.\n\nFinally, marginal $\\sigma$ distribution of each of the $\\pi^i$ is $p^{\\tilde{\\sigma}^i}$, which is bounded (Assumption A.1), i.e. there is a compact $K$ that $\\sigma(\\pi \\in K) = 1$. Combined with the previous observation, we obtain\n\n$$\\pi^{n}\\left(x \\in K, y \\in K_{\\varepsilon}\\right) \\geq 1-\\varepsilon\\quad (61)$$\n\nfor all n. The cartesian product $K\\times K_r$ is also compact. Theorem A.7 (Prokhorov) then implies that the sequence $\\pi^n$ is all.\n\n17\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nNow we are ready to prove the following\n\nLemma A.23. Ifinimum of the loss $\\mathcal{L}^{\\pi}(\\pi)$ over all generator-based transport plans $\\pi$ (with $\\pi_{\\pi} = p^{\\delta}$ and $\\pi(y = G(x))$ for some $G$ ) is attained on some plan $\\hat{\\pi}$ .\n\nProof. We start by observing that there is at least one feasible $w$ with the aforementioned properties. For this purpose one can take the optimal transport $G^*$ between $p^*$ and $p^T$ , which is unique by Theorem A.18 under Assumptions A.1 , A.2 , and A.3 .\n\nLet $\\pi^v$ be a sequence of feasible generator-based measures that $L^v(\\pi^v)$ converges to the corresponding infimum $L_{inf}^v$ (it exists by the definition of the infimum). Without loss of generality, we can assume that $L^v(\\pi^v) \\leq  L_{inf}^v + 1$ for all $v$ (if not, one can drop large enough sequence prefix). This implies that for all $v$ holds\n\n$$\\begin{array} { r } { \\alpha \\int \\limits _ { 0 } ^ { T } \\omega _ { t }  K L  \\left( \\pi _ { y , t } \\| p _ { t } ^ { T } \\right)  d  t \\leq  \\mathcal { L } _ {  i n f  } ^ { \\alpha } + 1 . } \\end{array} \\quad ( 6 2 )$$\n\nLemma A.22 implies that the sequence $\\pi^n$ is tight. Prochonov theorem then states that $\\pi^n$ has a weakly convergent subsequence $\\pi^{n+1} \\to \\pi$. Lower semi-continuity of the loss $\\mathcal{L}(\\pi^n)$ implies that\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { k \\to \\infty } \\mathcal { L } ^ { \\alpha } ( \\pi ^ { n _ { k } } ) \\geq \\mathcal { L } ^ { \\alpha } ( \\hat { \\pi } ) \\geq \\mathcal { L } _ {  i n i .  } ^ { \\alpha } . \\quad ( 6 3 ) } \\end{array}$$\n\nAt the same time, $\\mathcal{L}^{\\alpha}(\\pi^{n_{\\lambda}})$ is assumed to converge to $\\mathcal{L}_{inf}^{\\alpha}$, which means that $\\hat{\\pi}$ is indeed the minimum.\n\n### A.6. Finish of the proof\n\nTheorem A.1. proof. Finally, we combine the previous technical observations with the proof sketch from the Section A.1. Let $\\alpha_n \\to \\infty$ be a sequence of coefficients, $G^{\\alpha_n}$ be the optimal generators with respect to $\\mathcal{L}^{\\alpha_n}$ and $\\pi^{\\alpha_n}$ the joint distributions of $(x, G^{\\alpha_n}(x))$. Additionally, we define $\\pi^\\infty$ to be the optimal transport plan, corresponding to $(x, G^{\\infty}(x))$, where $G^{\\infty}(x)$ is the optimal transport map. First, due to the monotonicity of $\\mathcal{L}^{\\infty}$ with respect to $\\alpha$, we have\n\n$$\\mathcal { L } ^ { \\alpha _ { a } } ( \\pi ^ { \\alpha _ { a } } ) \\leq  \\mathcal { L } ^ { \\alpha _ { a + 1 } } ( \\pi ^ { \\alpha _ { a + 1 } } ) \\leq  \\mathcal { L } ^ { \\infty } ( \\pi ^ { \\infty } ) . \\quad ( 6 4 )$$\n\nThis implies that for all n holds\n\n$$\\begin{array} { r } { \\alpha _ { n } \\int _ { 0 } ^ { T } \\omega _ { 1 }  K  ( \\tilde { \\tau } _ { y , t } ^ { ( n ) } \\| p _ { t } ^ { n } )  d  t \\leq  \\mathcal { L } ^ { \\infty } ( \\pi ^ { \\infty } ) \\Rightarrow } \\end{array} \\quad ( 6 5 )$$\n\n$$\\begin{array} { r } { \\Rightarrow \\int \\limits _ { 0 } ^ { T } \\omega _ { t }  K L  \\left( \\pi _ { y , t } ^ { \\alpha _ { n } } \\Vert \\rho _ { t } ^ { T } \\right)  d  t \\leq  \\frac { \\mathcal { L } ^ { \\infty } ( \\pi ^ { \\infty } ) } { \\alpha _ { n } } \\leq  \\frac { \\mathcal { L } ^ { \\infty } ( \\pi ^ { \\infty } ) } { \\operatorname* { m i n } \\sigma _ { n } } , } \\end{array} \\quad ( 6 6 )$$\n\nwhich is finite, since $\\alpha_n \\to +\\infty$. One more time, we apply Lemma A.22 and conclude that the sequence $\\pi^{n+1}$ is tight. Let $\\pi^{n+1}$ be its weakly convergent subsequence: $\\pi^{n+1} \\xrightarrow{n} \\pi$. Analogously to the Section A.1, we observe that\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { k \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n + 1 } } ( \\pi ^ { \\alpha _ { n + 1 } } ) \\geq \\operatorname* { l i m } _ { k \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n + 1 } } ( \\pi ^ { \\alpha _ { n + 1 } } ) \\geq \\mathcal { L } ^ { \\alpha _ { n + 1 } } ( \\overline { \\pi } ) \\quad ( 6 7 ) } \\end{array}$$\n\nfor any fixed m. The first inequality is due to the monotonicity of $\\mathcal{L}^0$ with respect to $\\alpha$ and second is the implication of lower semi-continuity of the loss $\\mathcal{L}^0$ with respect to weak convergence. Taking the limit $m \\to \\infty$, we obtain\n\n$$\\begin{array} { r } { \\operatorname* { l i m } _ { k \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n _ { k } } } ( \\pi ^ { \\alpha _ { n _ { k } } } ) \\geq \\mathcal { L } ^ { \\infty } ( \\hat { \\pi } ) . } \\end{array} \\quad ( 6 8 )$$\n\nCombining all these observations, we obtain the following sequence of inequalities\n\n$$\\mathcal { L } ^ { \\infty } ( \\pi ^ { \\infty } ) \\geq \\operatorname* { l i m i n f } _ { k \\to \\infty } \\mathcal { L } ^ { \\alpha _ { n _ { k } } } ( \\pi ^ { \\alpha _ { n _ { k } } } ) \\geq \\mathcal { L } ^ { \\infty } ( \\hat { \\pi } ) \\geq \\mathcal { L } ^ { \\infty } ( \\pi ^ { \\infty } ) , \\quad   { ( 6 9 ) }$$\n\n18\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nwhich implies that the limiting measure $\\hat{\\pi}$ reaches the minimum of the objective over generator-based plans. By the uniqueness of the optimal transport map $G^*$ under the Assumptions A.1 , A.2 , A.3 , we conclude that all the convergent subsequences $\\pi^{n_k}$ converge to the optimal measure $\\pi^\\infty$ . Using Corollary A.9 of the Prokhorov theorem, we deduce that $\\pi^{n_n} \\to \\pi^\\infty$.\n\nFinally, we want to replace the weak convergence of $\\pi^{o_n}$ to $\\pi^\\infty$ with the convergence in probability of the generators, i.e. show\n\n$$G ^ { \\alpha _ { n } } \\xrightarrow { p ^ { \\xi } } G ^ { \\infty } . \\quad  ( 7 0 )$$\n\nTo this end, we represent the corresponding probability as the expectation of the indicator and upper bound it with a continuous function:\n\n$$\\begin{array} { r l } { p ^ { \\mathcal { Z } } \\left( \\left\\| G ^ { n _ { 0 } } ( x ) - G ^ { \\infty } ( x ) \\right\\| > \\varepsilon \\right) } & { = \\mathbb { E } _ { p ^ { \\mathcal { Z } } ( x ) } d \\left( \\left\\| G ^ { n _ { 0 } } ( x ) - G ^ { \\infty } ( x ) \\right\\| > \\varepsilon \\right) } \\\\ & { \\leq  \\mathbb { E } _ { p ^ { \\mathcal { Z } } ( x ) } d \\left( \\left\\| G ^ { n _ { 0 } } ( x ) - G ^ { \\infty } ( x ) \\right\\| > \\varepsilon \\right) . } \\end{array} \\quad \\begin{array} { r l } & { ( 7 1 ) } \\\\ & { ( 7 2 ) } \\end{array}$$\n\nwhere $d$ is a continuous indicator approximation, defined as\n\n$$d ( u , v ) = \\begin{cases} \\frac { \\| u - v \\| } { \\| v \\| } , &  ~ i ~ f ~  \\ 0 \\leq  \\| u - v \\| < c ; \\\\ 1 , &  ~ i ~ f ~  \\ \\| u - v \\| \\geq c . \\end{cases} \\quad ( 7 )$$\n\nWe define the test function\n\n$$\\varphi ( x , y ) = d \\left( y , G ^ { \\infty } ( x ) \\right) \\quad ( 7 4 )$$\n\nand rewrite the upper bound as\n\n$$\\begin{array} { r } { \\mathbb { E } _ { p ^ { < s } ( x ) } d \\left( G ^ { \\alpha _ { n } } ( x ) , G ^ { \\infty } ( x ) \\right) = \\mathbb { E } _ { p ^ { < s } ( x ) } \\varphi ( x , G ^ { \\alpha _ { n } } ( x ) ) = \\mathbb { E } _ { p ^ { \\alpha _ { n } } ( x , y ) } \\varphi ( x , y ) . \\quad ( 7 5 ) } \\end{array}$$\n\nDue to Assumptions A.1, A.2 and Theorem A.4 the optimal transport map $G^{**}$ is continuous, which implies that this test function is bounded and continuous. Given the weak convergence of $\\sigma^{n_{k}}$ , we have\n\n$$\\begin{array} { r l } { E _ { \\gamma = \\gamma } ( x , y ) \\varphi ( x , y ) \\to E _ { \\gamma = \\gamma } ( x , y ) \\varphi ( x , y ) E _ { \\gamma = \\gamma } ( x , G ^ { - } ( x ) ) = } & {  \\quad ( 7 6 ) } \\\\ { = E _ { \\gamma = \\gamma } ( d ( G ^ { - } ( x ) , G ^ { - } ( x ) ) ) = , } & {  \\quad ( 7 7 ) } \\end{array}$$\n\nwhich implies the desired\n\n$$p ^ { S } \\left( \\| G ^ { \\alpha _ { s } } ( x ) - G ^ { \\infty } ( x ) \\| > \\varepsilon \\right) \\to 0 . \\quad ( 7 8$$\n\n## B. Ablation of the initialization parameter\n\nIn this section, we further explore the design space of our method by investigating the effect of the fixed generator input noise parameter $\\sigma$ on the resulting quality. To this end, we take the colored version of the MNIST (LeCun, 1998) data set and perform translation between the digits \"2\" and \"3\" initializing from various $\\sigma$ . We use a small UNet architecture from (Gushchin et al., 2024) .\n\nThe parameter $\\alpha$ is residual from the pre-trained diffusion architecture and therefore fixed throughout training and evaluation. However, the target denoiser network tries to convert the expected noisy input into the corresponding sample from the output distribution. Consequently, one may expect that at a suitable noise level, the generator may change the input's details to make them look appropriate for the target while preserving the original structural properties.\n\nWe demonstrate this effect on various noise levels in Figure 6 . Here we observe that the small signals lead to the mapping close to the identity, whereas the large signals lead to almost constant blurry images, corresponding to the average \"3\" of the data set. However, there is a segment $[1.0, 10.0]$ of levels that gives a moderate-quality mapping in terms of both faithfulness and realism, which makes it a suitable initial point. Note that the FID-L2 plot is not monotone at high L2 values in comparison with the FID-Noise plot in Figure 5 (right). This is because the variance of the noise features can easily be investigate optimal $\\sigma$ choice by going through a 2D grid of the hyperparameters $(\\sigma, \\lambda)$ and aim to see if it is possible to choose the uniform best noise level. In Figure 6 we report the faithfulness-quality trade-off concerning various $\\sigma, \\lambda$ . We\n\n19\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\n![Figure](figures/37030_page_020_figure_001.png)\n\nFigure 6. Left: Visualization of the generator initialization at various $\\sigma \\in [0.1, 80, 0]$, where $\\sigma$ is the noise level parameter residual from the pre-trained diffusion architecture. Right: comparison of different $\\sigma$ terms of the quality-faithfulness trade-off. The metrics are obtained by initializing the generator at the corresponding $\\sigma$ level and training it with the RDMD procedure. Here, $\\lambda \\in \\{0, 1.0, 2.0, 4.0\\}$. Higher $\\lambda$ corresponds to the lower transport cost values.\n\nobserve that there is almost monotone dependence on $\\sigma$ on the segment $[1.0, 40.0]$; here the $\\sigma = 1.0$ gives almost uniformly best results in terms of both metrics. Similar results are obtained by the values $5.0, 10.0$ which have fair quality visual results at initialization. Therefore, we conclude that it is best to choose the least parameter $\\sigma$ among the parameters with appropriate visuals at the initial point.\n\n## C. Experimental Details\n\n### C.1. 2D experiments\n\nArchitecture. The architecture used for training diffusion model and generator ( De Bortoli et al. , 2021 ) consists of input-encoding MLP block, time-encoding MLP block, and decoding MLP block. Input encoding MLP block consists of 4 hidden layers with dimensions $[16, 32, 32, 32]$ interspersed by LeakyReLU activations. Time encoding MLP consists of a positional encoding layer ( Vaswani et al. , 2017 ) and then follows the same MLP block structure as the input encoder. The decoding MLP block consists of 5 hidden layers with dimensions $[128, 256, 128, 64, 2]$ and operates on concatenated time embedding and input embedding each obtained from their respective encoder. The model contains 88k parameters.\n\nTraining Diffusion Model. Diffusion model was trained for 100k iterations with batch size 1024 with Adam optimizer (Kingma & Ba, 2014) with learning rate $10^{-4}$.\n\nTraining RDMD. Fake denoising network was trained with Adam optimizer with learning rate $10^{-4}$ . The generator model was trained with a different Adam optimizer with a learning rate equal to $2\\cdot  10^{-5}$ . We trained RDMD for 100k iterations with batch size 1024.\n\nComputational resources. All the experiments were run on CPU. Running 100k iterations with the batch size 1024 took approximately 1 hour.\n\n### C.2. Colored MNIST\n\nArchitecture. We used the architecture from (Gushchin et al., 2024) , which utilizes convolutional UNet with conditional instance normalization on time embeddings used after each upscaling block of the decoder. $^5$ Model produces time embeddings via positional encoding. The model size was approximately $9.9M$ parameters.\n\n'https://github.com/ngushchin/EntropicNeuralOptimalTransport/blob/06feb6ba8b43865a30bd0b626384fa64da39b\n\nsrc/cunet.py\n\nRegularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation\n\nTraining Diffusion Model. The diffusion model was trained for 24500 iterations with batch size $192$. We used the Adam optimizer with a learning rate equal to $4 \\cdot  10^{-3}$. The model was trained in PP2P, it obtained FID equal to 2.09.\n\nTraining RDMD. Fake denoising network was trained with Adam optimizer with a learning rate equal to $2\\cdot 10^{-3}$. The generator model was trained with Adam optimizer with learning rate $5\\cdot 10^{-5}$. RDMD was trained for 7300 iterations with batch size 4096.\n\nComputational resources. All the experiments were run on 2x NVIDIA GeForce RTX 4090 GPUs. Training Diffusion model for 24500 iterations with batch size 8192 took approximately 6 hours. Training RDMD for 7300 iterations with batch size 4096 took approximately 3 hours.\n\n### C.3. Cat2Wild\n\nArchitecture. We used the SongUet architecture from EDM repository 6 , which corresponds to DDPM++ and NCSNN++ networks from the work ( Song et al. , 2020 ) . The model contains approximately 55M parameters.\n\nTraining Diffusion Model. The diffusion model was trained for 80k iterations. We set the batch size to 512 and chose the best checkpoint according to FID. We used the Adam optimizer with a learning rate equal to $2 \\cdot  10^{-4}$. We used a dropout rate equal to 0.25 during training and the augmentation pipeline from (Karras et al., 2022) with probability 0.15. The model was trained in FP32. It obtained FID equal to 2.01.\n\nTraining RDMD. In all runs we initialized the generator from the target diffusion model with the fixed $\\sigma=1.0$. We've run 4 models, corresponding to the regularization coefficients $[0.02, 0.05, 0.1, 0.2]$. All models were trained with Adam optimizer with generator learning rate $5 \\cdot  10^{-5}$ and fake diffusion learning rate $3 \\cdot  10^{-4}$. We trained all models for 25000 iterations with batch size 512. Training took approximately 35 hours on 4 x NVidia Tesla A100 80GB.\n\nILVR hyperparameters. The only hyperparameter of ILVR is the downsampling factor N for the low-pass filter, which determines whether guidance would be conducted on coarser or finer information. $n_\\text{steps}$ denotes number of sampling steps. All metrics in Figure 4 for ILVR were obtained on the following hyperparameter grid: $N = [2, 4, 8, 16, 32]$, $n_\\text{steps} = [18, 32, 50]$ . We excluded runs that have the same statistical significance and achieve FID higher than 20.0. The images in Figure 5 were obtained with hyperparameters $N = 16$ and $n_\\text{steps} = 18$ .\n\nSDEdit hyperparameters. The only hyperparameter of SDEdit is the noise level $\\sigma$ , which acts as a starting point for sampling. The higher the noise level, the closer is the sampling procedure to the unconditional generation. The smaller the noise values, the more features are carried over to the target domain at the expense of generation quality. $n_{samp}$ denotes number of sampling steps. All metrics in Figure 4 for SDEdit were obtained on the following hyperparameter grid: $\\sigma=\\{4.5, 10, 15, 20, 30, 40\\}$ . $n_{samp} = \\{18, 32, 50\\}$ . We exclude runs that have the same statistical significance and achieve FID higher than 20.0. The images in Figure 5 were obtained with hyperparameters $\\sigma=10$ and $n_{samp}=50$ .\n\nEGSDE hyperparameters. EGSDE sampling hyperparameters include the initial noise level $\\alpha$ at which the source image is perturbed, and the downsampling factor $N$ for the low-pass filter. $n_\\text{sample}$ denotes number of sampling steps. All metrics in Figure 4 for EGSDE were obtained on the following hyperparameter grid: $\\sigma = [5, 10, 15, 20, 40]$ , $N = [8, 16, 32]$ , $n_\\text{upsp} = [18, 32]$ . We exclude runs that have the same statistical significance and achieve FID higher than 20.0. The images in Figure 5 were obtained with hyperparameters $\\sigma = 10$ , $N = 32$ , $n_\\text{upsp} = 30$ .\n\n7 https://github.com/NVlabs/edm/blob/008a4e5316c8e3bfe61a62f874bddba254295afb/training/ networks.py\n\n21\n\n",
  "poster_markdown": "![Figure](figures/37030_figure_000.png)\n\n# Regularized Distribution Matching Distillation for One-step Unpaired Image-to-image Translation\n\n![Figure](figures/37030_figure_002.png)\n\nDenis Rakitin$^{1}$, Ivan Shchekotov$^{2}$, Dmitry Vetrov$^{3}$\n\n![Figure](figures/37030_figure_004.png)\n\nSkoltech\n\nCONSTRUCTOR\n\nUNIVERSITY\n\n![Figure](figures/37030_figure_006.png)\n\n![Figure](figures/37030_figure_007.png)\n\n![Figure](figures/37030_figure_008.png)\n\n## Unpaired Image-to-image Translation\n\n$$ Optimal transport (OT) problem $$\n\n$$\\left\\{\\begin{array}{l}\nE_{p(x)} c(x, G(x)) \\to \\min _{G} \\\\\nG(x) \\sim p^{\\tau}\n\\end{array}\\right.$$\n\nPerformance example\n\n![Figure](figures/37030_figure_013.png)\n\n### Diffusion Models\n\n- $\\bullet$ $x_t = x_0 + 6 \\xi E - noisy observations$\n$\\boxed{\\text{Training (Denoising Score Matching)}}$\n$\\bullet$ $\\int_0^T \\beta_t \\mathbb{E}_{p_{t}(x_0,x_t)} \\left\\| D_t^\\circ(x_t) - x_0 \\right\\|^2 dt \\to \\min_\\varrho$ $\\boxed{\\text{}}$\n$\\bullet$ $S_t^{\\theta^*}(x_t) = \\frac{\\rho}{6 \\xi} (D_t^\\circ(x_t) - x_t) \\approx \\nabla_x \\log p_t(x_t)$\n### Distribution Matching Distillation\n\n$$ Train one-step generator  G_{\\theta}(z) $$\n\n$$\\odot \\mathcal{L}(\\theta)=\\int_{0}^{T} \\omega_{t} K L\\left(P_{t}^{\\theta} \\| P_{t}^{real }\\right) d t$$\n\n$$ Joint training $$\n\n$$\\bullet \\quad \\nabla_{\\theta} \\mathcal{L}(\\theta) \\approx\\left(\\zeta_{t}^{\\theta}(\\cdot )-\\zeta_{t}^{real }(\\cdot )\\right) \\nabla_{\\theta} G_{\\theta}(z)$$\n\n$$\\left\\{\\begin{array}{l}\n\\int_{0}^{\\tau} \\omega_{t} E_{z, \\varepsilon} \\log \\frac{P^{\\varphi}\\left(G_{\\theta}(z)+G_{t} \\varepsilon\\right)}{P^{\\alpha}\\left(G_{\\theta}(z)+G_{t} \\varepsilon\\right)} d t \\to \\min _{\\theta} \\\\\n\\int_{0}^{\\tau} \\beta_{t} E_{z, \\varepsilon}\\left\\|D_{t}^{\\varphi}\\left(G_{\\theta}(z)+G_{t} \\varepsilon\\right)-G_{\\theta}(z)\\right\\|^{2} \\to \\min _{\\varphi}\n\\end{array}\\right.$$\n\n## Regularized Distribution Matching Distillation\n\n### $$  Modified objective for I2I  $$\n\n$$\\odot \\mathcal{L}_{\\lambda}(\\theta)=\\int_{0}^{T} \\omega_{t} K L\\left(P_{t}^{\\theta} \\| P_{t}^{T}\\right) d t+\\lambda \\underset{P_{s(x)}^{c}}{\\mathbb{E}}\\left(x, G_{\\theta}(x)\\right) \\to \\min _{\\theta}$$\n\nThm Let $G^{\\lambda}$ be the optimal generator for $L_{\\lambda}$ and $G^{*}$ be the OT map. Then, under reg. conditions $G^{\\lambda} \\xrightarrow{p^{s}} G^{*}$.\n\n### $$  Joint training  $$\n\n$$\\left\\{\\begin{array}{l}\n\\int_{0}^{\\tau} w_{t} \\underset{x, \\varepsilon}{E} \\log \\frac{p^{\\varphi}\\left(G_{\\theta}(x)+G_{t} \\varepsilon\\right)}{p^{\\tau}\\left(G_{\\theta}(x)+G_{t} \\varepsilon\\right)} d t+\\lambda \\underset{p^{s}(x)}{E} c\\left(x, G_{\\theta}(x)\\right) \\to \\min _{\\theta} \\\\\n\\int_{0}^{\\tau} R_{t} \\underset{x, \\varepsilon}{E}\\left\\|D_{t}^{\\varphi}\\left(G_{\\theta}(x)+G_{t} \\varepsilon\\right)-G_{\\theta}(x)\\right\\|^{2} \\to \\min _{\\varphi}\n\\end{array}\\right.$$\n\n### $$  Comparison with diffusion baselines  $$\n\n![Figure](figures/37030_figure_029.png)\n\n![Figure](figures/37030_figure_030.png)\n\n### Gaussian \u2192 SwissRoll\n\n![Figure](figures/37030_figure_032.png)\n\n![Figure](figures/37030_figure_033.png)\n\n![Figure](figures/37030_figure_034.png)\n\n![Figure](figures/37030_figure_035.png)\n\n![Figure](figures/37030_figure_036.png)\n\n$$ L.HSE University, Moscow, Russia $$\n\n$$ 2. Skoltech, Moscow, Russia $$\n\n3. Constructor University, Bremen, Germany\n\n",
  "poster_layout": {
    "paper_id": "37030",
    "poster_path": "archive/test_set/37030.png",
    "poster": {
      "orientation": "portrait",
      "aspect_ratio": "4:3",
      "background": "#ffffff"
    },
    "header": {
      "height_pct": 15,
      "background": "#ffffff",
      "gradient_colors": null,
      "title_alignment": "center",
      "logo_positions": [
        "left",
        "right"
      ],
      "has_author_affiliation_superscripts": true
    },
    "footer": {
      "present": true,
      "height_pct": 5,
      "background": "#ffffff",
      "content": "none"
    },
    "body": {
      "columns": 1,
      "column_widths": [
        "equal"
      ],
      "gutter_pct": 0
    },
    "sections": [
      {
        "id": 1,
        "title": "Generative Trilemma",
        "column": 1,
        "column_span": 1,
        "row_in_column": 1,
        "height_pct": 15,
        "style": {
          "header_bg": "#ffffff",
          "header_text_color": "#000000",
          "body_bg": "#ffffff",
          "border": "#000000",
          "border_radius": "none"
        },
        "content_type": "mixed",
        "content_layout": {
          "arrangement": "horizontal",
          "split": "text-left figure-right",
          "figure_count": 1,
          "has_equations": false,
          "has_tables": false
        }
      },
      {
        "id": 2,
        "title": "Unpaired Image-to-image Translation",
        "column": 1,
        "column_span": 1,
        "row_in_column": 2,
        "height_pct": 15,
        "style": {
          "header_bg": "#ffffff",
          "header_text_color": "#000000",
          "body_bg": "#ffffff",
          "border": "#000000",
          "border_radius": "none"
        },
        "content_type": "mixed",
        "content_layout": {
          "arrangement": "horizontal",
          "split": "text-left figure-right",
          "figure_count": 1,
          "has_equations": true,
          "has_tables": false
        }
      },
      {
        "id": 3,
        "title": "Diffusion Models",
        "column": 1,
        "column_span": 1,
        "row_in_column": 3,
        "height_pct": 15,
        "style": {
          "header_bg": "#ffffff",
          "header_text_color": "#000000",
          "body_bg": "#ffffff",
          "border": "#000000",
          "border_radius": "none"
        },
        "content_type": "text",
        "content_layout": {
          "arrangement": "vertical",
          "split": "none",
          "figure_count": 0,
          "has_equations": true,
          "has_tables": false
        }
      },
      {
        "id": 4,
        "title": "Distribution Matching Distillation",
        "column": 1,
        "column_span": 1,
        "row_in_column": 4,
        "height_pct": 15,
        "style": {
          "header_bg": "#ffffff",
          "header_text_color": "#000000",
          "body_bg": "#ffffff",
          "border": "#000000",
          "border_radius": "none"
        },
        "content_type": "mixed",
        "content_layout": {
          "arrangement": "horizontal",
          "split": "text-left figure-right",
          "figure_count": 1,
          "has_equations": true,
          "has_tables": false
        }
      },
      {
        "id": 5,
        "title": "Regularized Distribution Matching Distillation",
        "column": 1,
        "column_span": 1,
        "row_in_column": 5,
        "height_pct": 25,
        "style": {
          "header_bg": "#ffffff",
          "header_text_color": "#000000",
          "body_bg": "#ffffff",
          "border": "#000000",
          "border_radius": "none"
        },
        "content_type": "mixed",
        "content_layout": {
          "arrangement": "horizontal",
          "split": "text-left figure-right",
          "figure_count": 2,
          "has_equations": true,
          "has_tables": false
        }
      }
    ],
    "figures": [
      {
        "id": 1,
        "section_id": 1,
        "caption": "Generative Trilemma Venn diagram",
        "position": "right side of section 1",
        "type": "diagram",
        "description": "Venn diagram with three overlapping circles labeled GAN, Diffusion, VAE, with text labels for High Quality, Fast Sampling, Mode Coverage, and a question mark."
      },
      {
        "id": 2,
        "section_id": 2,
        "caption": "Performance example of image translation",
        "position": "right side of section 2",
        "type": "image",
        "description": "Grid of 8 images showing cat-to-animals translation results."
      },
      {
        "id": 3,
        "section_id": 4,
        "caption": "Distribution Matching Distillation equations",
        "position": "right side of section 4",
        "type": "equation",
        "description": "Two equations for training a one-step generator with distribution matching distillation."
      },
      {
        "id": 4,
        "section_id": 5,
        "caption": "Comparison with diffusion baselines",
        "position": "right side of section 5",
        "type": "chart",
        "description": "Three line charts comparing FID, L2 transport cost, and SSIM scores for different methods."
      },
      {
        "id": 5,
        "section_id": 5,
        "caption": "Gaussian \u2192 SwissRoll",
        "position": "bottom right of section 5",
        "type": "chart",
        "description": "Five scatter plots showing the effect of regularization parameter \u03bb on distribution matching."
      }
    ],
    "flowcharts": [],
    "special_elements": [
      {
        "type": "qr_code",
        "position": "top left corner",
        "description": "QR code for arXiv link"
      },
      {
        "type": "logo",
        "position": "top right corner",
        "description": "SPIGM @ ICML logo"
      },
      {
        "type": "highlight_box",
        "position": "center of section 1",
        "description": "Box highlighting RDMD (ours) contributions"
      }
    ],
    "color_scheme": {
      "primary": "#000000",
      "secondary": "#000000",
      "accent": "#ff9900",
      "text": "#000000",
      "background": "#ffffff"
    },
    "reading_order": "top-to-bottom"
  },
  "figure_matches": [
    {
      "poster_figure": {
        "figure_id": "37030_figure_030",
        "source_doc": "poster",
        "file_path": "test_output_staged/poster_parsed/37030/markdown/figures/37030_figure_030.png",
        "bbox": null,
        "caption": null,
        "figure_type": null,
        "page_number": 1
      },
      "paper_figure": {
        "figure_id": "37030_page_007_figure_001",
        "source_doc": "paper",
        "file_path": "test_output_staged/paper_parsed/37030/markdown/figures/37030_page_007_figure_001.png",
        "bbox": null,
        "caption": null,
        "figure_type": null,
        "page_number": 1
      },
      "match_confidence": "high",
      "feature_score": 0.256,
      "vlm_verdict": "same",
      "vlm_confidence": 1.0,
      "vlm_reasoning": "Both figures display identical scatter plots with the same data points, axes labels, ranges, legend, and annotations. The only difference is the background color (white vs. light gray), which does not affect the content or meaning of the visualization."
    }
  ],
  "conference": "Test",
  "year": 2024
}